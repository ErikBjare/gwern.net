---
title: Complexity no Bar to AI
description: Critics of AI risk suggest diminishing returns to computing means AI will be weak; I argue argument breaks if any premises rejected
tags: computer science, transhumanism
created: 01 June 2014
status: in progress
belief: possible
...



there's a lot of bad reasoning out there. for example, this is just an abuse of computational complexity:
http://www.antipope.org/charlie/blog-static/2014/02/the-singularity-is-further-tha.html
http://www.antipope.org/charlie/blog-static/2014/02/why-ais-wont-ascend-in-blink-of-an-eye.html
http://www.antipope.org/charlie/blog-static/2014/02/a-rebuttal-of-the-singularity-.html
and I've been thinking about writing a
rebuttal

I mean, where to start? he ignores increased resource consumption; constant factors; redefining problems for major speedups; algorithmic
breakthroughs; avoiding unnecessarily general algorithms...
he understands the very basics of asymptotic complexity but he ignores all the potential loopholes and what
asymptotic complexity explicitly excludes from its domain
and also omits the possibility of solving entirely different problems
see also Chalmer's paper on the singularity

http://rjlipton.wordpress.com/2014/07/21/shifts-in-algorithm-design/

> Now today, in the 21st century, we have a better way to attack problems. We change the problem, often to one that is more tractable and useful. In many situations solving the exact problem is not really what a practitioner needs. If computing X exactly requires too much time, then it is useless to compute it. A perfect example is the weather: computing tomorrow's weather in a week's time is clearly not very useful.
>
> The brilliance of the current approach is that we can change the problem. There are at least two major ways to do this:
>
> - Change the answer required. Allow approximation, or allow a partial answer. Do not insist on an exact answer.
> - Change the algorithmic method. Allow algorithms that can be wrong, or allow algorithms that use randomness. Do not insist that the algorithm is a perfect deterministic one.
>
> This is exactly what Chayes and her co-authors have done.

--

"The asymptotically best algorithms frequently turn out to be worst on all problems for which they are used." - D. G. CANTOR and H. ZASSENHAUS (1981)
  "Rule 3. Fancy algorithms are slow when n is small, and n is usually small. Fancy algorithms have big constants. Until you know that n is frequently going to be big, don't get fancy. (Even if n does get big, use Rule 2 first.)"  --Rob Pike" Ken Thompson rephrased Pike's rules 3 and 4 as "When in doubt, use brute force."." http://users.ece.utexas.edu/~adnan/pike.html  This list is from Rob's "Notes on Programming on C", written 25 years ago: http://doc.cat-v.org/bell_labs/pikestyle

"Knuth did a comparison between Fibonacci heap and binary heaps for minimum spanning trees back in 1993 for his book Stanford GraphBase. He found Fibonacci to be 30 to 60 percent slower than binary heaps at the graph sizes he was testing, 128 vertices at different densities." https://stackoverflow.com/questions/504823/has-anyone-actually-implemented-a-fibonacci-heap-efficiently

> In general I'm looking for more focus on algorithms that work fast with respect to problems whose size, n, is feasible. Most of today's literature is devoted to algorithms that are asymptotically great, but they are helpful only when n exceeds the size of the universe...Another issue, when we come down to earth, is the efficiency of algorithms on real computers. As part of the Stanford GraphBase project I implemented four algorithms to compute minimum spanning trees of graphs, one of which was the very pretty method that you developed with Cheriton and Karp. Although I was expecting your method to be the winner, because it examines much of the data only half as often as the others, it actually came out two to three times worse than Kruskal's venerable method. Part of the reason was poor cache interaction, but the main cause was a large constant factor hidden by O notation.

http://www.informit.com/articles/article.aspx?p=2213858

"The Coppersmith-Winograd algorithm is frequently used as a building block in other algorithms to prove theoretical time bounds. However, unlike the Strassen algorithm, it is not used in practice because it only provides an advantage for matrices so large that they cannot be processed by modern hardware.[6]" https://en.wikipedia.org/wiki/Coppersmith%E2%80%93Winograd_algorithm

# IQ

> Maybe I'm mistaken and most of the membership here views Less Wrong as a "super high G" community, but I don't.

How much time do you spend with normal people? What's your score on Murray's high-IQ bubble checklist?

> but still, for Ivy Leaguers, who have a high level of clout in our society, to have an average IQ around that level, does not address the question of whether additional IQ above that level has diminishing impact.

No, but the original claim was clearly wrong. Society *is* dominated by high-IQ people. Diminishing returns seems to be weirdly interpreted as 'no returns' in a lot of people's minds.

It may help if I quote a bit of what I've [written on a similar issue](http://www.gwern.net/the-long-stagnation) before about diminishing returns to research:

> The Long Stagnation thesis can be summarized as: "Western civilization is experiencing a general decline in marginal returns to investment". That is, every $1 or other resource (such as 'trained scientist') buys less in human well-being or technology than before, aggregated over the entire economy.
>
> This does *not* imply any of the following:
>
> 1. No exponential curves exist (rather, they are exponential curves which are part of sigmoids which have yet to level off; Moore's law and stagnation can co-exist)
>
>     Sudden dramatic curves can exist even amid an economy of diminishing marginal returns; to overturn the overall curve, such a spike would have to be a massive society-wide revolution that can make up for huge shortfalls in output.
> 2. Any metrics in absolute numbers have ceased to increase or have begun to fall (patents can continue growing each year if the amount invested in R&D or number of researchers increases)
> 3. We cannot achieve meaningful increases in standards of living or capabilities (the Internet is a major accomplishment)
> 4. Specific scientific or technological will not be achieved (eg. AI or nanotech) or be achieved by certain dates
> 5. The stagnation will be visible in a dramatic way (eg. barbarians looting New York City)

Similarly, arguing over diminishing returns to IQ is building in a rather strange premise to the argument: that the entities in discussion will be within a few standard deviations of current people. It may be true that people with IQs of 150 are only *somewhat* more likely to be billionaires ruling the world than 140, but how much does that help when you're considering the actions of people with IQs much much higher? The returns can really add up.

To take an example I saw today: [Hsu posted](http://infoproc.blogspot.com/2014/06/chicago-conference-on-genetics-and.html) slides from [an April talk](https://drive.google.com/file/d/0ByYDxaP-OyVjNzZBMWZlbEI3ZnM/edit?usp=sharing), which on pg10 points out that the estimates of the additive genetic influence on intelligence (the kind we can most easily identify and do stuff like embryo selection with) & estimates of number of minor alleles imply a potential upper bound of +25 SD if you can select all beneficial variants, or in more familiar notation, IQs of 475 (100 + 15 * 25). Suppose I completely totally grant all assumptions about diminishing marginal returns to IQ based on the small samples we have available of 130+; what happens when someone with an IQ of 475 gets turned loose? Who the heck knows; they'll probably rule the world, if they want.

-One of the problems with discussing this is that IQ scores and all research based on it is purely an ordinal scale based on comparing existing humans, while what we really want is to measures of intelligence on a cardinal scale which lets us compare not just humans but potential future humans and AIs too.

For all we know, diminishing returns in IQ is purely an artifact of human biology: maybe each standard deviation represents less and less 'objective intelligence', and the true gains to objective intelligence don't diminish at all or in some cases *increase* (chimps vs humans)!

(Hsu likes to cite a maize experiment where "over 100 generations of selection have produced a difference in oil content between the high and low selected strains of 32 times the original standard deviation!"; so when we're dealing with something that's clearly on a cardinal scale - oil content - the promised increases can be quite literal. Intelligence is not a fluid, so we're not going to get 25x more 'brain fluid', but that doesn't help us calculate the consequences: an intelligent agent is competing against humans and other software, and small absolute edges may have large consequences. A hedge fund trader who can be right 1% more of the time than his competition may be able to make a huge freaking fortune. Or, a researcher 1% better at all aspects of research may, under the log-normal model of research productivity [proposed by Shockley](http://www.gwern.net/docs/1967-shockley.pdf), be much more than 1% more productive than his peers.)

We know 'human' is not a inherent limit on possible cognition or a good measurement of all activities/problems: eg chess programs didn't stagnate in strength after Deep Blue beat Kasparov, having hit the ceiling on possible performance but they kept getting better. Human performance turned out to not run the gamut from worst to best-possible but rather marked out a fairly narrow window that the chess programs were in for a few decades but passed out of, on their trajectory upwards on whatever 'objective chess intelligence' metric there may be.

(I think this may help explain why some events surprise a lot of observers: when we look at entities below the human performance window, we just see it as a uniform 'bad' level of performance, we can't see any meaningful differences and can't see any trends, so our predictions tend to be hilariously optimistic or pessimistic based on our prior views; then, when they finally enter the human performance window, we can finally apply our existing expertise and become surprised and optimistic, and then the entities can with small objective increases in performance move out of the human window entirely and it becomes an activity humans are now uncompetitive at like chess but may still contribute a bit on the margin in things like advanced chess, and eventually becomes truly superhuman as computer chess will likely soon be.)

# Parable of the Worms

Once upon a time, two _[C. elegans](!Wikipedia)_ were debating the prospect of "transwormism", and specifically the possibility of hypothetical creatures from elsewhere in the space of all possible organisms, which might exceed worms by as much as worms exceed the bacteria they thrive upon.

Crawlviati argues for transwormism:

"There is no a priori reason to believe that worms must be the pinnacle of creation, and that there are no larger or more complex or more intelligent organisms possible.
We should be applying the Poopernican Principle here - the manure piles in which we live are but a tiny segment of the universe in both space and time, of no privileged perspective, and so in the Great Chain of Eating, we should expect us to be in a mediocre position.
Indeed, from the start of life, we can see many breakthroughs: multicellular life has produced endless forms most different from unicellular life, and fairly recently have neurons been invented, so we're at an exciting point in time.
We can speculate about the possibilities: a transworm might be completely different from us worms; or it might be similar in architecture to us worms, perhaps with a much longer body with many more neurons and so much smarter than us.
Regardless, a transworm would be difficult for us to predict, and may be able to develop very fast as it learns new ways of hunting bacteria and self-fertilization, in what we might call a Moundularity in which it piles up resources and offspring faster than anyone else; inasmuch as a transworm may have very different priorities from us and change the environment to fit its needs, it would be dangerous to us."

Slimeplicius disagrees:

"Ridiculous! Think for a moment about your claims.
We are blessed with 302 neurons, with which we can react to stimuli, move forward, move backward, hunt bacteria, and solve challenging navigational puzzles of many worm-lengths.
But these problems exhibit diminishing returns - optimal maze navigation is exponentially difficult, for example.
Transworms would immediately find their additional cognition to be of ever less marginal value as they run up against the wall of wormputational complexity.
What would they do with, say, 1000 neurons that would justify a metabolic cost of *over 3 times more*?
And to be truly worthy of the name transworm, they might need 10s of thousands, or even millions of neurons!
Consider the absurdity of such an architecture: can our great manure pile even support a single such transworm?
Where would the food come from? For that matter, how would its body support so many neurons?
And its genes could no longer specify cell placement one by on, but organization would have to somehow 'emerge', which you are rather obscure about how this might happen.
Not to mention the many problems you seem to gloss over in your faith in progress: for example, diffusion would no longer work to feed each cell, requiring nonexistent mechanisms to move fluids around.
If a transworm *could* exist, it would be exponentially difficult for it to eat bacteria and reproduce faster than regular worms, and its performance would likely converge with ours: it would solve our problems only slightly better than us, at tremendously increased cost.
(We could even make an evolutionary argument: we have evolved to be as smart as is optimal in our niche, but no smarter.)
Certainly, any Moundularity would be so slow us worms would smell it coming long in advance and wriggle together in a big ball to crush it."

Crawlviati:

"Your argument seems very narrow to me.
Yes, I agree that it would be difficult to support so many neurons packed together in one worm, but I'm sure the engineering difficulties can be overcome - there seems to be no fundamental limit to wormputation much greater than 302 neurons, so there must be a way.
And your food objection is likewise soluble: perhaps transworms can migrate from compost pile to garden regularly as they exhaust resources there, or even figure out some way to easily knock down low-hanging fruit.
They may not, bacterium for bacterium, be as efficient as us, but that doesn't matter as long as the diminishing returns don't turn into *negative* returns.
As long as the returns are positive, they will be able to pay for their increased resource utilization and continue climbing up the exponential curves.
And what does 'better' even mean here? The wormputational complexity of a maze may increase sharply with maze size, but that's a statement about mazes, not about comparing maze-solvers, which might be arbitrarily better or worse than each other, so there's a problem: maybe they could solve mazes 100x faster.
Then there's figuring out what any bit of performance means: if a transworm could solve mazes twice as fast as you or I, maybe it'll get *all* the treats at the end, not merely twice as many.
Heck, we're *worms*! What do we know about the world? Maybe there are all sorts of cool things which can be done, besides stimulus, response, stimulus, response - if we could just *think* for once in our lives!"

Slimeplicius:

"These claims seem to rest entirely on what I might call an appeal to ignorance: maybe mazes can be run faster than we can, maybe there are great things which could be done with more neurons, maybe there's lots of food we can't obtain but could with more intelligence...
Sure, maybe I can't prove that there aren't, but is any of this what a reasonable worm, the ordinary worm in the dirt, would believe? Certainly not."

Crawlviati:

"If you'll just think a little more about the possibilities..."

Slimeplicius:

"There are better things to worry about, like pile warming. What if the decay makes our mound too hot for us? We should discuss that instead."

So they did. A week later, the farm was sold to a developer, and the mound was rolled flat,and then paved over with asphalt - the construction workers neither hated nor loved the worms, but they were made of atoms that would be useful for stabilizing the road to the townhouses being built there.

# Parable of Nacirema

http://lesswrong.com/lw/kfd/a_parable_of_elites_and_takeoffs/

Let me tell you a parable of the future. Let's say, 70 years from now, in a large Western country we'll call Nacirema.

One day far from now: scientific development has continued apace, and a large government project (with, unsurprisingly, a lot of military funding) has taken the scattered pieces of cutting-edge research and put them together into a single awesome technology, which could revolutionize (or at least, vastly improve) all sectors of the economy. Leading thinkers had long forecast that this area of science's mysteries would eventually yield to progress, despite theoretical confusion and perhaps-disappointing initial results and the scorn of more conservative types and the incomprehension (or outright disgust, for 'playing god') of the general population, and at last - it had! The future was bright.

Unfortunately, it was hurriedly decided to use an early prototype outside the lab in an impoverished foreign country. Whether out of arrogance, bureaucratic inertia, overconfidence on the part of the involved researchers, condescending racism, the need to justify the billions of grant-dollars that cumulative went into the project over the years by showing *some* use of it - whatever, the reasons no longer mattered after the final order was signed. The technology was used, but the consequences turned out to be horrific: over a brief period of what seemed like mere days, entire cities collapsed and scores - hundreds - of thousands of people died. (Modern economies are extremely interdependent and fragile, and small disruptions can have large consequences; more people died in the chaos of the evacuation of the areas around Fukushima than will die of the radiation.)

An unmitigated disaster. Worse, the technology didn't even accomplish the assigned goal - that was thanks to a third party's actions! Ironic. But that's how life goes: ['Man Proposes, God Disposes'](https://www.royalholloway.ac.uk/archives/itemofthemonth/items/may2013.aspx).

So, what to do with it? The positive potential was still there, but no one could doubt anymore that there was a horrific dark side: they had just seen what it could do if misused, even if the authorities (as usual) were spinning the events as furiously as possible to avoid frightening the public. You could put it under heavy government control, and they did.

But what was to stop Nacirema's rivals from copying the technology and using it domestically or as a weapon against Nacirema? In particular, Nacirema's enormous furiously-industrializing rival far to the East in Asia, which aspired to regional hegemony, had a long history of being an "oriental despotism" and still had a repressive political system - ruled by an opaque corrupt oligarchy - which abrogated basic human rights such as free speech, and was not a little racist/xenophobic & angry at historical interference in its domestic affairs by Seilla & Nacirema...

The 'arms race' was obvious to anyone who thought about the issue. You had to obtain your own tech or be left in the dust. But an arms race was terrifyingly dangerous - one power with the tech was bad enough, but if there were two holders? A dozen? There was no reason to expect all the wishes to be benign once everyone had their own genie-in-a-bottle. It would not be hyperbolic to say that the fate of global civilization was at stake (even if there were survivors off-planet or in Hanson-style 'disaster refuges', they could hardly rebuild civilization on their own; not to mention that a lot of resources like hydrocarbons have already been depleted beyond the ability of a small primitive group to exploit) or maybe even the human race itself. If ever an x-risk was a clear and present danger, this was it.

Fortunately, the 'hard take-off' scenario did not come to pass, as each time it took years to double the power of the tech; nor was it something you could make in your bedroom, even if you knew the key insights (deducible by a grad student from published papers, as concerned agencies in Nacirema proved). Rather, the experts forecast a slower take-off, on a more human time-scale, where the technology escalated in power over the next two or three decades; importantly, they thought that the Eastern rival's scientists would not be able to clone the technology for another decade or perhaps longer.

So one of the involved researchers - a bona fide world-renowned genius who had made signal contributions to the design of the computers and software involved and had the utmost credibility - made the obvious suggestion. *Don't* let the arms race start. *Don't* expose humanity to an unstable equilibrium of the sort which has collapsed many times in human history. Instead, Nacirema should boldly deliver an ultimatum to the rival: submit to examination and verification that they were not developing the tech, or be destroyed. Stop the contagion from spreading and root out the x-risk. Research in the area would be proscribed, as almost all of it was inherently dual-use.

Others disagreed, of course, with many alternative proposals: perhaps researchers could be trusted to self-regulate; or, related research could be regulated by a special UN agency; or the tech could be distributed to all major countries to reach an equilibrium immediately; or, treaties could be signed; or Nacirema could voluntarily abandon the technology, continue to do things the old-fashioned way, and lead by moral authority.

You might think that the politicians would do *something*, even if they ignored the genius: the prognostications of a few obscure researchers and of short stories published in science fiction had turned out to be truth; the dangers had been realized in practice, and there was no uncertainty about what a war with the tech would entail; the logic of the arms race has been well-documented by many instances to lead to instability and propel countries into war (consider the battleship arms race leading up to WWI); the proposer had impeccable credentials and deep domain-specific expertise and was far from alone in being deeply concerned about the issue; there were multiple years to cope with the crisis after fair warning had been given, so there was enough time; and so on. If the Nacireman political system were to ever be willing to take major action to prevent an x-risk, this would seem to be the ideal scenario. So did they?

One might have faith in the political elites of this country. Surely given the years of warning as the tech became more sophisticated, people would see that this time really was different, this time it was the gravest threat humanity had faced, that the warnings of elite scientists of doomsday would be taken seriously; surely everyone would see the truth of proposition X, leading them to endorse Y and agree with the 'extremists' about policy decision Z (to condense our hopes into one formula); how can we doubt that policy-makers and research funders would begin to respond to the tech safety challenge? After all, we can point to some other instances where policymakers reached good outcomes for minor problems like CFC damages to the atmosphere.

So in our little future world, did the Nacireman political system respond effectively?

I'm a bit cynical, so let's say the answer was... No. Of course not. They did not follow his plan.

And it's not that they found a better plan, either. (Let's face it, any plan calling for more war has to be considered a last resort, even if you have a special new tech to help, and is likely to fail.) Nothing meaningful was done. "Man plans, God laughs." The trajectory of events was indistinguishable from bureaucratic inertia, self-serving behavior by various groups, and was the usual story. After all, what was in it for the politicians? Did such a strategy swell any corporation's profits? Or offer scope for further taxation & regulation? Or could it be used to appeal to anyone's emotion-driven ethics by playing on disgust or purity or in-group loyalty? The strategy had no constituency except those who were concerned by an abstract threat in the future (perhaps, as their opponents insinuated, they were neurotic 'hawks' hellbent on war). Besides, the Nacireman people were exhausted from long years of war in multiple foreign countries and a large domestic depression whose scars remained. Time passed.

Eventually the experts turned out to be wrong but in the worst possible way: the rival took half the time projected to develop their own tech, and the window of opportunity snapped shut. The arms race had begun, and humanity would tremble in fear, as it wondered if it would live out the century or the unthinkable happen.

Good luck, people of the future! I wish you all the best, although I can't be optimistic; if you survive, it will be by the skin of your teeth, and I suspect that due to hindsight bias and near-miss bias, you won't even be able to appreciate how dire the situation was afterwards and will forget your peril or minimize the danger or reason that the tech couldn't have been *that* dangerous since you survived - which would be a sad & pathetic coda indeed.

The End.

(Oh, I'm sorry. Did I write "70 years from now"? I meant: "70 years *ago*". The technology is, of course, nuclear fission which had many potential applications in civilian economy - if nothing else, every sector benefits from electricity 'too cheap to meter'; Nacirema is America & the eastern rival is Russia; the genius is John von Neumann, the SF stories were by Heinlein & Cartmill among others - the latter giving rise to the _Astounding_ incident; and we all know how the Cold War led civilization to the brink of thermonuclear war. Why, did you think it was about something else?)
