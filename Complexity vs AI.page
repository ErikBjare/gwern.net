---
title: Complexity no Bar to AI
description: Critics of AI risk suggest diminishing returns to computing means AI will be weak; I argue argument breaks if any premises rejected
tags: computer science, transhumanism
created: 1 June 2014
status: in progress
belief: possible
...

> Computational complexity theory describes the steep increase in computing power required for many algorithms to solve larger problems; frequently, the increase is large enough to render problems a few times larger totally intractable. Many of these algorithms are used in AI-relevant contexts. It has been argued that this implies that AIs will fundamentally be limited in accomplishing real-world tasks better than humans because they will run into the same computational complexity limit as humans, and so the consequences will be small, thus it is impossible for there to be any large fast global changes due to human or superhuman-level AIs. I examine the assumptions of this argument and find it neglects the many conditions under which computational complexity theorems are valid and so the argument doesn't work: problems can be solved more efficiently than complexity classes would imply, large differences in problem solubility between humans and AIs is possible, the real-world consequences of small differences on individual tasks can be large, such consequences can compound, and any of these independent objections being true destroys the argument.

# Background

[Computational complexity theory](!Wikipedia) attempts to describe the resource usage of algorithms from the abstract vantage point of considering how running time on some idealized computer relatively increases for a specific algorithm as the inputs scale in size towards infinity.
For many important algorithms used in AI and programming in general, the difficulty turns out to increase steeply with extra data - comparison-based sorting algorithms like [Quicksort](!Wikipedia) take only [Big O](!Wikipedia "Big O notation") $\mathcal{O}(n \cdot log(n))$ and so you can sort just about any amount of data in a feasible time, but more interesting problems like the [Traveling Salesman problem](!Wikipedia)/[3-SAT](!Wikipedia) become [nonpolynomially](!Wikipedia "NP-completeness") hard (or exponentially or worse) as the data increases and quickly go from fast to feasible to impossible.

# Complexity implies Singularities are impossible

One argument against powerful artificial intelligences, and scenarios corresponding to [Singularities](!Wikipedia "Technological singularity") in general, draws from [computational complexity theory](!Wikipedia).

http://www.antipope.org/charlie/blog-static/2014/02/the-singularity-is-further-tha.html
http://www.antipope.org/charlie/blog-static/2014/02/why-ais-wont-ascend-in-blink-of-an-eye.html
http://www.antipope.org/charlie/blog-static/2014/02/a-rebuttal-of-the-singularity-.html

http://lesswrong.com/r/discussion/lw/lke/the_unique_games_conjecture_and_fai_a_troubling/bvy0

This can be seen as one of the "structural objections" (as CH)

So the argument (filling in the gaps and omitting the various graphs showing scaling) goes something like this:

1. most tasks an intelligent agent (human or artificial intelligence) needs to solve are in difficult complexity classes, such as NP or NP-hard: Traveling Salesman, 3-SAT, [Bayesian network](!Wikipedia) belief propagation, [deep neural network](!Wikipedia) training, [theorem proving](!Wikipedia), playing [Go](!Wikipedia "Go (game)"), solving [POMDPs](!Wikipedia "Partially observable Markov decision process")...
2. a task in NP or higher complexity class can only be solved for small problem sizes
3. if a task can only be solved for small problem sizes, then the best agent will solve only slightly larger problem sizes
4. the real-world reward to an agent from solving a slightly larger problem is only slightly greater
5. the long-term consequence of slightly greater rewards is itself small
6. if an AI becomes the best agent, then it must solve problems in difficult complexity classes (1), so it will be able to solve only slightly larger programs (2-3), receiving slightly greater rewards (4), with only small long-term consequences (5)
7. if each AI has only small long-term consequences, all AIs together will have a small total long-term consequence
8. thus, AIs becoming intelligent agents will have only small total long-term consequences

This argument is valid as far as it goes and can probably be formalized successfully. But is the argument sound?

## Complexity caveats

One difficulty with applying computational complexity theory outside of its usual area is that people tend to neglect the requirements of complexity theory which gives it its generality: that it omits the 'constant factors' and the actual runtime, that many of the statements are lower/upper bounds or statements about worst-case complexity, that the statements tend to be about specific algorithms which are by no means the only way to solve a real-world problem, and that it doesn't try to say anything about utility or consequences.

Laid out bare, I would have to say that the argument depends critically on each of the premises being true, but every premise 1-5 is either questionable or false.

### Are all problems worst-case and NP-hard?

Premise 1 is incorrect because the proofs of those complexities generally depend on general solutions with deterministic exactly-optimal worst-case behavior.
The apparent barrier of a complex problem can be bypassed by:

0. converting it to a smaller complexity class: existing proofs could be incorrect, inapplicable, or based on open conjectures widely believed by humans one way but which could still resolve in the more advantageous direction (eg [P=NP](!Wikipedia))
1. needing good [average-case](http://www.karlin.mff.cuni.cz/~krajicek/ri5svetu.pdf "'A Personal View of Average-Case Complexity', Impagliazzo 1995") behavior rather than worst-case behavior

    In the *worst case*, 3-SAT/Traveling Salesman are wildly intractable for any realistic dataset like planning a trip across the USA; but the *average-case* performance is quite different and in practice, 3-SAT/Traveling Salesman are solved all the time, to the extent where SAT solvers are routinely used in computer security or theorem proving or type-checking and logistics companies are able to heavily optimize their shipping with them.
    Similarly for linear programming's [simplex algorithm](!Wikipedia) and other operations research algorithms, with are theoretically intimidating but in real-world problems yield solutions after reasonable runtime - they work in practice, but not in theory.
    For example, TSP instances up to [85,900 cities](https://web.archive.org/web/20160326013407/http://www.math.uwaterloo.ca/tsp/pla85900/index.html) have been solved.
2. giving up determinism and using [randomized algorithms](!Wikipedia) which are faster but [may not return an answer](!Wikipedia "Las Vegas algorithm") or a [correct answer](!Wikipedia "Monte Carlo algorithm")[^Lipton-shifts] (they typically can be run many times if correctness is important; after a few times, the probability of an erroneous error will be smaller than the probability that the computer hardware has suffered [a glitch due to cosmic rays](The Existential Risk of Mathematical Error)); randomization can be applied to algorithms and to [data structures](!Wikipedia "Category:Probabilistic data structures").
3. giving up optimality and computing an approximation of the optimal answer (often very close to the optimal answer); already mentioned is 3-SAT/TSP, for which there is a [World Tour of 1,904,711-cities](https://web.archive.org/web/20160308183523/http://www.math.uwaterloo.ca/tsp/world/ "World TSP") which has been solved with a tour within 0.0474% of the optimal tour by 2007
4. giving up generality: an algorithm may be unnecessarily general.

    A comparison sort can be done in $\mathcal{O}(n \cdot log(n))$, yes, but one frequently doesn't need to sort any kind of datatype for which an ordering is available but specifically strings, numbers, etc, for which quasi linear/$\mathcal{O}(n)$ sorting is possible using [counting sort](!Wikipedia)/[radix sort](!Wikipedia). An algorithm could also have prior information about the kind of data input which will be available - [Timsort](!Wikipedia) is aware that most inputs will be partially sorted already and can outperform a naive sort. Data structures can be tuned for particular distributions of data, and [JIT](!Wikipedia "Just-in-time compilation") & [profile-guided optimization](!Wikipedia) & [supercompilation](!Wikipedia) can be seen as [specializing](!Wikipedia "Futamura projection") general algorithms to the current or likely-future inputs.
4. changing the problem rather than succumbing to functional fixity: many problems or environments can be tweaked to bypass a challenge and leverage computer strengths.

    A [self-driving car](!Wikipedia) may not be as good at vision as a human driver, but [LIDAR](!Wikipedia) sensors can be incorporated into it in a way that cannot for humans as it would distract them; a [robot in a warehouses](!Wikipedia "Amazon Robotics") may not be as good at driving around as a human worker, but the environment can be altered with white lines or barcode tags on the floor so the robots always know where they are.
5. solving entirely different problems which humans cannot or will not solve.

    As [Hamming](http://www.cs.virginia.edu/~robins/YouAndYourResearch.html "You and Your Research") says, "There are wavelengths that people cannot see, there are sounds that people cannot hear, and maybe computers have thoughts that people cannot think." There are problems humans could never solve because it would require too much training, or too much memory, or too bizarre solutions. A human would never come up with many solutions that [genetic algorithms](!Wikipedia) or neural networks do, and they can be used on scales that humans never would; an unimportant but interesting example would be ["PlaNet - Photo Geolocation with Convolutional Neural Networks"](http://arxiv.org/pdf/1602.05314v1.pdf "Weyand et al 2016") - I can't imagine any human ever beginning to compete with such a CNN, or even trying to accomplish the same thing. In such cases, scaling concerns are totally besides the point.

[^Lipton-shifts]: ["Shifts In Algorithm Design"](http://rjlipton.wordpress.com/2014/07/21/shifts-in-algorithm-design/), Lipton/Regan:

    > Now today, in the 21st century, we have a better way to attack problems. We change the problem, often to one that is more tractable and useful. In many situations solving the exact problem is not really what a practitioner needs. If computing _X_ exactly requires too much time, then it is useless to compute it. A perfect example is the weather: computing tomorrow's weather in a week's time is clearly not very useful. The brilliance of the current approach is that we can change the problem. There are at least two major ways to do this:
    >
    > - Change the answer required. Allow approximation, or allow a partial answer. Do not insist on an exact answer.
    > - Change the algorithmic method. Allow algorithms that can be wrong, or allow algorithms that use randomness. Do not insist that the algorithm is a perfect deterministic one.
    >
    > This is exactly what Chayes and her co-authors have done.

### Are all implementations equally fast?

Premise 3 ignores that complexity classes by design try to abstract away from the 'constant factors' which is the computation time determined not by input size but by the details of computer architectures, implementations, and available computing hardware.

But with carefully optimized code, proper use of the cache hierarchy, and specialized hardware (eg GPUs, ASICs), it is possible to see [performance gains of multiple orders of magnitude](/Aria's past, present, and future#fn3), which implies that one can increase the input size several times before hitting the scaling way that another agent might who paid less attention to constant factors.
The importance of constant factors is one of the major traps in practical use of complexity classes: a fancy algorithm with a superior complexity class may easily be defeated by a simpler algorithm with worse complexity but faster implementation.[^galactic-algorithms] (One reason that programmers are exhorted to benchmark, benchmark, benchmark!)
Finally, increased resource use / brute force is always an option for a powerful agent. Thanks to its intelligence, humanity now controls a large fraction of the biosphere's energy and with a supercomputer, or tech giants like Google or Amazon who control millions of processor-cores, can compute things totally out of reach of other agents; no limits to the amount of computation that can be done on (or off) Earth have yet been reached.

This doesn't disprove the complexity class, which is about asymptotic scaling and will still kick in at some point, but if it's possible to double or dectuple or more the input, this is enough of an increase that it's hard to dismiss the difference between best and worst agents' problem sizes as being only 'slight'.

[^galactic-algorithms]: Some examples of this folk wisdom: Cantor & Zassenhaus 1981:

    > The asymptotically best algorithms frequently turn out to be worst on all problems for which they are used.

    ["Notes on Programming on C"](http://doc.cat-v.org/bell_labs/pikestyle), Rob Pike:

    > Rule 3. Fancy algorithms are slow when _n_ is small, and _n_ is usually small. Fancy algorithms have big constants. Until you know that _n_ is frequently going to be big, don't get fancy. (Even if _n_ does get big, use Rule 2 first.)

    [Knuth](http://www.informit.com/articles/article.aspx?p=2213858):

    > In general I'm looking for more focus on algorithms that work fast with respect to problems whose size, _n_, is feasible. Most of today's literature is devoted to algorithms that are asymptotically great, but they are helpful only when n exceeds the size of the universe...Another issue, when we come down to earth, is the efficiency of algorithms on real computers. As part of the Stanford GraphBase project I implemented four algorithms to compute minimum spanning trees of graphs, one of which was the very pretty method that you developed with Cheriton and Karp. Although I was expecting your method to be the winner, because it examines much of the data only half as often as the others, it actually came out two to three times worse than Kruskal's venerable method. Part of the reason was poor cache interaction, but the main cause was a large constant factor hidden by O notation.

    More specifically: ["Knuth did a comparison between Fibonacci heap and binary heaps for minimum spanning trees back in 1993 for his book _Stanford GraphBase_. He found Fibonacci to be 30 to 60% slower than binary heaps at the graph sizes he was testing, 128 vertices at different densities."](https://stackoverflow.com/questions/504823/has-anyone-actually-implemented-a-fibonacci-heap-efficiently)

    On the [Coppersmith-Winograd algorithm](!Wikipedia):

    > The Coppersmith-Winograd algorithm is frequently used as a building block in other algorithms to prove theoretical time bounds. However, unlike the Strassen algorithm, it is not used in practice because it only provides an advantage for matrices so large that they cannot be processed by modern hardware.[6]

    Some algorithms are particularly infamous for their excellent asymptotics but abysmal constant factors, such as the computable versions of AIXI. Lipton dubs such algorithms "[galactic algorithms](https://rjlipton.wordpress.com/2010/10/23/galactic-algorithms/)".

### Are all returns linear?

Premise 4 is where the argument starts trying to tie statements about complexity to real-world consequences.

Aside from the issue that the complexity claims are probably false, this one is particularly questionable: small advantages on a task *do* translate to large real-world consequences, particularly in competitive settings.
A horse or an athlete wins a race by a fraction of a second; a stock-market investing edge of 1% annually is worth a billionaire's fortune; a slight advantage in picking each move in a game likes chess translates to almost certain victory (consider how [AlphaGo](!Wikipedia)'s ranking changed with [small improvements in the CNN's ability to predict next moves](https://gogameguru.com/i/2016/03/deepmind-mastering-go.pdf "'Mastering the Game of Go with Deep Neural Networks and Tree Search', Silver et al 2016")); a logistics/shipping company which could shave the remaining 1-2% of inefficiency off its planning algorithms would have a major advantage over its rivals inasmuch as shipping is one their major costs & the profit margin of such companies is itself only a few percentage points of revenue; or consider [network effects](!Wikipedia) & winner-take-all markets.

Turning to human intelligence, the absolute range of human intelligence is very small: differences in reaction times are small, [backwards digit spans](!Wikipedia) range from 3-7, brain imaging studies have difficulty spotting neurological differences, the absolute genetic influence on intelligence is [on net minimal](Embryo selection#limits-to-iterated-selection), and this narrow range may be a general phenomenon about humans ([Wechsler 1935](https://catalog.hathitrust.org/Record/000385143 "The Range of Human Capacities")); and yet, in human society, [how critical](/iq) are these tiny absolute differences in determining who will become rich or poor, who will become a criminal, who will do cutting-edge scientific research, who will get into the Ivy Leagues, who will be a successful politician, and this holds true as high as IQ can be measured reliably (see [TIP/SMPY etc](Embryo selection#societal-effects)).
(I think this narrowness of objective performance may help explain why some events surprise a lot of observers: when we look at entities below the human performance window, we just see it as a uniform 'bad' level of performance, we can't see any meaningful differences and can't see any trends, so our predictions tend to be hilariously optimistic or pessimistic based on our prior views; then, when they finally enter the human performance window, we can finally apply our existing expertise and become surprised and optimistic, and then the entities can with small objective increases in performance move out of the human window entirely and it becomes an activity humans are now uncompetitive at like chess (because even grandmasters are constantly making mistakes[^Cowen-chess-mistakes]) but may still contribute a bit on the margin in things like [advanced chess](!Wikipedia), and within a few years, becomes truly superhuman.)

[^Cowen-chess-mistakes]: _[Average is Over](!Wikipedia)_, [Cowen](!Wikipedia "Tyler Cowen") 2013:

    > Vasik Rajlich, the programmer behind [Rybka](!Wikipedia), gives a more pessimistic spin to what we have learned from the chess-playing programs. In Rajlich's view, the striking fact about chess is how hard it is for humans to play it well. The output from the programs shows that we are making mistakes on a very large number of moves. Ken's measures show that even top grandmasters, except at the very peaks of their performances, are fortunate to match Rybka's recommendations 55% of the time. When I compare a grandmaster game to an ongoing Rybka evaluation, what I see is an initial position of value being squandered away by blunders-if only small ones-again and again and again. It's a bit depressing. Rajlich stresses that humans blunder constantly, that it is hard to be objective, hard to keep concentrating, and hard to calculate a large number of variations with exactness. He is not talking here about the club patzer but rather the top grandmasters: "I am surprised how far they are from perfection." In earlier times these grandmasters had a kind of aura about them among the chess-viewing public, but in the days of the programs the top grandmasters now command less respect. When a world-class player plays a club expert, the world-class player looks brilliant and invincible at the board. Indeed, the world-class player does choose a lot of very good moves. At some point his superior position starts "playing itself," to use an expression from the chess world, and just about everything falls into place. When the same world-class player takes on Shredder, to select an appropriately named program, he seems more like a hapless fool who must exert great care to keep the situation under control at all. And yet it is the very same player. That gap-between our perception of superior human intellect and its actual reality-is the sobering lesson of the programs.

This also ignores the many potential advantages of AIs which have nothing to do with computational complexity; AlphaGo may confront the same [PSPACE scaling wall](!Wikipedia "Go and mathematics#Computational complexity") that human Go players do, but as software it is immortal and can be continuously improved, among other advantages ([Sotala 2012](http://philpapers.org/archive/SOTAOA.pdf "Advantages of artificial intelligences, uploads, and digital minds"), [Yudkowsky 2013](https://intelligence.org/files/IEM.pdf "Intelligence Explosion Microeconomics")).

### Are all scenarios one-shot?

Premise 5 would seem to assume that there is no such thing as compound interest or exponential growth or that small advantages can accumulate to become crushing ones; which of course there is for companies, countries, and individuals alike.

Something similar has been noted about human intelligence - while any particular day-to-day decision has little to do with intelligence, the effects of intelligence are consistently beneficial and accumulate over a lifetime, so the random noise starts to cancel out, and intelligence is seen to have strong correlations with long-term outcomes (eg [Gottfredson 1997](http://www.udel.edu/educ/gottfredson/reprints/1997whygmatters.pdf "Why _g_ Matters: The Complexity of Everyday Life")).
More abstractly, many career or intellectual outcomes have been noticed to follow a roughly [log-normal distribution](!Wikipedia) ([Shockley 1957](/docs/1957-shockley.pdf "On the Statistics of Individual Variations of Productivity in Research Laboratories"), [Murray 2003](/docs/2003-murray-human-accomplishment.pdf "Human Accomplishment")); a log-normal distributed can be generated when an outcome is caused by a 'leaky pipeline' (scientific output might be due to motivation times intelligence times creativity...), in which case a small improvement on each variable can yield a large improvement in the output.[^Shockley-example]
For repeated actions, advantages can build up.  A chess example: [Magnus Carlsen](!Wikipedia) may be the strongest human chess player in history, with a peak [ELO](!Wikipedia "Elo rating system") rating of ~2882; as of 2016, the top-rated [chess engine](!Wikipedia) is probably Komodo at ELO 3358. The ELO expected score formula implies that if Komodo 2016 played peak Carlsen, it would have an expected score of $\frac{1}{1 + 10^{\frac{(2882-3358)}{400}}} = 0.94$, so it would win ~94% of its games; this is impressive enough (it would lose only 1 time out of 20), however, in the standard 5-game match, it would win not 94%, but ~99.8% of the 5-game matches (losing only 1 time out of *500*).
One thinks of Amara's law: "We tend to overestimate the effect of a technology in the short run and underestimate the effect in the long run."

[^Shockley-example]: Shockley notes that with 8 variables and an advantage of 50%, the output under a log-normal model would be increased by as much as 25x:

    ~~~{.R}
    simulateLogNormal <- function(advantage, n.variables, iters=100000) {
    regular <- 1
    advantaged <- replicate(iters, Reduce(`*`, rnorm(n.variables, mean=(1+advantage), sd=1), 1))
    ma <- mean(advantaged)
    return(ma)
    }
    simulateLogNormal(0.5, 8)
    # [1] 25.58716574
    ~~~

    With more variables, the output difference would be larger still, and is connected to the [o-ring theory of productivity](!Wikipedia). This poses a challenge to those who expect small differences in ability to lead to small output differences, as the log-normal distribution is common in the real world, and also implies that hybrid human-AI systems are unstable as the human will quickly become a bottleneck.

<!-- TODO ping Aaronson? -->

### Are AI agents rare?

Expanding on the observation that AIs can have advantages which are unrelated to computational complexity or solving larger instances of problems, is it really the case that a Singularity can happen *only* if AIs are able to surpass humans on particular algorithmic tasks?
This is unlikely. For example, in a [whole-brain emulation](!Wikipedia) scenario ([Sandberg & Bostrom 2008](http://www.fhi.ox.ac.uk/wp-content/uploads/brain-emulation-roadmap-report1.pdf "Whole Brain Emulation: A Roadmap")), such uploaded minds would not necessarily be gifted with any new accomplishments with regard to complexity classes but can we really swallow the argument's conclusion that this scenario simply *cannot* lead to any major changes worthy of the name Singularity?
Far from it - it seems that such an achievement would radically change human society in a number of ways, ranging from redefining mortality to accelerating neuroscience to transformations of the economy as such emulated brains can be copied indefinitely onto as much hardware as is available (consider [Robin Hanson](!Wikipedia)'s extrapolations in [_Age of Em_](http://ageofem.com/)).
It would be surprising if one could run human-level intelligences (perhaps arbitrarily many of them) on a pocket smartphone, with millions or billions of them across the world, even outnumbering regular biological humans, and still have no 'Singularity' through sheer numbers.

## Parable of the Worms

Once upon a time, two _[C. elegans](!Wikipedia)_ were debating the prospect of "transwormism", and specifically the possibility of hypothetical creatures from elsewhere in the space of all possible organisms, which might exceed worms by as much as worms exceed the bacteria they thrive upon.

Crawlviati argues for transwormism:

> "There is no a priori reason to believe that worms must be the pinnacle of creation, and that there are no larger or more complex or more intelligent organisms possible.
> We should be applying the Poopernican Principle here - the manure piles in which we live are but a tiny segment of the universe in both space and time, of no privileged perspective, and so in the Great Chain of Eating, we should expect us to be in a mediocre position.
> Indeed, from the start of life, we can see many breakthroughs: multicellular life has produced endless forms most different from unicellular life, and fairly recently have neurons been invented, so we're at an exciting point in time.
> We can speculate about the possibilities: a transworm might be completely different from us worms; or it might be similar in architecture to us worms, perhaps with a much longer body with many more neurons and so much smarter than us.
> Regardless, a transworm would be difficult for us to predict, and may be able to develop very fast as it learns new ways of hunting bacteria and self-fertilization, in what we might call a Moundularity in which it piles up resources and offspring faster than anyone else; inasmuch as a transworm may have very different priorities from us and change the environment to fit its needs, it would be dangerous to us."

Slimeplicius disagrees:

> "Ridiculous! Think for a moment about your claims.
> We are blessed with 302 neurons, with which we can react to stimuli, move forward, move backward, hunt bacteria, and solve challenging navigational puzzles of many worm-lengths.
> But these problems exhibit diminishing returns - optimal maze navigation is exponentially difficult, for example.
> Transworms would immediately find their additional cognition to be of ever less marginal value as they run up against the wall of wormputational complexity.
> What would they do with, say, 1000 neurons that would justify a metabolic cost of *over 3 times more*?
> And to be truly worthy of the name transworm, they might need 10s of thousands, or even millions of neurons!
> Consider the absurdity of such an architecture: could our manure pile support a single such transworm?
> Where would the food come from? For that matter, how would its body support so many neurons?
> And its genes could no longer specify cell placement one by one, but organization would have to somehow 'emerge' or be 'learned', which you are rather obscure about how this might happen.
> Not to mention the many problems you seem to gloss over in your faith in progress: for example, diffusion would no longer work to feed each cell, requiring novel mechanisms to move fluids around.
> If a transworm *could* exist, it would be exponentially difficult for it to eat bacteria and reproduce faster than regular worms, and its performance would likely converge with ours: it would solve our problems only slightly better than us, at tremendously increased cost.
> (We could even make an evolutionary argument: we have evolved to be as smart as is optimal in our niche - and no more or less.)
> Certainly, any Moundularity would be so slow us worms would smell it coming long in advance and wriggle together in a big ball to crush it."
>
Crawlviati:

> "Your argument seems very narrow to me.
> Yes, I agree that it would be difficult to support so many neurons packed together in one worm, but I'm sure the engineering difficulties can be overcome - there seems to be no fundamental limit to wormputation much greater than 302 neurons, so there must be a way.
> And your food objection is likewise soluble: perhaps transworms can migrate from compost pile to garden to compost pile regularly as they exhaust resources in each one, or even figure out some way to easily knock down low-hanging fruit & let them rot.
> They may not, bacterium for bacterium or cell for cell, be as efficient as us, but that doesn't matter as long as the diminishing returns don't turn into *negative* returns.
> As long as the returns are positive, they will be able to pay for their increased resource utilization and continue climbing up the exponential curves.
> And what does 'better' even mean here? The wormputational complexity of a maze may increase sharply with maze size, but that's a statement about mazes, not about comparing maze-solvers, which might be arbitrarily better or worse than each other, so there's a problem: maybe they could solve mazes 100x faster.
> Then there's figuring out what any bit of performance means: if a transworm could solve mazes twice as fast as you or I, maybe it'll get *all* the treats when it beats us in a race to the end, and not less than twice as much.
> Heck, we're *worms* - what do we know about the world? Maybe there are all sorts of cool things which can be done, besides stimulus, response, stimulus, response, stimulus, response - if we could just *think* for once in our lives!"

Slimeplicius:

> "These claims seem to rest entirely on what I might call an appeal to ignorance: maybe mazes can be run faster than we can, maybe there are great things which could be done with more neurons, maybe there's lots of food we can't obtain but could with more intelligence...
> Sure, maybe I can't prove that there aren't, but is any of this what a reasonable worm, the ordinary worm in the dirt, would believe? Certainly not."
>
Crawlviati:

> "If you'll just think a little more about the possibilities..."

Slimeplicius:

> "There are better things to worry about, like pile warming. What if our wastes and their decay make our mound too hot for us? We should discuss that instead."

So they did.
A week later, the farm was sold to a developer, and the mound was rolled flat, and then paved over with asphalt - the construction workers neither hated nor loved the worms, but they were made of atoms useful for stabilizing the road to the townhouses being built there.

# See also

- [Embryo selection]()
- [Simulation inferences]()
- [Algernon's law](/Drug heuristics)
