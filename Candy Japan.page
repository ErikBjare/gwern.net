---
title: Candy Japan's new box A/B test
description: Bayesian decision-theoretic analysis of subscription cancellation experiment
tags: statistics, decision theory
created: 6 May 2016
status: finished
belief: likely
...

> I analyze an A/B test from a mail-order company of two different kinds of box packaging from a Bayesian decision-theory perspective, balancing posterior probability of improvements & greater profit against the cost of packaging & risk of worse results, finding that as the company's analysis suggested, the new box is unlikely to be sufficiently better than the old and, calculating expected values of information, not worth experimenting on further.

# Background

[Candy Japan](https://www.candyjapan.com/) is a small mail-order company which, since 2011, sends semi-monthly small packages of Japanese candies & novelty foods from Japan to subscribers typically in the West.
While mail-order subscriptions services such as tea or chocolate of the month clubs are not rare, Candy Japan publishes unusually detailed blog posts discussing their business, such as [how they were nearly killed by credit card fraud](https://www.candyjapan.com/how-i-got-credit-card-fraud-somewhat-under-control "How Candy Japan got credit card fraud somewhat under control"), [pricing](http://www.bemmu.com/how-i-decided-the-price-for-my-japanese-candy "How I decided the price for my Japanese candy subscription service"), [their overhead](https://www.reddit.com/r/startups/comments/44lgr7/running_costs_for_candy_japan/), [(non) sales from YouTube stardom](https://www.candyjapan.com/sales-results-from-getting-3-million-views-on-youtube "Sales results from getting 3 million views on YouTube"), [life as an expat in Japan](http://www.bemmu.com/two-years-of-candy), and annual summaries (eg [2012](http://www.bemmu.com/first-year-of-candy-japan "First year of Candy Japan"), [2013](http://www.candyjapan.com/2013-year-in-review), [2014](http://www.candyjapan.com/2014-year-in-review), [2015](http://www.candyjapan.com/2015-year-in-review)), which are [often discussed on Hacker News](https://hn.algolia.com/?query=%22Candy%20Japan%22&sort=byPopularity&prefix&page=0&dateRange=all&type=all).

# Box A/B test

In 2016, [CJ ran an A/B test](https://www.candyjapan.com/results-from-box-design-ab-test) comparing subscription cancellation rates of customers sent candy in the standard undecorated box, and customers sent candy in prettier but more expensive boxes:

> Plain unbranded boxes go for just \$0.34 a piece, while a box with a full-color illustration printed on the cover costs almost twice as much: \$0.67. This may not seem like such a big difference, but in absolute terms using the new box means around \$500 less profit per month or roughly 10% of profit margin.
>
> In group A 38.27%, or 168 of the 439 customers receiving the new package design canceled during the test. In group B 39.59%, or 175 of the 442 customers receiving the old package design canceled during the test. This is not a statistically significant difference. In a world where it makes no difference which package is sent, you would get a result as significant as this 80% of the time.

(These cancellation rates strike me as bizarrely high: almost half of CJ's customers are so unhappy they cancel their first month? Is there an issue with selecting bad candy, or are there unrealistic expectations of how much candy can be bought for the subscription price of \$25/month, or do people not like semi-monthly delivery, or not like the limit on candy imposed by two deliveries rather than one, or do people only want to try it once, or what? Looking through early CJ posts, I see these issues, as well as other potential problems like not documenting past shipments adequately, have been raised repeatedly by commenters but never addressed or experimented on by CJ. Have any surveys been done to find out why the cancellation rate is so high? But moving on.)

CJ has here a straightforward decision problem: should it switch to the decorated boxes or not, based on the information provided by this randomized experiment? After all, the decorated boxes *did* perform slightly better than the undecorated boxes.

# Analysis
## NHST

The most conventional approach here would be what CJ did: treat this as a 2-sample test of proportions, or a binomial regression, and compute a _p_-value for the treatment; if the _p_-value of a decreased cancellation rate is smaller than the arbitrary threshold 0.05, switch to the new boxes.

We can easily replicate CJ's analysis given the provided percentages and _n_s (the sufficient statistics in this case):

~~~{.R}
prop.test(c(175, 168), c(442, 439))
#
#   2-sample test for equality of proportions with continuity correction
#
# data:  c(175, 168) out of c(442, 439)
# X-squared = 0.11147052, df = 1, p-value = 0.7384761
# alternative hypothesis: two.sided
# 95% confidence interval:
#  -0.05341862656  0.07989797596
# sample estimates:
#       prop 1       prop 2
# 0.3959276018 0.3826879271
~~~

So CJ's _p_-value matches mine. Arguably, a one-tailed test here is appropriate since you are only interested in whether the new box has a better cancellation rate, which would be expected to halve the _p_-value, as it does:

~~~{.R}
prop.test(c(175, 168), c(442, 439), alternative="greater")
#
#   2-sample test for equality of proportions with continuity correction
#
# data:  c(175, 168) out of c(442, 439)
# X-squared = 0.11147052, df = 1, p-value = 0.3692381
# alternative hypothesis: greater
# 95% confidence interval:
#  -0.04306671907  1.00000000000
# sample estimates:
#       prop 1       prop 2
# 0.3959276018 0.3826879271
~~~

But of course, what does a _p_-value mean, anyway? CJ [correctly](!Wikipedia "P-value#Misunderstandings") interprets what they do mean ("In a world where it makes no difference which package is sent, you would get a result as statistically-significant as this 80% of the time."), but that doesn't give us much help in understanding if we *are* in the world where which box is sent *does* make an important difference and how *much* of a difference.
_p_-values are an unreliable criterion for decisions: the result might be consistent with informative priors about the possible effect of box improvements; one might have compelling reason from earlier experiments to decide to switch to the new box; one might prefer the new boxes for other reasons; the new boxes might be profitable despite weak evidence (for example, if they cost as much); the result might be promising enough to justify further experimentation...

## Bayesian

It would be better to do a more informed Bayesian decision analysis including our knowledge that improvements should be small, giving a concrete probability that there is an improvement and if there is, how large it is - letting us calculate the probability that the switch would be profitable, and the profitability of further experimentation.

### Uninformative

Switching over to a Bayesian approach, using [BayesianFirstAid](https://github.com/rasmusab/bayesian_first_aid) gives us a `bayes.prop.test` function with the exact same interface as `prop.test` using a highly uninformative `beta(1,1)` prior (effectively a flat prior implying that every probability from 0-1 is equally likely):

~~~{.R}
library(BayesianFirstAid)
fit <- bayes.prop.test(c(175, 168), c(442, 439)); fit
#
#   Bayesian First Aid proportion test
#
# data: c(175, 168) out of c(442, 439)
# number of successes:  175, 168
# number of trials:     442, 439
# Estimated relative frequency of success [95% credible interval]:
#   Group 1: 0.40 [0.35, 0.44]
#   Group 2: 0.38 [0.34, 0.43]
# Estimated group difference (Group 1 - Group 2):
#   0.01 [-0.051, 0.078]
# The relative frequency of success is larger for Group 1 by a probability
# of 0.651 and larger for Group 2 by a probability of 0.349 .
~~~

(BayesianFirstAid is overkill in this case as it calls out to JAGS for MCMC, but the binomial/proportion test uses the famously tractable [beta distribution](!Wikipedia) where we can do the Bayesian update as simply as `Beta(1,1)` ~> `Beta(1+175, 1+442)` vs `Beta(1+168, 1+439)` for the two groups; the overlap can be found analytically or using `dbeta` or simulated using `rbeta`.)

### Informative

It's often said that a null-hypothesis significance test is similar to Bayesian inference with flat priors, so our _p_-value winds up having a suspicious similarity to our posterior probability that the new box helped (which is _P_=0.651).
Of course, we have prior knowledge here: A/B tests, or switching boxes, cannot possibly lead to cancellation rates as high as 100% or as low as 0%, and in fact, it would be shocking if the usual 39% cancellation rate could be changed by more than a few percentage points; even ±5% would be surprising (but important).

A more realistic prior than `beta(1,1)` would have a mean of 0.39 and a distribution narrow enough that, say, 95% of it falls within ±5% (34-44).
We can come up with a beta prior which encodes this belief.
The mean of our beta distribution will be $0.39 = \frac{a}{a+b}$ which can be rearranged to $b=1.5461 \cdot a$; to solve for an _a_ which gives the desired ±5% 95% CI, I looked at quantiles of random samples by increasing _b_ until it is adequately narrow (there's probably an analytic equation here but I didn't bother looking it up):

~~~{.R}
a <- 900; quantile(rbeta(100000, a, 1.5642*a))
#           0%          25%          50%          75%         100%
# 0.3496646950 0.3831142015 0.3899169957 0.3967543674 0.4365377333
a; 1.5642*a
# [1] 900
# [1] 1407.78
~~~

BayesianFirstAid [does not support user-specified priors](https://github.com/rasmusab/bayesian_first_aid/issues/13) but it does make it relatively easy to incorporate any change we wish by printing out all the code we need to run it in the form of the R boilerplate and the JAGS model.
We locate the `dbeta(1,1)` line and replace it with `dbeta(900, 1407)`, and add in a convenience line `theta_diff <- theta[1] - theta[2]` which reports directly on what we care about (how much the new box decreases the cancellation rate):

~~~{.R}
model.code(fit)
# ...
## copy, edit, paste:
require(rjags)
x <- c(175, 168)
n <- c(442, 439)
model_string <- "model {
  for(i in 1:length(x)) {
    x[i] ~ dbinom(theta[i], n[i])
    theta[i] ~ dbeta(900, 1407)
    x_pred[i] ~ dbinom(theta[i], n[i])
  }
  theta_diff <- theta[1] - theta[2]
}"
model <- jags.model(textConnection(model_string), data = list(x = x, n = n),
                    n.chains = getOption("mc.cores"))
samples <- coda.samples(model, c("theta", "x_pred", "theta_diff"), n.iter=10000)
summary(samples)
# 1. Empirical mean and standard deviation for each variable,
#    plus standard error of the mean:
#
#                    Mean           SD     Naive SE Time-series SE
# theta[1]   3.910082e-01  0.009262463 3.274775e-05   3.279952e-05
# theta[2]   3.889790e-01  0.009300708 3.288297e-05   3.270599e-05
# theta_diff 2.029166e-03  0.013132415 4.643010e-05   4.614718e-05
# x_pred[1]  1.727948e+02 11.062655213 3.911239e-02   3.908579e-02
# x_pred[2]  1.707583e+02 10.938572590 3.867369e-02   3.840689e-02
#
# 2. Quantiles for each variable:
#
#                    2.5%           25%          50%          75%        97.5%
# theta[1]     0.37298246   0.384715477 3.909551e-01   0.39720057   0.40937636
# theta[2]     0.37083603   0.382659446 3.889795e-01   0.39532425   0.40718608
# theta_diff  -0.02381196  -0.006805355 1.998807e-03   0.01086074   0.02785733
# x_pred[1]  151.00000000 165.000000000 1.730000e+02 180.00000000 195.00000000
# x_pred[2]  149.00000000 163.000000000 1.710000e+02 178.00000000 192.00000000
posterior <-  (samples[1][[1]])[,3]
sum(posterior > 0) / length(posterior)
# [1] 0.5623
~~~

So a much more informative prior reduces the posterior probability that the new box reduced the cancellation rate to _P_=0.5623.

# Decision
## Benefit

Since, while small, this is still >50%, it is possible that switching to the new box is a good idea, but we will need to consider costs and benefits to turn our posterior estimate of the reduction into a posterior distribution of gains/losses.

What is the value of a reduction in cancellations?

CJ says that it has a profit margin of ~\$5000 on the $439+442=881$ customers (881 is consistent with the [totals reported in an earlier blog post](https://www.candyjapan.com/candy-japan-crosses-10000-mrr "Candy Japan crosses $10000 MRR")) or ~\$5.7 profit per customer per month.
So a reduction from 0.3909 to 0.3847 is 4.96 customers staying, each of whom is worth \$5.7, so a gain of \$28.7 per month.

What is the *total* benefit? Well, presumably it increases the lifetime value of a customer: the total they spend, and hence your amount of profit on them. If you profit \$1 per month on a customer and they stay 1 year before canceling, you make \$12; but if they stay 2 years on average, you'd make \$24, which is better.

If a 39% cancellation rate per month is typical, then a customer will stick around for an average of 2.56 months (this is a [geometric distribution](!Wikipedia) with _p_=0.39, whose mean is $\frac{1}{p}=\frac{1}{0.39}=2.56$), for a lifetime value of $5.7 \cdot \frac{1}{0.39} = 14.59$.
Then a gain is the difference between two lifetime values; so if we could somehow reduce cancellation rates by 10% points for free, the gain per customer would be (new lifetime value - old lifetime value) - cost:

~~~{.R}
((5.7 * (1/(0.39-0.10)))  - (5.7 * (1/0.39))) - 0
# [1] 5.039787798
~~~

Which over a batch 881 customers is a cool additional \$13553.

## Cost-benefit

We don't have a 10% point reduction in hand, but we do have a posterior distribution of reduction estimates.

So to transform the posterior distribution of cancellation decreases, each sample of our posterior distribution is run through the formula:

~~~{.R}
gain <- sapply(posterior, function(r) { ((5.7 * (1/(0.39-r)))  - (5.7 * (1/0.39))) } )
summary(gain)
#        Min.     1st Qu.      Median        Mean     3rd Qu.        Max.
# -1.64456500 -0.25657610  0.07018243  0.09104651  0.41660780  2.24548300
~~~

In this case, we estimate that rolling out the new box would increase per customer lifetime value by an average of \$0.09.
The new boxes cost \$0.33 more per month than the old boxes, and a customer with a new box lasts 2.57 months, for a total box cost of 0.33 * 2.57 = \$0.84  so their cost exceeds their benefit by a factor of 10.
To justify spending on the new box, we need a reduction of at least ~2.26% in the cancellation rate down to ~36.7%:

~~~{.R}
lifetimeGain <- function(r) {
    old_lifetime = 1/0.39
    new_lifetime = 1/(0.39-r)
    old_revenue = old_lifetime * 5.7
    new_revenue = new_lifetime * 5.7
    cost = new_lifetime * 0.33
    return(new_revenue - cost - old_revenue) }
lifetimeGain(0.20)
# [1] 13.64777328
lifetimeGain(0.05)
# [1] 1.178733032
lifetimeGain(0.0226)
# [1] 0.0008374858674
~~~

If we were absolutely certain that the reduction was as large as 2.26%, then the new boxes would break even. How probable is it that the decrease in cancellation rate is as or larger? Improbable, only ~6%:

~~~{.R}
sum(posterior >= 0.0226) / length(posterior)
# [1] 0.0591
~~~

## Value of Information
### Expected Value of Perfect Information (EVPI)

Still, 6% is not totally negligible. There is still a chance that we are making a mistake in not using the new boxes.
Are the results favorable enough to justify an additional month or two of A/B testing?

This gets into the "[expected value of perfect information](!Wikipedia)" (EVPI): how valuable would be a definitive answer to the question of whether the decrease is better than 2.26%?
Would we be willing to pay \$10 or \$100 or \$1000 for an oracle's answer?

In 94% of the cases, we believe the answer would be 'no', the oracle is worth nothing since it could only tell us what we already believe (that the decrease is less than 2.26%), in which case we are no better off, so in those cases the oracle is worth \$0; in 6% of the cases, the answer would be 'yes'.
But in that case, our gain depends on how large the cancellation reduction is - if it's exactly 2.27%, the gain is almost \$0 so we care little, but if the gain is 4% or higher, then we would have been leaving real money on the table.
So we need to go back to the posterior distribution to weight our gains per customer by the possible reductions:

~~~{.R}
mean(sapply(posterior, function(r) { if(r<0.0226) { 0; } else { lifetimeGain(r) } } ))
# [1] 0.01346836819
0.01346836819 * 881
# [1] 11.86563238
~~~

So because of the small possibility of a slightly profitable box, we'd be willing to pay up to a grand total of \$12 for certain information.

\$12 would be enough to pay for 36 new boxes; used as part of another A/B test (using, say, 845 old boxes for comparison), we can intuit it would not provide anything near certainty[^power], so we can safely guess that further experimentation is not profitable.

[^power]: The difference between 36 boxes whose customers cancel at 39%, and 36 boxes whose customers cancel at 39%-2.26%, is 14 cancellations vs 13 cancellations on average. Or to go back to NHST, a well-powered experiment to detect a difference between 39% and 36.74% would require _n_=14460 (_n_=7230 in each arm):

    ~~~{.R}
    power.prop.test(power=0.8, p1=0.39, p2=0.3674)

         Two-sample comparison of proportions power calculation

                  n = 7230.138793
                 p1 = 0.39
                 p2 = 0.3674
          sig.level = 0.05
              power = 0.8
        alternative = two.sided

    NOTE: n is number in *each* group
    ~~~

### Expected Value of Sample Information (EVSI)

How much would just 1 observation more be worth, the "[expected value of sample information](!Wikipedia)"?
We can estimate this by repeatedly simulating drawing 1 more observation, doing a Bayesian update, recalculating our expected profit, and seeing how much higher it is.

Looking at it on the monthly level, we could automate the analysis like so:

~~~{.R}
monthlyGain <- function(reduction, profit, cost) { (profit * reduction) - cost }
posteriorProfit <- function(x, n, monthlyProfit, monthlyCost) {
    model_string <- "model {\n  for(i in 1:length(x)) {\n    x[i] ~ dbinom(theta[i], n[i])
                     theta[i] ~ dbeta(900, 1407)\n
                     }\n  theta_diff <- theta[1] - theta[2]\n}"

    model <- jags.model(textConnection(model_string), data = list(x = x, n = n),
                        inits = list(theta=c(0.39,0.39)),
                        n.chains = getOption("mc.cores"), n.adapt=100)
    samples <- coda.samples(model, c("theta", "theta_diff"), n.iter=10000)

    posteriorReduction <-  (samples[1][[1]])[,3]

    gains <- sapply(posteriorReduction, function(r) { monthlyGain(r, monthlyProfit, monthlyCost) })
    return(list(Profit=gains, Reduction=posteriorReduction))
}
gains <- posteriorProfit(x=c(175, 168), n=c(442, 439), monthlyProfit=5.7, monthlyCost=0.33)
mean(gains$Profit)
# [1] -0.3176010559
~~~

So with the current data, we would suffer an average loss of \$0.31 per customer per month by switching to the new box.

It is easy to simulate collecting another datapoint: for the control group, flip a coin with 39% to decide whether the customer 'canceled' or not; for the experimental group, draw a possible reduction from the posterior distribution samples, add it to 39%, and then flip a coin.

~~~{.R}
simulateData <- function(control=TRUE, posterior) {
  if (control) { rbinom(1,1, prob=0.39) } else { rbinom(1,1, prob=(0.39 + sample(posterior, 1))) } }
~~~

Now we can repeatedly simulate fake data, add it to the real data, rerun the analysis, see how often an additional datapoint would change our decision, and whether the profit resulting from that justifies the cost of another datapoint (\$0.33).
For simplicity, I'll only simulate fake datapoints for the new boxes.

~~~{.R}
gains <- posteriorProfit(x=c(175, 168), n=c(442, 439), monthlyProfit=5.7, monthlyCost=0.33)
n_additional <- 5000
evsi <- replicate(100, { newdata <- replicate(n_additional, simulateData(control=FALSE, gains$Reduction))
                           simGains <- posteriorProfit(x=c(175, 168+sum(newdata))
                                                       n=c(442, 439+n_additional),
                                                       monthlyProfit=5.7, monthlyCost=0.33)
                           return(mean(simGains$Profit)) }
                           )
evsi
Filter(function(x){x>0}, evsi)
# numeric()
~~~

Indeed, the gain from 5000 additional datapoints turns out to be so little we have a hard time distinguishing it from zero, as expected from the very small EVPI, and certainly less than the cost of collecting that much additional data.
We can say that further experimentation is definitely not cost-effective.

# Conclusion

So to round up:

1. there is only a 56-65% (depending on priors) chance that the new boxes reduced cancellation
2. the plausible reductions are so small that the expected value of switching is negative and there is only ~6% chance that switching to new boxes would be profitable
3. a definitive resolution of the remaining uncertainty would be worth only ~\$12
4. further sampling is essentially worthless and should not be done
5. future experimentation should focus on the more important business issues CJ seems to have

# See Also

- [When Should I Check The Mail? Bayesian decision-theoretic analysis of mail delivery times](/Mail delivery)
