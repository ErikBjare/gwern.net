---
description: Miscellaneous statistical stuff
tags: statistics, decision theory
created: 17 July 2014
status: in progress
belief: possible
...

# Critiques

- moxibustion mouse study https://plus.google.com/103530621949492999968/posts/TisYM64ckLM
- criticism of teeth-removal experiment in rats http://lesswrong.com/r/discussion/lw/kfb/open_thread_30_june_2014_6_july_2014/b1u3
- criticism of small Noopept self-experiment http://www.bluelight.org/vb/threads/689936-My-Paper-quot-Noopept-amp-The-Placebo-Effect-quot?p=11910708&viewfull=1#post11910708
- why Soylent is not a good idea http://lesswrong.com/lw/hht/link_soylent_crowdfunding/90y7
- misinterpretation of fluoridation meta-analysis and ignorance of VoI http://theness.com/neurologicablog/index.php/anti-fluoride-propaganda-as-news/#comment-76400
- http://lesswrong.com/lw/1lt/case_study_melatonin/8mgf
- Fulltext: https://dl.dropboxusercontent.com/u/280585369/2014-dubal.pdf is this possible? http://nextbigfuture.com/2014/05/kl-vs-gene-makes-up-six-iq-points-of.html#comment-1376748788 http://www.reddit.com/r/Nootropics/comments/25233r/boost_your_iq_by_6_points/chddd7f
- Facebook emotion study: http://www.reddit.com/r/psychology/comments/29vg9j/no_emotions_arent_really_contagious_over_facebook/cip7ln5 https://plus.google.com/103530621949492999968/posts/1PqPdLyzXhn
- tACS causes lucid dreaming: http://www.reddit.com/r/LucidDreaming/comments/27y7n6/no_brain_stimulation_will_not_get_you_lucid/ck6isgo
- Herbalife growth patterns: http://www.reddit.com/r/business/comments/24aoo2/what_unsustainable_growth_looks_like_herbalife/ch5hwtv
- Plausible correlate of Fairtrade: http://www.reddit.com/r/Economics/comments/26jb2d/surprise_fairtrade_doesnt_benefit_the_poor/chrx9s4
- slave whippings vs cotton production http://lesswrong.com/r/discussion/lw/kwc/open_thread_sept_17_2014/bajv
- whether a study on mental illness & violence shows schizophrenics are not more likely to murder but rather be murdered: http://www.reddit.com/r/psychology/comments/2fwjs8/people_with_mental_illness_are_more_likely_to_be/ckdq50k / http://www.nationalelfservice.net/publication-types/observational-study/people-with-mental-illness-are-more-likely-to-be-victims-of-homicide-than-perpetrators-of-homicide/#comment-95507 (see also http://slatestarscratchpad.tumblr.com/post/120950150581/psycholar-giraffepoliceforce-museicetc )
- Fortune analysis of higher female CEO returns http://lesswrong.com/r/discussion/lw/l3b/contrarian_lw_views_and_their_economic/bftw
- failed attempt at estimating P(causation|correlation) https://plus.google.com/103530621949492999968/posts/UzMMmPgyyaV
- algae/IQ: http://lesswrong.com/r/discussion/lw/l9v/open_thread_nov_17_nov_23_2014/bm7o
- synaesthesia/IQ: https://www.reddit.com/r/psychology/comments/2mryte/surprising_iq_boost_12_in_average_by_a_training/cm760v8
- misinterpretation: http://slatestarcodex.com/2014/12/08/links-1214-come-ye-to-bethlinkhem/#comment-165197
- underpowered/multiple-correction jobs program: http://slatestarcodex.com/2014/12/08/links-1214-come-ye-to-bethlinkhem/#comment-165197
- vitamin D/caffeine claim based on weak in vitro claims, inconsistent with more relevant in vivo results: https://plus.google.com/u/0/103530621949492999968/posts/AUg3udezXMS [already included in Nootropics.page]
- claimed fall in digit span backwards minuscule and non-statistically-significant, no evidence of heterogeneity beyond variability due to sample size http://drjamesthompson.blogspot.com/2015/04/digit-span-bombshell.html?showComment=1428096775425#c4097303932864318518
- Claimed randomized experiment of whether sushi tastes worse after freezing is not actually a randomized experiment https://www.reddit.com/r/science/comments/324xmf/randomized_doubleblind_study_shows_the_quality_of/cq8dmsb
- aerobic vs weightlifting exercise, multiple problems but primarily p-hacking, difference-in-statistical-significance-is-not-a-significant-difference, and controlling for intermediate variable: https://plus.google.com/103530621949492999968/posts/aeZqB8JWUiQ
- sexual openness result undermined by ceiling effect http://mindhacks.com/2015/04/28/when-society-isnt-judging-womens-sex-drive-rivals-mens/#comment-362749
- music study claiming WM interaction: possible ceiling effect? see FB PM
- attempt to measure effect of Nazi eugenics program failed to use breeder's equation to estimate possible size of effect, which is too small to detect with available data and hence attempt is foredoomed: https://www.reddit.com/r/eugenics/comments/3hqdll/between_73_and_100_of_all_individuals_with/cul2nzw
- claim high IQ types almost 100% failure rates due to inappropriate model assumption of normal distribution with very narrow standard deviation: http://polymatharchives.blogspot.com/2015/01/the-inappropriately-excluded.html?showComment=1441741719623#c1407914596750199739

# Estimating censored test scores

An acquaintance asks the following question: he is applying for a university course which requires a certain minimum score on a test for admittance, and wonders about his chances and a possible trend of increasing minimum scores over time.
(He hasn't received his test results yet.)
The university doesn't provide a distribution of admittee scores, but it does provide the minimum scores for 2005-2013, unless all applicants were admitted because they all scored above an unknown cutoff - in which case it provides no minimum score.
This leads to the dataset:

~~~{.R}
2005,NA
2006,410
2007,NA
2008,NA
2009,398
2010,407
2011,417
2012,NA
2013,NA
~~~

A quick eyeball tells us that we can't conclude much: only 4 actual datapoints, with 5 hidden from us.
We can't hope to conclude anything about time trends, other than there doesn't seem to be much of one: the last score, 417, is not much higher than 410, and the last two scores are low enough to be hidden.
We might be able to estimate a mean, though.

We can't *simply* average the 4 scores and conclude the mean minimum is 410 because of those NAs: a number of scores have been 'censored' because they were too low, and while we don't know what they were, we do know they were \<398 (the smallest score) and so a bunch of \<398s will pull down the uncensored mean of 410.

On approach is to treat it as a [Tobit model](!Wikipedia) and estimate using something like the [`censReg`](http://cran.r-project.org/package=censReg) library ([overview](http://cran.r-project.org/web/packages/censReg/vignettes/censReg.pdf)).

But if we try a quick call to `censReg`, we are confounded: a Tobit model expects you to provide the cutoff below which the observations were censored, but that is something we don't know.
All we know is that it must be below 398, we weren't told it was exactly 395, 394, etc.
Fortunately, this is a solved problem. For example: ["The Tobit model with a non-zero threshold"](http://econweb.ucsd.edu/~rcarson/papers/TobitEJ07.pdf), Carson & Sun 2007 tells us:

> In this paper, we consider estimating the unknown censoring threshold by the minimum of the uncensored $y_i$'s. We show that the estimator $γ'$ of $γ$ is superconsistent and asymptotically exponentially distributed. Carson (1988, 1989) also suggests estimating the unknown censoring threshold by the minimum of the uncensored $y_i$'s. In a recent paper, [Zuehlke (2003)](/docs/statistics/2003-zuehlke.pdf "Estimation of a Tobit model with unknown censoring threshold") rediscovers these unpublished results and demonstrates via simulations that the asymptotic distribution of the maximum likelihood estimator does not seem to be affected by the estimation of the censoring threshold.

That seems to be almost *too* simple and easy, but it makes sense and reminds me a little of the [German tank problem](!Wikipedia): the minimum might not be that accurate a guess (it's unlikely you just happened to draw a sample right on the censoring threshold) and it definitely can't be wrong in the sense of being too low. (A Bayesian method might be able to do better with a prior like a exponential.)

With that settled, the analysis is straightforward: load the data, figure out the minimum score, set the NAs to 0, regress, and extract the model estimates for each year:

~~~{.R}
scores <- data.frame(Year=2005:2013,
                     MinimumScore=c(NA,410,NA,NA,398,407,417,NA,NA));
censorThreshold <- min(scores$MinimumScore, na.rm=T)
scores[is.na(scores)] <- 0

library(censReg)
# 'censorThreshold-1' because censReg seems to treat threshold as < and not <=
summary(censReg(MinimumScore ~ Year, left=censorThreshold-1, data=scores))
# Warning message:
# In censReg(MinimumScore ~ Year, left = censorThreshold - 1, data = scores) :
#   at least one value of the endogenous variable is smaller than the left limit
#
# Call:
# censReg(formula = MinimumScore ~ Year, left = censorThreshold -
#     1, data = scores)
#
# Observations:
#          Total  Left-censored     Uncensored Right-censored
#              9              5              4              0
#
# Coefficients:
#              Estimate Std. error t value Pr(> t)
# (Intercept) -139.9711        Inf       0       1
# Year           0.2666        Inf       0       1
# logSigma       2.6020        Inf       0       1
#
# Newton-Raphson maximisation, 37 iterations
# Return code 1: gradient close to zero
# Log-likelihood: -19.35 on 3 Df
-139.9711 + (0.2666 * scores$Year)
# [1] 394.6 394.8 395.1 395.4 395.6 395.9 396.2 396.4 396.7
~~~

With so little data the results aren't very reliable, but there is one observation we can make.

The fact that half the dataset is censored tells us that the uncensored mean may be a *huge* overestimate (since we're only looking at the 'top half' of the underlying data), and indeed it is.
The original mean of the uncensored scores was 410; however, the estimate including the censored data is much lower, 397 (*13* less)!

This demonstrates the danger of ignoring systematic biases in your data.

So, trying to calculate a mean or time effect is not helpful.
What might be better is to instead exploit the censoring directly: if the censoring happened because *everyone* got in, then if you showed up in a censored year, you have 100% chance of getting in; while in a non-censored year you have an unknown but \<100% chance of getting in; so the probability of a censored year sets a lower bound on one's chances, and this is easy to calculate as a simple binomial problem - 5 out of 9 years were censored years, so:

~~~{.R}
binom.test(c(5,4))
#
#   Exact binomial test
#
# data:  c(5, 4)
# number of successes = 5, number of trials = 9, p-value = 1
# alternative hypothesis: true probability of success is not equal to 0.5
# 95% confidence interval:
#  0.212 0.863
# sample estimates:
# probability of success
#                 0.5556
~~~

So we can tell him that he may have a \>55% chance of getting in.

# The Traveling Gerontologist problem

A quick probability exercise: [Wikipedia](!Wikipedia "Centenarian#Centenarian_populations_by_country") mentions Finland has 566 centenarians as of 2010.

That's few enough you could imagine visiting them all to research them and their longevity, in a sort of traveling salesman problem but with gerontologists instead.
Except, because of the [exponential increase in mortality](https://en.wikipedia.org/wiki/Gompertz%E2%80%93Makeham_law_of_mortality), centenarians have high annual mortality rates; it depends on the exact age but you could call it >30% (eg Finnish 99yos in 2012 had a death toll of 326.54/1000).
So you might well try to visit a centenarian and discover they'd died before you got there.

How bad a risk is this?
Well, if the risk per year is 30%, then one has a 70% chance of surviving a year.
To survive a year, you must survive all 365 days; by the multiplication rule, the risk is $x$ where $0.7 = x \cdot x \cdot x \cdot ... * x \text{[365.25 times]}$ or $0.7 = x^{365.25}$; solving, $x = 0.999024$.

It takes time to visit a centenarian - it wouldn't do to be abrupt and see them for only a few minutes, you ought to listen to their stories, and you need to get to a hotel or airport, so let's assume you visit 1 centenarian per day.

If you visit centenarian A on day 1, and you want to visit centenarian B on day 2, then you can count on a 99.9% chance B is still alive. So far so good.
And if you wanted to visit 566 centenarians (let's imagine you have a regularly-updated master list of centenarians from the Finnish population registry), then you only have to beat the odds 566 times in a row, which is not _that_ hard: $0.999024^{566} = 0.5754023437943274$.

But that's coldblooded of you to objectify those Finnish centenarians! "Any centenarian will do, I don't care."
What if you picked the *current* set of 566 centenarians and wanted to visit just them, specifically - with no new centenarians introduced to the list to replace any dead ones.

That's a little more complicated. When you visit the first centenarian, it's the same probability: 0.999024.
When you visit the second centenarian the odds change since now she (and it's more often 'she' than 'he', since remember the exponential and males having shorter mean lifetimes) has to survive 2 days, so it's $0.999024 \cdot 0.999024$ or $0.999024^2$; for the third, it's $0.999024^3$, and so on to #566 who has been patiently waiting and trying to survive a risk of $0.999024^566$, and then you need to multiply to get your odds of beating every single risk of death and the centenarian not leaving for a more permanent rendezvous: $0.999024 \cdot 0.999024^2 \cdot 0.999024^3 \cdot ... \cdot 0.999024^{566}$, which would be $\prod_{n=1}^{566} 0.999024^n$, or in Haskell:

~~~{.Haskell}
product (map (\x -> 0.999024**x) [1..566])
~> 8.952743340164081e-69
~~~

(A little surprisingly, Wolfram Alpha can [solve the TeX expression](https://www.wolframalpha.com/input/?i=\prod_{n%3D1}^{566}+0.999024^n) too.)

Given the use of floating point in that function (567 floating point exponentiations followed by as many multiplications) and the horror stories about floating point, one might worry the answer is wrong & the real probability is much larger.
We can retry with an implementation of computable reals, [`CReal`](https://hackage.haskell.org/package/numbers-2009.8.9/docs/Data-Number-CReal.html#t:CReal), which can be very slow but should give more precise answers:

~~~{.Haskell}
:module + Data.Number.CReal
showCReal 100 (product (map (\x -> 0.999024**x) [1..566]))
~> 0.0000000000000000000000000000000000000000000000000000000000000000000089527433401308585720915431195262
~~~

Looks good - agrees with the floating point version up to the 11th digit:

    8.9527433401 64081e-69
    8.9527433401 308585720915431195262

We can also check by rewriting the product equation to avoid all the exponentiation and multiplication (which might cause issues) in favor of a single exponential:

1. $p^1 * p^2 * ... p^n$ (as before)
2. = $p^{1+2+...+n}$ (since $(x^m) * (x^n) = x^(m + n)$)
3. = $p^{\frac{n \cdot (1 + n)}{2}}$ (by [arithmetic progression](!Wikipedia)/[Gauss's famous classroom trick](http://mathworld.wolfram.com/ArithmeticSeries.html) since $\sum_1^n = n \cdot \frac{a_1 + a_n}{2}$)
4. = $0.999024^{\frac{566 \cdot (1 + 566)}{2}}$ (start substituting in specific values)
5. = $0.999024^{\frac{320922}{2}}$
6. = $0.999024^{160461}$

So:

~~~{.Haskell}
0.999024^160461
~> 8.95274334014924e-69
~~~

Or to go back to the longer version:

~~~{.Haskell}
0.999024**((566*(1 + 566)) / 2)
~> 8.952743340164096e-69
~~~

Also close. All probabilities of success are minute.

How fast would you have to be if you wanted to at least *try* to accomplish the tour with, say, a 50-50 chance?

Well, that's easy: you can consider the probability of all of them surviving one day and as we saw earlier, that's $0.999024^{566} = 0.58$, and two days would be $(0.999024 ^ {566}) ^ 2 = 0.33$
So you can only take a little over a day before you've probabilistically lost & one of them has died; if you hit all 566 centenarians in 24 hours, that's ~24 centenarians per hour or ~2 minutes to chat with each one and travel to the next. If you're trying to collect DNA samples, better hope they're all awake and able to give consent!

So safe to say, you will probably not be able to manage the Traveling Gerontologist's tour.


# Bayes nets
## Daily weight data graph

As the datasets I'm interested in grow in number of variables, it becomes harder to justify doing analysis by simply writing down a simple linear model with a single dependent variable and throwing in the independent variables and maybe a few transformations chosen by hand.
I can instead write down some simultaneous-equations/structural-equation-models, but while it's usually obvious what to do for _k_<4 and if it's not I can compare the possible variants, 4 variables is questionable what the right SEM is, and >5, it's hopeless.
Factor analysis to extract some latent variables is a possibility, but the more general solution here seems to be probabilistic graphical models such as Bayesian networks.

I thought I'd try out some Bayes net inference on some of my datasets.
In this case, I have ~150 daily measurements from my Omron body composition scale, measuring total weight, body fat percentage, and some other things (see [an Omron manual](http://ecx.images-amazon.com/images/I/B1v0aFQLGFS.pdf)):

1. Total weight
2. BMI
3. Body fat percentage
4. Muscle percentage
5. Resting metabolism in calories
6. "Body age"
7. Visceral fat index

The 7 variables are interrelated, so this is definitely a case where a simple `lm` is not going to do the trick.
It's also not 100% clear how to set up a SEM; some definitions are obvious (the much-criticized BMI is going to be determined solely by total weight, muscle and fat percentage might be inversely related) but others are not (how does "visceral fat" relate to body fat?).
And it's not a hopelessly small amount of data.

The Bayes net R library I'm trying out is [`bnlearn`](http://www.bnlearn.com) ([paper](http://www.jstatsoft.org/v35/i03/paper)).

~~~{.R}
library(bnlearn)
# https://www.dropbox.com/s/4nsrszm85m47272/2015-03-22-gwern-weight.csv
weight <- read.csv("selfexperiment/weight.csv")
weight$Date <- NULL; weight$Weight.scale <- NULL
# remove missing data
weightC <- na.omit(weight)
# bnlearn can't handle integers, oddly enough
weightC <- as.data.frame(sapply(weightC, as.numeric))
summary(weightC)
#   Weight.Omron        Weight.BMI        Weight.body.fat    Weight.muscle
#  Min.   :193.0000   Min.   : 26.90000   Min.   :27.00000   Min.   :32.60000
#  1st Qu.:195.2000   1st Qu.: 27.20000   1st Qu.:28.40000   1st Qu.:34.20000
#  Median :196.4000   Median : 27.40000   Median :28.70000   Median :34.50000
#  Mean   :196.4931   Mean   : 28.95409   Mean   :28.70314   Mean   :34.47296
#  3rd Qu.:197.8000   3rd Qu.: 27.60000   3rd Qu.:29.10000   3rd Qu.:34.70000
#  Max.   :200.6000   Max.   : 28.00000   Max.   :31.70000   Max.   :35.50000
#  Weight.resting.metabolism Weight.body.age    Weight.visceral.fat
#  Min.   :1857.000          Min.   :52.00000   Min.   : 9.000000
#  1st Qu.:1877.000          1st Qu.:53.00000   1st Qu.:10.000000
#  Median :1885.000          Median :53.00000   Median :10.000000
#  Mean   :1885.138          Mean   :53.32704   Mean   : 9.949686
#  3rd Qu.:1893.000          3rd Qu.:54.00000   3rd Qu.:10.000000
#  Max.   :1914.000          Max.   :56.00000   Max.   :11.000000
cor(weightC)
#                             Weight.Omron     Weight.BMI Weight.body.fat  Weight.muscle
# Weight.Omron               1.00000000000  0.98858376919    0.1610643221 -0.06976934825
# Weight.BMI                 0.98858376919  1.00000000000    0.1521872557 -0.06231142104
# Weight.body.fat            0.16106432213  0.15218725566    1.0000000000 -0.98704369855
# Weight.muscle             -0.06976934825 -0.06231142104   -0.9870436985  1.00000000000
# Weight.resting.metabolism  0.96693236051  0.95959140245   -0.0665001241  0.15621294274
# Weight.body.age            0.82581939626  0.81286141659    0.5500409365 -0.47408608681
# Weight.visceral.fat        0.41542744168  0.43260100665    0.2798756916 -0.25076619829
#                           Weight.resting.metabolism Weight.body.age Weight.visceral.fat
# Weight.Omron                           0.9669323605    0.8258193963        0.4154274417
# Weight.BMI                             0.9595914024    0.8128614166        0.4326010067
# Weight.body.fat                       -0.0665001241    0.5500409365        0.2798756916
# Weight.muscle                          0.1562129427   -0.4740860868       -0.2507661983
# Weight.resting.metabolism              1.0000000000    0.7008354776        0.3557229425
# Weight.body.age                        0.7008354776    1.0000000000        0.4840752389
# Weight.visceral.fat                    0.3557229425    0.4840752389        1.0000000000

## create alternate dataset expressing the two percentage variables as pounds, since this might fit better
weightC2 <- weightC
weightC2$Weight.body.fat <- weightC2$Weight.Omron * (weightC2$Weight.body.fat / 100)
weightC2$Weight.muscle   <- weightC2$Weight.Omron * (weightC2$Weight.muscle / 100)
~~~

Begin analysis:

~~~{.R}
pdap <- hc(weightC)
pdapc2 <- hc(weightC2)
## bigger is better:
score(pdap, weightC)
[1] -224.2563072
score(pdapc2, weightC2)
[1] -439.7811072
## stick with the original, then
pdap
#   Bayesian network learned via Score-based methods
#
#   model:
#    [Weight.Omron][Weight.body.fat][Weight.BMI|Weight.Omron]
#    [Weight.resting.metabolism|Weight.Omron:Weight.body.fat]
#    [Weight.body.age|Weight.Omron:Weight.body.fat]
#    [Weight.muscle|Weight.body.fat:Weight.resting.metabolism][Weight.visceral.fat|Weight.body.age]
#   nodes:                                 7
#   arcs:                                  8
#     undirected arcs:                     0
#     directed arcs:                       8
#   average markov blanket size:           2.57
#   average neighbourhood size:            2.29
#   average branching factor:              1.14
#
#   learning algorithm:                    Hill-Climbing
#   score:                                 BIC (Gauss.)
#   penalization coefficient:              2.534452101
#   tests used in the learning procedure:  69
#   optimized:                             TRUE
plot(pdap)
## https://i.imgur.com/nipmqta.png
~~~

This inferred graph is obviously wrong in several respects, violating prior knowledge about some of the relationships.

More specifically, my prior knowledge:

- `Weight.Omron` == total weight; should be influenced by `Weight.body.fat` (%), `Weight.muscle` (%), & `Weight.visceral.fat`
- `Weight.visceral.fat`: ordinal variable, <=9 = normal; 10-14 = high; 15+ = very high; from the Omron manual:

    > Visceral fat area (0 - approx. 300 cm , 1 inch=2.54 cm) distribution with 30 levels. NOTE: Visceral fat levels are relative and not absolute values.
- `Weight.BMI`: BMI is a simple function of total weight & height (specifically `BMI = round(weight / height^2)`), so it should be influenced only by `Weight.Omron`, and influence nothing else
- `Weight.body.age`: should be influenced by `Weight.Omron`, `Weight.body.fat`, and `Weight.muscle`, based on the description in the manual:

    > Body age is based on your resting metabolism. Body age is calculated by using your weight, body fat percentage and skeletal muscle percentage to produce a guide to whether your body age is above or below the average for your actual age.
- `Weight.resting.metabolism`: a function of the others, but I'm not sure which exactly; manual talks about what resting metabolism is generically and specifies it has the range "385 to 3999 kcal with 1 kcal increments"; https://en.wikipedia.org/wiki/Basal_metabolic_rate suggests the Omron may be using one of several approximation equations based on age/sex/height/weight, but it might also be using lean body mass as well.

Unfortunately, bnlearn doesn't seem to support any easy way of encoding the prior knowledge - for example, you can't say 'no outgoing arrows from node X' - so I iterate, adding bad arrows to the blacklist.

Which arrows violate prior knowledge?

- `[Weight.visceral.fat|Weight.body.age]` (read backwards, as `Weight.body.age ~> Weight.visceral.fat`)
- `[Weight.muscle|Weight.resting.metabolism]`

Retry, blacklisting those 2 arrows:

~~~{.R}
pdap2 <- hc(weightC, blacklist=data.frame(from=c("Weight.body.age", "Weight.resting.metabolism"), to=c("Weight.visceral.fat","Weight.muscle")))
~~~

New violations:

- `[Weight.visceral.fat|Weight.BMI]`
- `[Weight.muscle|Weight.Omron]`

~~~{.R}
pdap3 <- hc(weightC, blacklist=data.frame(from=c("Weight.body.age", "Weight.resting.metabolism", "Weight.BMI", "Weight.Omron"), to=c("Weight.visceral.fat","Weight.muscle", "Weight.visceral.fat", "Weight.muscle")))
~~~

New violations:

- `[Weight.visceral.fat|Weight.Omron]`
- `[Weight.muscle|Weight.BMI]`

~~{.R}
pdap4 <- hc(weightC, blacklist=data.frame(from=c("Weight.body.age", "Weight.resting.metabolism", "Weight.BMI", "Weight.Omron", "Weight.Omron", "Weight.BMI"), to=c("Weight.visceral.fat","Weight.muscle", "Weight.visceral.fat", "Weight.muscle", "Weight.visceral.fat", "Weight.muscle")))
~~~

One violation:

- `[Weight.muscle|Weight.body.age]`

~~~{.R}
pdap5 <- hc(weightC, blacklist=data.frame(from=c("Weight.body.age", "Weight.resting.metabolism", "Weight.BMI", "Weight.Omron", "Weight.Omron", "Weight.BMI", "Weight.body.age"), to=c("Weight.visceral.fat","Weight.muscle", "Weight.visceral.fat", "Weight.muscle", "Weight.visceral.fat", "Weight.muscle", "Weight.muscle")))
#   Bayesian network learned via Score-based methods
#
#   model:
#    [Weight.body.fat][Weight.muscle|Weight.body.fat][Weight.visceral.fat|Weight.body.fat]
#    [Weight.Omron|Weight.visceral.fat][Weight.BMI|Weight.Omron]
#    [Weight.resting.metabolism|Weight.Omron:Weight.body.fat]
#    [Weight.body.age|Weight.Omron:Weight.body.fat]
#   nodes:                                 7
#   arcs:                                  8
#     undirected arcs:                     0
#     directed arcs:                       8
#   average markov blanket size:           2.57
#   average neighbourhood size:            2.29
#   average branching factor:              1.14
#
#   learning algorithm:                    Hill-Climbing
#   score:                                 BIC (Gauss.)
#   penalization coefficient:              2.534452101
#   tests used in the learning procedure:  62
#   optimized:                             TRUE
plot(pdap5)
## https://i.imgur.com/nxCfmYf.png

## implementing all the prior knowledge cost ~30:
score(pdap5, weightC)
# [1] -254.6061724
~~~

No violations, so let's use the network and estimate the specific parameters:

~~~{.R}
fit <- bn.fit(pdap5, weightC); fit
#   Bayesian network parameters
#
#   Parameters of node Weight.Omron (Gaussian distribution)
#
# Conditional density: Weight.Omron | Weight.visceral.fat
# Coefficients:
#         (Intercept)  Weight.visceral.fat
#       169.181651376          2.744954128
# Standard deviation of the residuals: 1.486044472
#
#   Parameters of node Weight.BMI (Gaussian distribution)
#
# Conditional density: Weight.BMI | Weight.Omron
# Coefficients:
#   (Intercept)   Weight.Omron
# -0.3115772322   0.1411044216
# Standard deviation of the residuals: 0.03513413381
#
#   Parameters of node Weight.body.fat (Gaussian distribution)
#
# Conditional density: Weight.body.fat
# Coefficients:
# (Intercept)
# 28.70314465
# Standard deviation of the residuals: 0.644590085
#
#   Parameters of node Weight.muscle (Gaussian distribution)
#
# Conditional density: Weight.muscle | Weight.body.fat
# Coefficients:
#     (Intercept)  Weight.body.fat
#   52.1003347352    -0.6141270921
# Standard deviation of the residuals: 0.06455478599
#
#   Parameters of node Weight.resting.metabolism (Gaussian distribution)
#
# Conditional density: Weight.resting.metabolism | Weight.Omron + Weight.body.fat
# Coefficients:
#     (Intercept)     Weight.Omron  Weight.body.fat
#   666.910582196      6.767607964     -3.886694779
# Standard deviation of the residuals: 1.323176507
#
#   Parameters of node Weight.body.age (Gaussian distribution)
#
# Conditional density: Weight.body.age | Weight.Omron + Weight.body.fat
# Coefficients:
#     (Intercept)     Weight.Omron  Weight.body.fat
#  -32.2651379176     0.3603672788     0.5150134225
# Standard deviation of the residuals: 0.2914301529
#
#   Parameters of node Weight.visceral.fat (Gaussian distribution)
#
# Conditional density: Weight.visceral.fat | Weight.body.fat
# Coefficients:
#     (Intercept)  Weight.body.fat
#    6.8781100009     0.1070118125
# Standard deviation of the residuals: 0.2373649058
## residuals look fairly good, except for Weight.resting.metabolism, where there are some extreme residuals in what looks a bit like a sigmoid sort of pattern, suggesting nonlinearities in the Omron scale's formula?
bn.fit.qqplot(fit)
## https://i.imgur.com/mSallOv.png
~~~

We can double-check the estimates here by turning the Bayes net model into a SEM and seeing how the estimates compare, and also seeing if the p-values suggest we've found a good model:

~~~{.R}
library(lavaan)
Weight.model1 <- '
    Weight.visceral.fat ~ Weight.body.fat
    Weight.Omron ~ Weight.visceral.fat
    Weight.BMI ~ Weight.Omron
    Weight.body.age ~ Weight.Omron + Weight.body.fat
    Weight.muscle ~ Weight.body.fat
    Weight.resting.metabolism ~ Weight.Omron + Weight.body.fat
                   '
Weight.fit1 <- sem(model = Weight.model1,  data = weightC)
summary(Weight.fit1)
# lavaan (0.5-16) converged normally after 139 iterations
#
#   Number of observations                           159
#
#   Estimator                                         ML
#   Minimum Function Test Statistic               71.342
#   Degrees of freedom                                 7
#   P-value (Chi-square)                           0.000
#
# Parameter estimates:
#
#   Information                                 Expected
#   Standard Errors                             Standard
#
#                    Estimate  Std.err  Z-value  P(>|z|)
# Regressions:
#   Weight.visceral.fat ~
#     Weight.bdy.ft     0.107    0.029    3.676    0.000
#   Weight.Omron ~
#     Wght.vscrl.ft     2.745    0.477    5.759    0.000
#   Weight.BMI ~
#     Weight.Omron      0.141    0.002   82.862    0.000
#   Weight.body.age ~
#     Weight.Omron      0.357    0.014   25.162    0.000
#     Weight.bdy.ft     0.516    0.036   14.387    0.000
#   Weight.muscle ~
#     Weight.bdy.ft    -0.614    0.008  -77.591    0.000
#   Weight.resting.metabolism ~
#     Weight.Omron      6.730    0.064  104.631    0.000
#     Weight.bdy.ft    -3.860    0.162  -23.837    0.000
#
# Covariances:
#   Weight.BMI ~~
#     Weight.body.g    -0.000    0.001   -0.116    0.907
#     Weight.muscle    -0.000    0.000   -0.216    0.829
#     Wght.rstng.mt     0.005    0.004    1.453    0.146
#   Weight.body.age ~~
#     Weight.muscle     0.001    0.001    0.403    0.687
#     Wght.rstng.mt    -0.021    0.030   -0.700    0.484
#   Weight.muscle ~~
#     Wght.rstng.mt     0.007    0.007    1.003    0.316
#
# Variances:
#     Wght.vscrl.ft     0.056    0.006
#     Weight.Omron      2.181    0.245
#     Weight.BMI        0.001    0.000
#     Weight.body.g     0.083    0.009
#     Weight.muscle     0.004    0.000
#     Wght.rstng.mt     1.721    0.193
~~~

Comparing the coefficients by eye, they tend to be quite close (usually within 0.1) and the p-values are all statistically-significant.

The network itself looks right, although some of the edges are surprises: I didn't know visceral fat was predictable from body fat (I thought they were measuring separate things), and the relative independence of muscle suggests that in any exercise plan I might be better off focusing on the body fat percentage rather than the muscle percentage since the former may be effectively determining the latter.

So what did I learn here?

- learning network structure and direction of arrows is hard; even with only 7 variables and  _n_=159 (accurate clean data), the hill-climbing algorithm will learn at least 7 wrong arcs.

    - and the derived graphs depend disturbingly heavily on choice of algorithm; I used the `hc` hill-climbing algorithm (since I'm lazy and didn't want to specify arrow directions), but when I try out the alternatives like `iamb` on the same data & blacklist, the found graph looks rather different
- Gaussians are, as always, sensitive to outliers: I was surprised the first graph didn't show BMI connected to anything, so I took a closer look and found I had miscoded a BMI of 28 as *280*!
- bnlearn, while not as hard to use as I expected, could still use usability improvements: I should not need to coerce integer data into exactly equivalent numeric types just because bnlearn doesn't recognize integers; and blacklisting/whitelisting needs to be more powerful - iteratively generating graphs and manually inspecting and manually blacklisting is tedious and does not scale

    - hence, it may make more sense to find a graph using `bnlearn` and then convert it into simultaneous-equations and manipulate it using more mature SEM libraries

## Zeo sleep data

Here I look at my Zeo sleep data; more variables, more complex relations, and more unknown ones, but on the positive side, ~12x more data to work with.

~~~{.R}
zeo <- read.csv("~/wiki/docs/zeo/gwern-zeodata.csv")
zeo$Sleep.Date <- as.Date(zeo$Sleep.Date, format="%m/%d/%Y")

## convert "05/12/2014 06:45" to "06:45"
zeo$Start.of.Night <- sapply(strsplit(as.character(zeo$Start.of.Night), " "), function(x) { x[2] })
## convert "06:45" to 24300
interval <- function(x) { if (!is.na(x)) { if (grepl(" s",x)) as.integer(sub(" s","",x))
                                           else { y <- unlist(strsplit(x, ":")); as.integer(y[[1]])*60 + as.integer(y[[2]]); }
                                         }
                          else NA
                        }
zeo$Start.of.Night <- sapply(zeo$Start.of.Night, interval)
## correct for the switch to new unencrypted firmware in March 2013;
## I don't know why the new firmware subtracts 15 hours
zeo[(zeo$Sleep.Date >= as.Date("2013-03-11")),]$Start.of.Night <- (zeo[(zeo$Sleep.Date >= as.Date("2013-03-11")),]$Start.of.Night + 900) %% (24*60)

## after midnight (24*60=1440), Start.of.Night wraps around to 0, which obscures any trends,
## so we'll map anything before 7AM to time+1440
zeo[zeo$Start.of.Night<420 & !is.na(zeo$Start.of.Night),]$Start.of.Night <- (zeo[zeo$Start.of.Night<420 & !is.na(zeo$Start.of.Night),]$Start.of.Night + (24*60))

zeoSmall <- subset(zeo, select=c(ZQ,Total.Z,Time.to.Z,Time.in.Wake,Time.in.REM,Time.in.Light,Time.in.Deep,Awakenings,Start.of.Night,Morning.Feel))
zeoClean <- na.omit(zeoSmall)
# bnlearn doesn't like the 'integer' class that most of the data-frame is in
zeoClean <- as.data.frame(sapply(zeoClean, as.numeric))
~~~

Prior knowledge:

- `Start.of.Night` is temporally first, and cannot be caused
- `Time.to.Z` is temporally second, and can be influenced by `Start.of.Night` (likely a connection between how late I go to bed and how fast I fall asleep) & `Time.in.Wake` (since if it takes 10 minutes to fall asleep, I must spend ≥10 minutes in wake) but not others
- `Morning.Feel` is temporally last, and cannot cause anything
- `ZQ` is a synthetic variable invented by Zeo according to an opaque formula, which cannot cause anything but is determined by others
- `Total.Z` should be the sum of `Time.in.Light`, `Time.in.REM`, and `Time.in.Deep`
- `Awakenings` should have an arrow with `Time.in.Wake` but it's not clear which way it should run

~~~{.R}
library(bnlearn)
## after a bunch of iteration, blacklisting arrows which violate the prior knowledge
bl <- data.frame(from=c("Morning.Feel", "ZQ", "ZQ", "ZQ", "ZQ", "ZQ", "ZQ", "Time.in.REM", "Time.in.Light", "Time.in.Deep", "Morning.Feel", "Awakenings", "Time.in.Light", "Morning.Feel", "Morning.Feel","Total.Z", "Time.in.Wake", "Time.to.Z", "Total.Z", "Total.Z", "Total.Z"),
                 to=c("Start.of.Night", "Total.Z", "Time.in.Wake", "Time.in.REM", "Time.in.Deep", "Morning.Feel","Start.of.Night", "Start.of.Night","Start.of.Night","Start.of.Night", "Time.to.Z", "Time.to.Z", "Time.to.Z", "Total.Z", "Time.in.Wake","Time.to.Z","Time.to.Z", "Start.of.Night", "Time.in.Deep", "Time.in.REM", "Time.in.Light"))

zeo.hc <- hc(zeoClean, blacklist=bl)
zeo.iamb         <- iamb(zeoClean, blacklist=bl)
## problem: undirected arc: Time.in.Deep/Time.in.REM; since hc inferred [Time.in.Deep|Time.in.REM], I'll copy that for iamb:
zeo.iamb <- set.arc(zeo.iamb, from = "Time.in.REM", to = "Time.in.Deep")
zeo.gs <- gs(zeoClean, blacklist=bl)
## same undirected arc:
zeo.gs <- set.arc(zeo.gs, from = "Time.in.REM", to = "Time.in.Deep")

## Bigger is better:
score(zeo.iamb, data=zeoClean)
# [1] -44776.79185
score(zeo.gs, data=zeoClean)
# [1] -44776.79185
score(zeo.hc, data=zeoClean)
# [1] -44557.6952
## hc scores best, so let's look at it:
zeo.hc
#   Bayesian network learned via Score-based methods
#
#   model:
#    [Start.of.Night][Time.to.Z|Start.of.Night][Time.in.Light|Time.to.Z:Start.of.Night]
#    [Time.in.REM|Time.in.Light:Start.of.Night][Time.in.Deep|Time.in.REM:Time.in.Light:Start.of.Night]
#    [Total.Z|Time.in.REM:Time.in.Light:Time.in.Deep][Time.in.Wake|Total.Z:Time.to.Z]
#    [Awakenings|Time.to.Z:Time.in.Wake:Time.in.REM:Time.in.Light:Start.of.Night]
#    [Morning.Feel|Total.Z:Time.to.Z:Time.in.Wake:Time.in.Light:Start.of.Night]
#    [ZQ|Total.Z:Time.in.Wake:Time.in.REM:Time.in.Deep:Awakenings]
#   nodes:                                 10
#   arcs:                                  28
#     undirected arcs:                     0
#     directed arcs:                       28
#   average markov blanket size:           7.40
#   average neighbourhood size:            5.60
#   average branching factor:              2.80
#
#   learning algorithm:                    Hill-Climbing
#   score:                                 BIC (Gauss.)
#   penalization coefficient:              3.614556939
#   tests used in the learning procedure:  281
#   optimized:                             TRUE

plot(zeo.hc)
## https://i.imgur.com/nD3LXND.png

fit <- bn.fit(zeo.hc, zeoClean); fit
#
#   Bayesian network parameters
#
#   Parameters of node ZQ (Gaussian distribution)
#
# Conditional density: ZQ | Total.Z + Time.in.Wake + Time.in.REM + Time.in.Deep + Awakenings
# Coefficients:
#    (Intercept)         Total.Z    Time.in.Wake     Time.in.REM    Time.in.Deep      Awakenings
# -0.12468522173   0.14197043518  -0.07103211437   0.07053271816   0.21121000076  -0.56476256303
# Standard deviation of the residuals: 0.3000223604
#
#   Parameters of node Total.Z (Gaussian distribution)
#
# Conditional density: Total.Z | Time.in.Wake + Start.of.Night
# Coefficients:
#    (Intercept)    Time.in.Wake  Start.of.Night
# 907.6406157850   -0.4479377278   -0.2680771514
# Standard deviation of the residuals: 68.90853885
#
#   Parameters of node Time.to.Z (Gaussian distribution)
#
# Conditional density: Time.to.Z | Start.of.Night
# Coefficients:
#    (Intercept)  Start.of.Night
# -1.02898431407   0.01568450832
# Standard deviation of the residuals: 13.51606719
#
#   Parameters of node Time.in.Wake (Gaussian distribution)
#
# Conditional density: Time.in.Wake | Time.to.Z
# Coefficients:
#   (Intercept)      Time.to.Z
# 14.7433880499   0.3289378711
# Standard deviation of the residuals: 19.0906685
#
#   Parameters of node Time.in.REM (Gaussian distribution)
#
# Conditional density: Time.in.REM | Total.Z + Start.of.Night
# Coefficients:
#      (Intercept)           Total.Z    Start.of.Night
# -120.62442964234     0.37864195651     0.06275760841
# Standard deviation of the residuals: 19.32560757
#
#   Parameters of node Time.in.Light (Gaussian distribution)
#
# Conditional density: Time.in.Light | Total.Z + Time.in.REM + Time.in.Deep
# Coefficients:
#   (Intercept)        Total.Z    Time.in.REM   Time.in.Deep
#  0.6424267863   0.9997862624  -1.0000587988  -1.0001805537
# Standard deviation of the residuals: 0.5002896274
#
#   Parameters of node Time.in.Deep (Gaussian distribution)
#
# Conditional density: Time.in.Deep | Total.Z + Time.in.REM
# Coefficients:
#   (Intercept)        Total.Z    Time.in.REM
# 15.4961459056   0.1283622577  -0.1187382535
# Standard deviation of the residuals: 11.90756843
#
#   Parameters of node Awakenings (Gaussian distribution)
#
# Conditional density: Awakenings | Time.to.Z + Time.in.Wake + Time.in.REM + Time.in.Light + Start.of.Night
# Coefficients:
#     (Intercept)        Time.to.Z     Time.in.Wake      Time.in.REM    Time.in.Light
# -18.41014329148    0.02605164827    0.05736596152    0.02291139969    0.01060661963
#  Start.of.Night
#   0.01129521977
# Standard deviation of the residuals: 2.427868657
#
#   Parameters of node Start.of.Night (Gaussian distribution)
#
# Conditional density: Start.of.Night
# Coefficients:
# (Intercept)
# 1413.382886
# Standard deviation of the residuals: 64.43144125
#
#   Parameters of node Morning.Feel (Gaussian distribution)
#
# Conditional density: Morning.Feel | Total.Z + Time.to.Z + Time.in.Wake + Time.in.Light + Start.of.Night
# Coefficients:
#     (Intercept)          Total.Z        Time.to.Z     Time.in.Wake    Time.in.Light
# -0.924662971061   0.004808652252  -0.010127269154  -0.008636841492  -0.002766602019
#  Start.of.Night
#  0.001672816480
# Standard deviation of the residuals: 0.7104115719

## some issues with big residuals at the extremes in the variables Time.in.Light, Time.in.Wake, and Time.to.Z;
## not sure how to fix those
bn.fit.qqplot(fit)
# https://i.imgur.com/fmP1ca0.png

library(lavaan)
Zeo.model1 <- '
    Time.to.Z ~ Start.of.Night
    Time.in.Wake ~ Total.Z + Time.to.Z
    Awakenings ~ Time.to.Z + Time.in.Wake + Time.in.REM + Time.in.Light + Start.of.Night
    Time.in.Light ~ Time.to.Z + Start.of.Night
    Time.in.REM ~ Time.in.Light + Start.of.Night
    Time.in.Deep ~ Time.in.REM + Time.in.Light + Start.of.Night
    Total.Z ~ Time.in.REM + Time.in.Light + Time.in.Deep
    ZQ ~ Total.Z + Time.in.Wake + Time.in.REM + Time.in.Deep + Awakenings
    Morning.Feel ~ Total.Z + Time.to.Z + Time.in.Wake + Time.in.Light + Start.of.Night
                   '
Zeo.fit1 <- sem(model = Zeo.model1,  data = zeoClean)
summary(Zeo.fit1)
# lavaan (0.5-16) converged normally after 183 iterations
#
#   Number of observations                          1379
#
#   Estimator                                         ML
#   Minimum Function Test Statistic               22.737
#   Degrees of freedom                                16
#   P-value (Chi-square)                           0.121
#
# Parameter estimates:
#
#   Information                                 Expected
#   Standard Errors                             Standard
#
#                    Estimate  Std.err  Z-value  P(>|z|)
# Regressions:
#   Time.to.Z ~
#     Start.of.Nght     0.016    0.006    2.778    0.005
#   Time.in.Wake ~
#     Total.Z          -0.026    0.007   -3.592    0.000
#     Time.to.Z         0.314    0.038    8.277    0.000
#   Awakenings ~
#     Time.to.Z         0.026    0.005    5.233    0.000
#     Time.in.Wake      0.057    0.003   16.700    0.000
#     Time.in.REM       0.023    0.002   10.107    0.000
#     Time.in.Light     0.011    0.002    6.088    0.000
#     Start.of.Nght     0.011    0.001   10.635    0.000
#   Time.in.Light ~
#     Time.to.Z        -0.348    0.085   -4.121    0.000
#     Start.of.Nght    -0.195    0.018  -10.988    0.000
#   Time.in.REM ~
#     Time.in.Light     0.358    0.018   19.695    0.000
#     Start.of.Nght     0.034    0.013    2.725    0.006
#   Time.in.Deep ~
#     Time.in.REM       0.081    0.012    6.657    0.000
#     Time.in.Light     0.034    0.009    3.713    0.000
#     Start.of.Nght    -0.017    0.006   -3.014    0.003
#   Total.Z ~
#     Time.in.REM       1.000    0.000 2115.859    0.000
#     Time.in.Light     1.000    0.000 2902.045    0.000
#     Time.in.Deep      1.000    0.001  967.322    0.000
#   ZQ ~
#     Total.Z           0.142    0.000  683.980    0.000
#     Time.in.Wake     -0.071    0.000 -155.121    0.000
#     Time.in.REM       0.071    0.000  167.090    0.000
#     Time.in.Deep      0.211    0.001  311.454    0.000
#     Awakenings       -0.565    0.003 -178.407    0.000
#   Morning.Feel ~
#     Total.Z           0.005    0.001    8.488    0.000
#     Time.to.Z        -0.010    0.001   -6.948    0.000
#     Time.in.Wake     -0.009    0.001   -8.592    0.000
#     Time.in.Light    -0.003    0.001   -2.996    0.003
#     Start.of.Nght     0.002    0.000    5.414    0.000
~~~

Again no major surprises, but one thing I notice is that `ZQ` does not seem to connect to `Time.in.Light`, though `Time.in.Light` does connect to `Morning.Feel`; I've long suspected that `ZQ` is a flawed summary and thought it was insufficiently taking into account wakes or something else, so it looks like it's `Time.in.Light` specifically which is missing.
`Start.of.night` also is more highly connected than I had expected.

Comparing graphs from the 3 algorithms, they don't seem to differ as badly as the weight ones did. Is this thanks to the much greater data or the constraints?

# Genome sequencing costs

~~~{.R}
# http://www.genome.gov/sequencingcosts/
# http://www.genome.gov/pages/der/sequencing_costs_apr2014.xls
# converted to CSV & deleted cost per base (less precision); CSV looks like:
# https://dl.dropboxusercontent.com/u/182368464/sequencing_costs_apr2014.csv
## Date, Cost per Genome
## Sep-01,"$95,263,072"
## ...
sequencing <- read.csv("sequencing_costs_apr2014.csv")
sequencing$Cost.per.Genome <- as.integer(gsub(",", "", sub("\\$", "", as.character(sequencing$Cost.per.Genome))))
# interpret month-years as first of month:
sequencing$Date <- as.Date(paste0("01-", as.character(sequencing$Date)), format="%d-%b-%y")
head(sequencing)
##         Date Cost.per.Genome
## 1 2001-09-01        95263072
## 2 2002-03-01        70175437
## 3 2002-09-01        61448422
## 4 2003-03-01        53751684
## 5 2003-10-01        40157554
## 6 2004-01-01        28780376

l <- lm(log(Cost.per.Genome) ~ Date, data=sequencing); summary(l)
##
## Coefficients:
##                 Estimate   Std. Error  t value   Pr(>|t|)
## (Intercept) 50.969823683  1.433567932  35.5545 < 2.22e-16
## Date        -0.002689621  0.000101692 -26.4486 < 2.22e-16
##
## Residual standard error: 0.889707 on 45 degrees of freedom
## Multiple R-squared:  0.939559,   Adjusted R-squared:  0.938216
## F-statistic: 699.528 on 1 and 45 DF,  p-value: < 2.22e-16
plot(log(Cost.per.Genome) ~ Date, data=sequencing)
## https://i.imgur.com/3XK8i0h.png
# as expected: linear in log (Moore's law) 2002-2008, sudden drop, return to Moore's law-ish ~December 2011?
# but on the other hand, maybe the post-December 2011 behavior is a continuation of the curve
library(segmented)
# 2 break-points / 3 segments:
piecewise <- segmented(l, seg.Z=~Date, psi=list(Date=c(13970, 16071)))
summary(piecewise)
## Estimated Break-Point(s):
##             Est. St.Err
## psi1.Date 12680 1067.0
## psi2.Date 13200  279.8
##
## t value for the gap-variable(s) V:  0 0 2
##
## Meaningful coefficients of the linear terms:
##                 Estimate   Std. Error  t value   Pr(>|t|)
## (Intercept) 35.841699121  8.975628264  3.99322 0.00026387
## Date        -0.001504431  0.000738358 -2.03754 0.04808491
## U1.Date      0.000679538  0.002057940  0.33020         NA
## U2.Date     -0.002366688  0.001926528 -1.22847         NA
##
## Residual standard error: 0.733558 on 41 degrees of freedom
## Multiple R-Squared: 0.962565,  Adjusted R-squared:   0.958
with(sequencing, plot(Date, log(Cost.per.Genome), pch=16)); plot(piecewise, add=T)
## https://i.imgur.com/HSRqkJO.png
# The first two segments look fine, but the residuals are clearly bad for the third line-segment:
# it undershoots (damaging the second segment's fit), overshoots, then undershoots again. Let's try again with more breakpoints:

lots <- segmented(l, seg.Z=~Date, psi=list(Date=NA), control=seg.control(stop.if.error=FALSE, n.boot=0))
summary(segmented(l, seg.Z=~Date, psi=list(Date=as.Date(c(12310, 12500, 13600, 13750,  14140,  14680,  15010, 15220), origin = "1970-01-01", tz = "EST"))))
# delete every breakpoint below t-value of ~|2.3|, for 3 breakpoints / 4 segments:
piecewise2 <- segmented(l, seg.Z=~Date, psi=list(Date=as.Date(c("2007-08-25","2008-09-18","2010-03-12"))))
with(sequencing, plot(Date, log(Cost.per.Genome), pch=16)); plot(piecewise2, add=T)

# the additional break-point is used up on a better fit in the curve. It looks like an exponential decay/asymptote,
# so let's work on fitting that part of the graph, the post-2007 curve:
sequencingRecent <- sequencing[sequencing$Date>as.Date("2007-10-01"),]
lR <- lm(log(Cost.per.Genome) ~ Date, data=sequencingRecent); summary(lR)
piecewiseRecent <- segmented(lR, seg.Z=~Date, psi=list(Date=c(14061, 16071))); summary(piecewiseRecent)
## Estimated Break-Point(s):
##             Est. St.Err
## psi1.Date 14290  36.31
## psi2.Date 15290  48.35
##
## t value for the gap-variable(s) V:  0 0
##
## Meaningful coefficients of the linear terms:
##                 Estimate   Std. Error   t value   Pr(>|t|)
## (Intercept)  1.13831e+02  6.65609e+00  17.10182 2.0951e-13
## Date        -7.13247e-03  4.73332e-04 -15.06865 2.2121e-12
## U1.Date      4.11492e-03  4.94486e-04   8.32161         NA
## U2.Date      2.48613e-03  2.18528e-04  11.37668         NA
##
## Residual standard error: 0.136958 on 20 degrees of freedom
## Multiple R-Squared: 0.995976,  Adjusted R-squared: 0.994971

with(sequencingRecent, plot(Date, log(Cost.per.Genome), pch=16)); plot(piecewiseRecent, add=T)

lastPiece <- lm(log(Cost.per.Genome) ~ Date, data=sequencingRecent[as.Date(15290, origin = "1970-01-01", tz = "EST")<sequencingRecent$Date,]); summary(lastPiece)
## Coefficients:
##                 Estimate   Std. Error  t value   Pr(>|t|)
## (Intercept) 17.012409648  1.875482507  9.07095 1.7491e-05
## Date        -0.000531621  0.000119056 -4.46528  0.0020963
##
## Residual standard error: 0.0987207 on 8 degrees of freedom
## Multiple R-squared:  0.71366,    Adjusted R-squared:  0.677867
with(sequencingRecent[as.Date(15290, origin = "1970-01-01", tz = "EST")<sequencingRecent$Date,], plot(Date, log(Cost.per.Genome), pch=16)); abline(lastPiece)

predictDays <- seq(from=sequencing$Date[1], to=as.Date("2030-12-01"), by="month")
lastPiecePredict <- data.frame(Date = predictDays, Cost.per.Genome=c(sequencing$Cost.per.Genome, rep(NA, 305)), Cost.per.Genome.predicted = exp(predict(lastPiece, newdata = data.frame(Date = predictDays))))

nlmR <- nls(log(Cost.per.Genome) ~ SSasymp(as.integer(Date), Asym, r0, lrc), data=sequencingRecent); summary(nlmR)
##
## Parameters:
##          Estimate   Std. Error    t value Pr(>|t|)
## Asym  7.88908e+00  1.19616e-01   65.95328   <2e-16
## r0    1.27644e+08  1.07082e+08    1.19203   0.2454
## lrc  -6.72151e+00  5.05221e-02 -133.04110   <2e-16
##
## Residual standard error: 0.150547 on 23 degrees of freedom
with(sequencingRecent, plot(Date, log(Cost.per.Genome))); lines(sequencingRecent$Date, predict(nlmR), col=2)

# side by side:
with(sequencingRecent, plot(Date, log(Cost.per.Genome), pch=16))
plot(piecewiseRecent, add=TRUE, col=2)
lines(sequencingRecent$Date, predict(nlmR), col=3)
# as we can see, the 3-piece linear fit and the exponential decay fit identically;
# but exponential decay is more parsimonious, IMO, so I prefer that.

predictDays <- seq(from=sequencingRecent$Date[1], to=as.Date("2020-12-01"), by="month")
data.frame(Date = predictDays, Cost.per.Genome.predicted = exp(predict(nlmR, newdata = data.frame(Date = predictDays))))
~~~

http://www.unz.com/gnxp/the-intel-of-sequencing/#comment-677904
https://biomickwatson.wordpress.com/2015/03/25/the-cost-of-sequencing-is-still-going-down/

# Proposal: hand-counting mobile app for more fluid group discussions

> Groups use voting for decision-making, but existing vote systems are cumbersome.
> Hand-raising is faster, but does not scale because hand-counting hands is slow.
> Advances in machine vision may make it possible for AI to count hands in photos accurately.
> Combined with a smartphone's camera, this could yield an app for fast voting in even large groups.

Medium-large (>10 people) groups face a problem in reaching consensus: ballot or pen-and-paper voting is sufficiently slow and clunky that it is too costly to use for anything but the most important discussions.
A group is forced to adopt other discussion norms and save a formal vote for only the final decision, and even then the long delay kills a lot of enthusiasm and interest.
Voting could be used for many more decisions if it could be faster, and of course all existing group votes would benefit from increased speed.
(I am reminded of anime conventions and film festivals where, particularly for short films such as AMVs, one seems to spend more time filling out a ballot & passing them along aisles & the staff painfully counting through each ballot by hand than one actually spends watching the media in question!)

It would be better if voting could be as fluent and easy as simply raising your hand like in a small group such as a classroom - a mechanism which makes it so easy to vote that votes can be held as fast as the alternatives can be spoken aloud and a glance suffices to count (an alert group could vote on 2 or 3 topics in the time it takes to read this sentence).
But hand-raising, as great as it is, suffers from the flaw that it does not scale due to the counting problem: a group of 500 people can raise their hands as easily as a group of 50 or 5, but it takes far too long to count ~250 hands: the person counting will quickly tire of the tedium, they will make mistakes counting, and this puts a serious lag on each vote, a lag which increases linearly with the number of voters.
(Hands can be easy to approximate if almost everyone votes for or against something, but if consensus is so overwhelming, one doesn't need to vote in the first place! The hard case of almost-balanced votes is the most important case.)

One might suggest using an entirely different strategy: a website with HTML polls or little clicker gizmos like used in some college lectures to administer quick quizzes.
This have the downsides that they require potentially expensive equipment (I used a clicker in one class and I think it cost at least $20, so if a convention wanted to use that in an audience of hundreds, that's a major upfront cost & my experience was that clickers were unintuitive, did not always work, and slowed things down if anything; a website would only work if you assume everyone has smartphones and is willing to pull them out to use at an instance's notice and of course that there's working WiFi in the room, which cannot be taken for granted) and considerable overhead in explaining to everyone how it works and getting them on the same page and making sure every person who wanders in also gets the message.
(If anyone is going to be burdened with understanding or using a new system, it should be the handful of conference/festival/group organizers, not the entire audience!)
It's hard to imagine a film festival running using either system, and difficult to see either system improving on pen-and-paper ballots which at least are cheap, relatively straightforward, and well-known.

Hand-counting really does seem like the best solution, if only the counting could be fixed.
Counting is something computers do fast, so that is the germ of an idea.
What if a smartphone could count the votes? You don't want a smartphone app on the entire audiences' phones, of course, since that's even worse than having everyone go to a website to vote; but machine vision has made enormous strides in the 2000s-2010s, reaching human-equivalent performance on challenging image recognition contests like ImageNet.
(Machine vision is complicated, but the important thing is that it's the kind of complicated which can be outsourced to someone else and turned into a dead-easy-to-use app, and the burden does not fall on the primary users - the audience.)
What if the organizer had an app which took a photo of the entire audience with lifted arms and counted hands & faces and returned a vote count in a second?

Such an app would be ideal for any cultural, political, or organizational meeting. Now the flow for, eg, a film festival could go: [no explanation given to audience, one just starts] "OK, how many people liked the first short, 'Vampire Deli' by Ms Houston?" [everyone raises hand, smartphone flashes, 1s passes] "OK, 140 votes. How many liked the second short, 'Cthulicious' by Mr Iouston?" [raises hands, smartphone flashes, 1s passes] "OK... 130 people. Congratulations Ms Houston!"
And so on.

Such an app might be considered an infeasible machine vision task, but I believe it could be feasible: facial localization is an old and well-studied image recognition task (and effective algorithms are built into every consumer camera), hands/fingers have very distinct shapes, and both tasks seem easier than the subtle discriminations between, say, various dog breeds demanded of ImageNet contestants.

Specifically, one could implement the machine vision core as follows:

1. multilayer neural networks trained for one task can be repurposed to similar tasks by removing the highest layer and retraining on the new task, potentially reaping great performance gains as the hybrid network has already learned much of what it needs for the second task (["transfer learning"](https://cs231n.github.io/transfer-learning/)). So one could take a publicly available NN trained for ImageNet (such as [AlexNet](http://www.cs.toronto.edu/~fritz/absps/imagenet.pdf "'ImageNet Classification with Deep Convolutional Neural Networks', Krizhevsky et al 2012"), available in [caffe](https://github.com/BVLC/caffe)), remove the top two layers, and retrain on a dataset of audiences; this will perform better since the original NN has already learned how to detect edges, recognize faces, etc
2. raid Flickr and Google Images for CC-licensed photos of audiences raising their arms; then one can manually count how many arms are raised (or outsource to Amazon Mechanical Turk). With the boost from a transferred convolutional deep network, one might get good performance with just a few thousand photos to train with. If each photo takes a minute to obtain and count, then one can create a useful corpus in a week or two of work.
3. train the NN, applying the usual data augmentation tricks to increase one's meager corpus, trying out random hyperparameters, tweaking the architecture, etc

    (Note that while NNs are very slow and computationally intensive to *train*, they are typically quite fast to *run*; the smartphone app would not be training a NN, which is indeed completely infeasible from a CPU and battery life standpoint - it is merely running the NN created by the original developer.)
4. with an accurate NN, one can wrap it in a mobile app framework. The UI, at the simplest, is simply a big button to press to take a photo, feed it into the NN, and display the count. Some additional features come to mind:

    - "headcount mode": one may not be interested in a vote, but in how many people are in an audience (to estimate how popular a guest is, whether an event needs to move to a new bigger space, etc). If the NN can count faces and hands to estimate a vote count, it can simply report the count of faces instead.
    - the app should save every photo & count, both as an audit trail and also to support post-vote recounts in case of disputes or a desire for a more definitive count
    - the reported count should come with an indication of the NN's uncertainty/error-rate, so users are not misled by their little handheld oracle and so they can redo a vote if the choice is borderline; Bayesian methods, in which previous votes are drawn upon, might be relevant here.

        - if the original photo could be annotated with graphical notes for each recognized/counted hand & face, this would let the user 'see' what the NN is thinking and would help build confidence a great deal
    - it should support manually entering in a vote-count; if the manual count differs, then this indicates the NN made an error and the photo & count should be uploaded to the original developer so it can be added to the corpus and the NN's performance fixed in future releases of the app
    - smartphone cameras may not be high-resolution or have a sufficiently wide field-of-view to capture the entire audience at once; some sort of "montage mode" should exist so the user can swing the phone across the audience, bursts of shots taken, and the overlapping photos stitched together into a single audience photo which can be then fed into the NN as usual
    - a burst of photos might be superior to a single photo due to smartphone & hand movement blur; I don't know if it's best to try to combine the photos, run the NN multiple times and take the median, or feed multiple photos into the NN (perhaps by moving to a RNN architecture?)
    - the full-strength NN might still be too slow and energy-hungry to run pleasantly on a smartphone; there are [model compression](http://www.cs.cornell.edu/~caruana/compression.kdd06.pdf "'Model Compression', Bucila et al 2006") [techniques for simplifying deep NNs](http://arxiv.org/abs/1503.02531 "'Distilling the Knowledge in a Neural Network', Hinton et al 2015") to reduce the number of nodes or [have fewer layers](http://arxiv.org/abs/1312.6184 "'Do Deep Nets Really Need to be Deep?', Ba & Caruana 2013")  without losing much performance, which might be useful in this context (and indeed, were originally motivated by wanting to make speech-recognition run better on smartphones)

Given this breakdown, one might estimate building such an app as requiring, assuming one is already reasonably familiar with deep networks & writing mobile apps:

1. 1 week to find an ImageNet NN, learn how to modify it, and set it up to train on a fresh corpus
2. 3 weeks to create a corpus of <5000 photos with manually-labeled hand counts
3. 5 weeks to train the NN (NNs as large as ImageNet NNs take weeks to train; depending on the GPU hardware one has access to and how many tweaks and hyperparameters one tries, 5 weeks could be drastically optimistic; but on the plus side, it's mostly waiting as the GPUs suck electricity like crazy)
4. 5 weeks to make an intuitive simple app, submitting to an app store, etc
5. These estimates are loose and probably too optimistic (although I would be surprised if it took a good developer more than 6 months to develop this app), but that would suggest >14 weeks or 784 hours of work for a developer, start to finish. (Even at minimum wage, this represents a substantial development cost of >\$6k; at more plausible developer salaries, easily >\$60k of salary.)

How large is the market for such an app?
Groups such as anime conventions or anything on a college campus are cheapskates and would balk at a price higher than \$4.99 (even if only 5 or 10 staffers need to buy it and it makes the experience much smoother). There are probably several hundred anime or video game conventions which might use this to vote, so that might be 1000 sales there.
There's easily [13,000 business conventions or conferences in the USA](http://www.quora.com/How-many-conferences-conventions-tradeshows-and-exhibitions-happen-in-the-United-States-and-the-world-each-year), which might not need voting so much, but would be attracted by a headcount mode to help optimize their event.
This suggests perhaps \$70k in sales with much less profit after the app store cut & taxes, much of which sales would probably be one-offs as the user reuses it for each conference.
So even a wild success, in which most events adopt use of such voting software, would barely recoup the development costs; as a product, it seems this is just too much of a niche unless one could develop it *much* faster (such as by finding an existing corpus of hands/photos, or be certain of banging out the mobile app in much less than I estimated), find a larger market (theaters for audience participation?), or increase price substantially (10x the price and aim at only businesses?).

# Air conditioner replacement

> Is my old air conditioner inefficient enough to replace? After calculating electricity consumption for it and a new air conditioner, with discounting, and with uncertainty in parameters evaluated by a Monte Carlo method, I conclude that the savings are too small by an order of magnitude to pay for a new replacement air conditioner.

I have an old Whirlpool air conditioner (AC) in my apartment, and as part of insulating and cooling my apartment, I've wondered if the AC should be replaced on energy efficiency grounds.
Would a new AC save more than it costs upfront? What is the optimal decision here?

Initially I was balked in analysis because I couldn't figure out what model it was, and thus anything about it like its energy efficiency.
(No model number or name appears anywhere visible on it, and I'm not going to rip it out of the wall just to look at hidden parts.)

## Parameters

So I began looking at all the old Whirlpool AC photographs in Google, and eventually I found one whose appearance exactly matches mine and which was released around when I think the AC was installed.
The old AC is the ["Whirlpool ACQ189XS"](http://www.ajmadison.com/cgi-bin/ajmadison/ACQ189XS.html) ([official](http://www.whirlpool.com/-%5BACQ189XS%5D-1004030/ACQ189XS/)) (cost: \$0, sunk cost), which is claimed to have [an EER of 10.7](http://www.whirlpool.com/digitalassets/ACQ189XS/Energy%20Guide_EN.pdf).

For comparison, I browsed Amazon looking for highly-rated [Energy Star](!Wikipedia) AC models with at least 5000 BTU cooling power and costing \$250-\$300, picking out the Sunpentown WA-8022S 8000 BTU Window Air Conditioner (\$271) with [11.3 EER](http://www.sylvane.com/sunpentown-wa8011s-window-ac.html).
(Checking some other entries on Amazon, this is fairly representative on both cost & EER.)

Question: what is the electrical savings and hence the payback period of a new AC?

The efficiency unit here is the EER or energy efficiency ratio, defined as BTUs (amount of heat being moved by the AC) divided by watts consumed.
Here we have ACs with 10.7 EER vs 11.2 EER; I need ~10k BTUs to keep the apartment cool (after fixing a lot of cracks, installing an attic fan and two box fans, putting tin foil over some windows, insulation under a floor etc), so the ACs will use up $EER = \frac{10000}{x \text{watts}}$, and then _x_ = 898 watts and 934 watts respectively.

(EER is a lot like [miles per gallon](!Wikipedia)/MPG as a measure of efficiency, and shares the same drawbacks: from a cost-perspective, EER/MPG don't necessarily tell you what you want to know and can be misleading and harder to work with than if efficiency were reported as, say, gallons per mile. As watts per BTU or gallons per mile, it is easy to see that after a certain point, the cost differences have become absolutely small enough that improvements are not worth paying for. Going from 30 gallons of gas to 15 gallons of gas is worth more than going from 3 gallons to 1.5 gallons, even if the relative improvement is the same.)

So while operating, the two ACs will use 898 watts vs 934 watts or 0.89kWh vs 0.934kWh to cool; a difference of 36 watts or 0.036kWh.

Each kWh costs around \$0.09 so the cost-difference is \$0.00324 per hour.

AC is on May-September (5 months), and on almost all day although it only runs intermittently, so say a third of the day or 8 hours, for a total of 1200 hours of operation.

## Cost-benefit

Thus, then the annual benefit from switching to the new AC with 11.2 EER is $\$0.00324 \cdot 8 \cdot 30 \cdot 5 = \$3.888$ or \$3.9.

The cost is \$271 amortized over _n_ years.
At \$3.9 a year, it will take $\frac{\$271}{\$3.9}$ annually = 68 years to payback (ignoring breakage and discounting/interest/opportunity-cost).
This is not good.

Decision: do not replace.

### Discounting

To bring in discounting/interest:
For what annual payment (cost-savings) would we be willing to pay the price of a new AC?
More specifically, if it costs \$271 and has an average payout period of 7 years, then at my usual annual discount rate of 5%, how much must each payout be?

$\sum \limits_{t=1}^7 \frac{r}{(1+0.05)^t} \geq \$271$

_r_ turns out to be ≥\$46.83, which sounds about right. (Discounting penalizes future savings, so _r_ should be greater than $\frac{\$271}{7}$ or \$39, which it is.)

\$47 is 12x larger than the estimated savings of \$3.9, so the conclusion remains the same.

We could also work backward to figure out what EEC *would* justify an upgrade by treating it as an unknown _e_ and solving for it; let's say it must payback in 7 years (I doubt average AC lifetime is much longer) at least \$271, with the same kWh & usage as before, what must the rival EEC be? as an equation:

$(\frac{\frac{10000}{10.7} - \frac{10000}{e}}{1000} \cdot 0.09 \cdot 8 \cdot 30 \cdot 5) > 47$

and solving,

$e > 20.02$

I am pretty sure there are no ACs with EER>20!

Another way to look at it: if a new good AC costs ~\$300 and I expect it to last ~7 years, then that's an annual cost of \$43.
The current AC's *total* annual cost to run is $1200 \text{hours} \cdot \text{kWhs} \cdot \text{cost per kWh}$ or $(8 \cdot 30 \cdot 5) \cdot 0.934 \cdot 0.09 = \$101$.
So it's immediately clear that the energy savings must be huge - half! - before it can hope to justify a new purchase.

## Sensitivity analysis

The above analyses were done with point-estimates.
It's only fair to note that there's a lot of uncertainty lurking in those estimates: \$0.09 was just the median of the estimates I found for my state's electricity rates, the AC might be on 4 or 6 months, the hours per day might be considerably higher (or lower) than my guess of 8 hours, 10.7 & 11.2 EERs are probably best-case estimates and the real efficiencies lower (they're always lower than nominal), the discount rate may be a percent lower or higher and so minimum savings would be off by as much as \$4 in either direction, and so on.
It would be good to do a bit of a sensitivity analysis to make sure that this is not being driven by any particular number.
(Based on the definition, since it's using mostly multiplication, the final value *should* be robust to considerable error in estimating each parameter, but you never know.)
Throwing together my intuition for how much uncertainty is in each parameter and modeling most as normals, I can simulate my prior distribution of savings:

~~~{.R}
set.seed(2015-07-26)
simulate <- function() {
    BTUs <- rnorm(1, 10000, 100)
    EER_old <- 10.7 - abs(rnorm(1, 0, 0.5)) # half-normals because efficiencies only get worse, not better
    EER_new <- 11.2 - abs(rnorm(1, 0, 0.5))
    kWh <- rnorm(1, 0.09, 0.01)
    dailyUsage <- rnorm(1, 8, 2)
    months <- sample (4:6, 1)
    minimumSavings <- rnorm(1, 47, 4)

    annualNetSavings <- ((((BTUs / EER_old ) - (BTUs / EER_new)) / 1000) * kWh * dailyUsage * 30 * months) - minimumSavings
    return(annualNetSavings)
}
sims <- replicate(100000, simulate())
summary(sims)
##        Min.     1st Qu.      Median        Mean     3rd Qu.        Max.
## -70.3666500 -46.2051500 -42.3764100 -42.1133700 -38.3134600  -0.7334517
quantile(sims, p=c(0.025, 0.975))
##        2.5%        97.5%
## -53.59989114 -29.13999204
~~~

Under every simulation, a new AC is a net loss.
(Since we have no observed data to update our priors with, this is an exercise in probability, not Bayesian inference, and so there is no need to bring in JAGS.)

There are two choices: replace or not.
The expected-value of a replacement is $100\% \cdot -\$42$ or -\$42, and the expected-value of not replacing is $100\% \cdot \$0$ or \$0; the latter is larger than the former, so we should choose the latter and not replace the old AC.

Hence we can be confident that not getting a new AC really is the right decision.

# Some ways of dealing with measurement error

Prompted by [a question on LessWrong](http://lesswrong.com/r/discussion/lw/mk8/stupid_questions_august_2015/cngg), some examples of how to analyze noisy measurements in R:

~~~
## Create a simulated dataset with known parameters, and then run a ML multilevel model, a ML SEM,
## and a Bayesian multilevel model; with the last, calculate Expected Value of Sample Information (EVSI):

## SIMULATE
set.seed(2015-08-11)
## "There is a variable X, x belongs to [0, 100]."
toplevel <- rnorm(n=1, 50, 25)
## "There are n ways of measuring it, among them A and B are widely used."
## "For any given measurer, the difference between x(A) and x(B) can be up to 20 points."
A <- toplevel + runif(1, min=-10, max=10)
B <- toplevel + runif(1, min=-10, max=10)
c(toplevel, A, B)
# [1] 63.85938385 55.43608379 59.42333264
### the true level of X we wish to recover is '63.85'

## "Between two any measurers, x(A)1 and x(A)2 can differ on average 10 points, likewise with B."
### let's imagine 10 hypothetical points are sample using method A and method B
### assume 'differ on average 10 points' here means something like 'the standard deviation is 10'
A_1 <- rnorm(n=10, mean=A, sd=10)
B_1 <- rnorm(n=10, mean=B, sd=10)

data <- rbind(data.frame(Measurement="A", Y=A_1), data.frame(Measurement="B", Y=B_1)); data
#    Measurement           Y
# 1            A 56.33870025
# 2            A 69.07267213
# 3            A 40.36889573
# 4            A 48.67289213
# 5            A 79.92622603
# 6            A 62.86919410
# 7            A 53.12953462
# 8            A 66.58894990
# 9            A 47.86296948
# 10           A 60.72416003
# 11           B 68.60203507
# 12           B 58.24702007
# 13           B 45.47895879
# 14           B 63.45308935
# 15           B 52.27724328
# 16           B 56.89783535
# 17           B 55.93598486
# 18           B 59.28162022
# 19           B 70.92341777
# 20           B 49.51360373

## MLM

## multi-level model approach:
library(lme4)
mlm <- lmer(Y ~ (1|Measurement), data=data); summary(mlm)
# Random effects:
#  Groups      Name        Variance Std.Dev.
#  Measurement (Intercept)  0.0000  0.000000
#  Residual                95.3333  9.763877
# Number of obs: 20, groups:  Measurement, 2
#
# Fixed effects:
#              Estimate Std. Error  t value
# (Intercept) 58.308250   2.183269 26.70685
confint(mlm)
#                    2.5 %       97.5 %
# .sig01       0.000000000  7.446867736
# .sigma       7.185811525 13.444112087
# (Intercept) 53.402531768 63.213970887

## So we estimate X at 58.3 but it's not inside our confidence interval with such little data. Bad luck?

## SEM

library(lavaan)
X.model <- '        X =~ B + A
                    A =~ a
                    B =~ b'
X.fit <- sem(model = X.model, meanstructure = TRUE, data = data2)
summary(X.fit)
# ...                   Estimate  Std.err  Z-value  P(>|z|)
# Latent variables:
#   X =~
#     B                 1.000
#     A              7619.504
#   A =~
#     a                 1.000
#   B =~
#     b                 1.000
#
# Intercepts:
#     a                58.555
#     b                58.061
#     X                 0.000
#     A                 0.000
#     B                 0.000
## Well, that didn't work well - explodes, unfortunately. Probably still not enough data.

## MLM (Bayesian)

library(R2jags)
## rough attempt at writing down an explicit multilevel model which
## respects the mentioned priors about errors being reasonably small:
model <- function() {
  grand.mean ~ dunif(0,100)

  delta.between.group ~ dunif(0, 10)

  sigma.between.group ~ dunif(0, 100)
  tau.between.group <- pow(sigma.between.group, -2)

  for(j in 1:K){
   # let's say the group-level differences are also normally-distributed:
   group.delta[j] ~ dnorm(delta.between.group, tau.between.group)
   # and each group also has its own standard-deviation, potentially different from the others':
   group.within.sigma[j] ~ dunif(0, 20)
   group.within.tau[j] <- pow(group.within.sigma[j], -2)

   # save the net combo for convenience & interpretability:
   group.mean[j] <- grand.mean + group.delta[j]
  }

  for (i in 1:N) {
   # each individual observation is from the grand-mean + group-offset, then normally distributed:
   Y[i] ~ dnorm(grand.mean + group.delta[Group[i]], group.within.tau[Group[i]])
  }

  }
jagsData <- list(N=nrow(data), Y=data$Y, K=length(levels(data$Measurement)),
             Group=data$Measurement)
params <- c("grand.mean","delta.between.group", "sigma.between.group", "group.delta", "group.mean",
            "group.within.sigma")
k1 <- jags(data=jagsData, parameters.to.save=params, inits=NULL, model.file=model); k1
# ...                      mu.vect sd.vect    2.5%     25%     50%     75%   97.5%  Rhat n.eff
# delta.between.group     4.971   2.945   0.221   2.353   4.967   7.594   9.791 1.008   260
# grand.mean             52.477  11.321  23.453  47.914  53.280  58.246  74.080 1.220    20
# group.delta[1]          6.017  11.391 -16.095   0.448   5.316  10.059  34.792 1.152    21
# group.delta[2]          5.662  11.318 -15.836   0.054   5.009  10.107  33.548 1.139    21
# group.mean[1]          58.494   3.765  50.973  56.188  58.459  60.838  66.072 1.001  3000
# group.mean[2]          58.139   2.857  52.687  56.366  58.098  59.851  63.999 1.003   920
# group.within.sigma[1]  12.801   2.766   8.241  10.700  12.446  14.641  18.707 1.002  1100
# group.within.sigma[2]   9.274   2.500   5.688   7.475   8.834  10.539  15.700 1.002  1600
# sigma.between.group    18.031  21.159   0.553   3.793   9.359  23.972  82.604 1.006  1700
# deviance              149.684   2.877 145.953 147.527 149.081 151.213 156.933 1.001  3000

## VOI

posteriorXs <- k1$BUGSoutput$sims.list[["grand.mean"]]
MSE <- function(x1, x2) { (x2 - x1)^2 }
lossFunction <- function(x, predictions) { mean(sapply(predictions, function(x2) { MSE(x, x2)}))}
## our hypothetical mean-squared loss if we predicted, say, X=60:
lossFunction(60, posteriorXs)
# [1] 184.7087612
## of the possible values for X, 1-100, what value of X minimizes our squared error loss?
losses <- sapply(c(1:100), function (n) { lossFunction(n, posteriorXs);})
which.min(losses)
# [1] 52
## 52 also equals the mean estimate of X, which is good since it's well known that the mean is what minimizes
## the loss when the loss is squared-error so it suggests that I have not screwed up the definitions
losses[52]
[1] 128.3478462

## to calculate EVSI, we repeatedly simulate a few hundred times the existence of a hypothetical 'C' measurement
## and draw n samples from it;
## then we add the C data to our existing A & B data; run our Bayesian multilevel model again on the bigger dataset;,
## calculate what the new loss is, and compare it to the old loss to see how much the new data
## reduced the loss/mean-squared-error.
## Done for each possible n (here, 1-30) and averaged out, this tells us how much 1 additional datapoint is worth,
## 2 additional datapoints, 3 additional datapoints, etc.
sampleValues <- NULL
for (i in seq(from=1, to=30)) {

    evsis <- replicate(500, {
        n <- i

        C <- toplevel + runif(1, min=-10, max=10)
        C_1 <- rnorm(n=n, mean=C, sd=10)
        ## all as before, more or less:
        newData <- rbind(data, data.frame(Measurement="C", Y=C_1))

        jagsData <- list(N=nrow(newData), Y=newData$Y, K=length(levels(newData$Measurement)),
                         Group=newData$Measurement)
        params <- c("grand.mean","delta.between.group", "sigma.between.group", "group.delta", "group.mean",
                    "group.within.sigma")
        jEVSI <- jags(data=jagsData, parameters.to.save=params, inits=NULL, model.file=model)

        posteriorTimesEVSI <- jEVSI$BUGSoutput$sims.list[["grand.mean"]]
        lossesEVSI <- sapply(c(1:100), function (n) { lossFunction(n, posteriorTimesEVSI);})

        oldOptimum <- 128.3478462 # losses[52]
        newOptimum <- losses[which.min(lossesEVSI)]
        EVSI <- newOptimum - oldOptimum
        return(EVSI)
        }
        )

    print(i)

    print(mean(evsis))
    sampleValues[i] <- mean(evsis)
}
sampleValues
#  [1] 13.86568780 11.07101087 14.15645538 13.05296681 11.98902668 13.86866619 13.65059093 14.05991443
#  [9] 14.80018511 16.36944874 15.47624541 15.64710237 15.74060632 14.79901214 13.36776390 15.35179426
# [17] 14.31603459 13.70914727 17.20433606 15.89925289 16.35350861 15.09886204 16.30680175 16.27032067
# [25] 16.30418553 18.84776433 17.86881713 16.65973397 17.04451609 19.17173439

## As expected, the gain in reducing MSE continues increasing as data comes in but with diminishing returns;
## this is probably because in a multilevel model like this, you aren't using the _n_ datapoints to estimate X
## directly so much as you are using them to estimate a much smaller number of latent variables, which are then
## the _n_ used to estimate X. So instead of getting hyperprecise estimates of A/B/C, you need to sample from additional
## groups D/E/F/... Trying to improve your estimate of X by measuring A/B/C many times is like trying to estimate
## IQ precisely by administering a WM test a hundred times.

## If we wanted to compare with alternatives like instead sampling n data points from C and a D, it's easy to modify
## the EVSI loop to do so: generate `D <- toplevel + runif(1, min=-10, max=10); D_1 <- rnorm(n=n, mean=D, sd=10)`
## and now `rbind` D_1 in as well. At a guess, after 5-10 samples from the current group, estimates of X will be improved more
## by then sampling from a new group.

## Or the loss function could be made more realistic. It's unlikely one is paid by MSE, and if one adds in how much
## money each sample costs, with a realistic loss function, one could decide exactly how much data is optimal to collect.

## To very precisely estimate X, when our measurements are needed to measure at least 3 latent variables,
## requires much more data than usual.

## In general, we can see the drawbacks and benefits of each approach. A canned MLM
## is very fast to write but doesn't let us include prior information or easily run
## additional analyses like how much additional samples are worth. SEM works poorly
## on small samples but is still easy to write in if we have more complicated
## models of measurement error. A full-blown modeling language like JAGS is quite
## difficult to write in and MCMC is slower than other approaches but handles small
## samples without any errors or problems and offers maximal flexibility in using
## the known prior information and then doing decision-theoretic stuff. Overall for
## this problem, I think JAGS worked out best, but possibly I wasn't using LAVAAN
## right and that's why SEM didn't seem to work well.
~~~

# Value of Information: clinical prediction instruments for suicide

http://slatestarcodex.com/2015/08/31/magic-markers/#comment-232970

I agree. When criticizing the study for claiming the blood levels added predictive power and it's not clear they did, this is solely a statistical claim and can be done in a vacuum. But when one then goes on to pan the predictive power of the underlying clinical prediction instruments as useless in all circumstances, based on just the prediction stats:

> So when people say "We have a blood test to diagnose suicidality with 92% accuracy!", even if it's true, what they mean is that they have a blood test which, if it comes back positive, there's still less than 50-50 odds the person involved is suicidal. Okay. Say you're a psychiatrist. There's a 48% chance your patient is going to be suicidal in the next year. What are you going to do? Commit her to the hospital? I sure hope not. Ask her some questions, make sure she's doing okay, watch her kind of closely? You're a psychiatrist and she's your depressed patient, you would have been doing that anyway. This blood test is not really actionable. And then remember that this isn't the blood test we have. We have some clinical prediction instruments that do this...But having "a blood test for suicide" won't be very useful, even if it works.

One is implicitly making some strong cost-benefit claims here and stepping from statistics ('what are the probabilities?') to decision theory ('given these probabilities, how should I act?'). They are not identical: no AUC graph will ever tell you if a model's predictions are useful or not, and there is no universal threshold where 92% specificity/sensitivity is totally useless but 95% would make a difference - these clinical prediction instruments might be useless indeed, but that will depend on costs, base rates, and available actions. (I tried to make this point to Coyne on Twitter earlier but I don't think he understood what I was getting at & he blew me off.)

Discontinuities come from our actions; our inferences are incremental. There are some contexts where a tiny 1% improvement in AUC might be worth a lot (Wall Street) and there are some contexts where sensitivity or specificity of 99% is still useless because it won't change your actions at all (I'm currently comparing my riding lawn mower to a robotic lawn mower, and thus far, it doesn't matter how precise my parameters are, the robotic lawn mowers are, to my disappointment, just too expensive right now). I think p-values have shown us how well arbitrary thresholds work out in practice (and remember where they came from in the first place! decision rules set per problem - Gosset, in optimizing a brewery, did not have the pathologies we have with p<0.05 fetishism.) I also don't believe your choices are really that restricted: you mean if you were absolutely convinced that your patient was about to commit suicide, there is absolutely nothing you could do besides treat them like any other depressive? That seems unlikely. But whatever, even if commitment is the <i>only</i> alternative, there is still a value to the information provided by a clinical prediction instrument, and we can calculate it, and you should if you want to rule it out as having any value, in the same way that in criticizing a study as weak, it's better to ignore the p-values and just work out the right posterior and demonstrate directly how little evidence it contains.

---

Let's try this as an example, it's not hard or terribly complex (just tedious). So we have a ward of 100 depressive patients where we are interested in preventing suicide; our prior probability is that 7.5% or ~7 of them will commit suicide. The value of a life has been given a lot of different valuations, but \$10 million is a good starting point.

Action 1:

What are our costs or losses? We could say that we expect a loss of 7.5*\$10m or -\$75m, and if we stand by and do no treatment or intervention whatsoever, we spend no more money and so the total loss is

0 + 0.075 * 100 * 10,000,000 = -\$75,000,000

Action 2:

Let's say they all stay by default for one week and this costs a net \$1000 a day; let's say further that, since commitment is the mentioned alternative, while committed a suicide attempt will fail. And since we know that suicides are so often spontaneous and major depression comes and goes, a frustrated suicide attempt doesn't simply mean that they will immediately kill themselves as soon as they get out. This 7% comes from a followup period of a year, so the probability any will attempt suicide in the next week might be 0.075/52 or 0.001442307692. So this gives us our default setup: we have 100 patients staying for 7 days at a net cost of \$1000 a day or \$700,000 total, and by having them stay, we stop an expected average of 0.14 suicides and thus we prevent an expected loss of 0.14 * \$10m = \$1,440,000, for a total loss of treatment-cost minus treatment-gain plus remaining-loss:

\$700,000 - (0.14 * \$10m) - \$10m * 100 * (0.075-(0.075/52)) = -\$74,257,692.

Note that this loss is smaller than in the scenario in which we don't do any commitment at all; since one week of suicide-watch reduced the suicide loss more than it cost, this is not surprising.

Specifically, the benefit is:

action1 - action2 = gain to switching
75000000 - 74257692 = \$742,308

Not fantastic, but it's in the right order of magnitude (you can't expect more from a low base-rate event and a treatment with such a low probability of making a difference, after all) so it looks plausible, and it's still more than zero. We can reject the action of not committing them at all as being inferior to committing them for one week.

Action 3:

What if we were instead choosing between one week and committing them for a full year - thus catching the full 7.5% of suicides during the 1-year followup? Does that work? First, the loss from this course of action:

((100\*365.2\*1000) - (0 * 10000000) - (10000000 * 100 * (0.075-(0.075/1)))) = -\$36,520,000

Since there are no suicides, we avoid the default loss of -\$75m, but we still have to spend \$36,520,000 to pay for the long-term commitment. However, the benefit to the patients has increased dramatically since we stop so many more suicides:

action 2 - action 3 = \$35,637,692.31

(We go from a loss of -\$74m to a loss of -\$36m.) So we see action 3 is even better than action 2 for the patients. Of course, we can't extrapolate out any further than 1 year, because that's what our followup number is, and we don't know how the suicide risk falls after the 1 year point - if it drops to ~0, then further commitment is a terrible idea. So I'm not going to calculate out any further. (Since this is all linear stuff, the predicted benefit will increase smoothly over the year and so there's no point in calculating out alternatives like 1 month, 3 months, 6 months, 9 months, etc.) What's that, action 3 is totally infeasible and no one would ever agree to this - the patients would scream their heads off and the health insurance companies would never go for it - even if we could show that long commitments do reduce the suicide rate enough to justify the costs? And, among other things, I've oversimplified in assuming the 7% risk is evenly distributed over the year rather than a more plausible distribution like exponentially decreasing from Day 1, so likely commitment stops being a good idea more like month 3 or something? Yeah, you're probably right, so let's go back to using action 2's loss as our current best alternative.

Now, having set out some of the choices available, we can find out how much better information is worth. First, let's ask what the Expected Value of Perfect Information is: if we were able to take our 100 patients and exactly predict which 7 were depressive and would commit suicide this year in the absence of any intervention, where our choice is between committing them for one week or not at all. Given such information we can eject the 93 who we now know were never a suicide risk, and we hold onto the 7 endangered patients, and we have a new loss of the commitment cost of 7 people for a week vs the prevented loss of the chance they will try to commit suicide that week of this year:

((7*7*1000) - (0.14 * 10000000) - (10000000 * 7 * (1-(1/52)))) = -\$70,004,846

How much did we gain from our perfect information? About \$4m:

74257692 - 70004846 = \$4,252,846

(This passes our sanity checks: additional information should never hurt us, so the amount should be >=\$0, but we are limited by the intervention to doing very little, so the ceiling should be a low amount compared to the total loss, which this is.)

So as long as the perfect information did not cost us more than \$4m or so, we would have net gained from it: we would have been able to focus commitment on the patients at maximal risk. So suppose we had a perfect test which cost \$1000 a patient to run, and we wanted to know if the gained information was valuable enough to bother with using this expensive test; the answer in this case is definitely <i>yes</i>: with 100 patients, it'll cost \$100,000 to run the test but it'll save \$4.25m for a net profit of \$4.15m. In fact, we would be willing to pay per-patient costs up to \$42k, at which point we hit break-even (4252846 / 100).

OK, so that's perfect information. What about <i>im</i>perfect information? Well, imperfect is a lot like perfect information, just, y'know - less so. Let's consider this test: with the same prior, a negative on it means the patient now has P=0.007 to commit suicide that year, and a positive means P=0.48, and the sensitivity/specificity at 92%. (Just copying that from OP & ButYouDisagree, since those sound plausible.) So when we run the test on our patients, we find of the 4 possible outcomes:

- 85.1 patients are non-suicidal and the test will not flag them
- 7.4 are non-suicidal but the test will flag them
- 6.9 are suicidal and the test will flag them
- 0.6 are suicidal but the test will not flag them

So if we decide whether to commit or not commit solely based on this test, we will send home 85.1 + 0.6 = 85.7 patients (and indeed 0.6/85.7=0.007), and we will retain the remaining 7.4 + 6.9 = 14.3 patients (and indeed, 6.9/14.3=0.48). So our loss is the wrongly ejected patient of 0.6 suicides plus the cost of committing 14.3 patients (both safe and at-risk) for a week in exchange for the gain of a small chance of stopping the suicide of the 6.9 actually at risk:

(10000000\*85.7\*0.007) + (14.3\*7\*1000) + (10000000 * (0.48*14.3) * (1-(1/52))) = -\$73,419,100

How much did we gain from our imperfect information? About \$0.8m:

74257692 - 73419100 = \$838,592

or \$8,385.92 per patient. (This passes our sanity check: greater than \$0, but much less than the perfect information. The exact amount may seem lame, but as a fraction of the value of perfect information, it's not too bad: the test gets us 20% - 838592 / 4252846 - of the way to perfection.)

And that's our answer: the test is not worth \$0 - it's worth \$8k. And once you know what the cost of administering the test is, you simply subtract it and now you have the Net Expected Value of Information for this test. (I can't imagine it costs \$8k to administer what this sounds like, so at least in this model, the value is highly likely >\$0.)

---

By taking the posterior of the test and integrating all the estimated costs and benefits into a single framework, we can nail down exactly how much value these clinical instruments could deliver if used to guide decision-making. And if you object to some particular parameter or assumption, just build another decision-theory model and estimate the new cost. For example, maybe commitment actually costs, once you take into account all the disruption to lives and other such side-effects, not \$1000 but net of \$5000 per day, what then? Then the gain halves to \$438,192, etc. And if it costs \$10000 then the test is worth nothing because you won't commit anyone ever because it's just way too expensive, and now you know it's worth \$0; or if commitment is so cheap that it's more like \$100 a day, then the test is also worth \$0 because you would just commit everyone (since breakeven is then a suicide probability way below 7%, all the way at ~0.4% which is still below the 0.7% which the test can deliver, so the test result doesn't matter for deciding whether to commit, so it's worth \$0), or if you adopt a more reasonable value of life like \$20m, the value of perfect information shoots up (obviously, since the avoided loss doubles) but the value of imperfect information drops like a stone (since now that one suicidal patient sent home blows away your savings from less committing) and the test becomes worthless; and playing with the formulas, you can figure out the various ranges of assumptions in which the test has positive value and estimate how much it has under particular parameters, and of course if parameters are uncertain, you can cope with that uncertainty by embedding this in a Bayesian model to get posterior distributions of particular parameters incorporating all the uncertainty.

So to sum up: there are no hard thresholds in decision-making and imposing them can cost us better decision-making, so to claim additional information is worthless, more analysis needed, and this analysis must be done with respect to the available actions & their consequences, which even under the somewhat extreme conditions here of very weak interventions & low base-rates, suggests that the value of this information is positive.

<!--
# Dysgenics

"Estimating the genotypic intelligence of populations and assessing the impact of socioeconomic factors and migrations.", Piffer https://thewinnower.com/papers/estimating-the-genotypic-intelligence-of-populations-and-assessing-the-impact-of-socioeconomic-factors-and-migrations

http://openpsych.net/forum/showthread.php?tid=127
http://openpsych.net/forum/showthread.php?tid=224

http://www.unz.com/akarlin/genetics-iq-and-convergence/

http://humanvarieties.org/2014/10/03/is-there-no-population-genetic-support-for-a-racial-hereditarian-hypothesis/

Piffer, Davide (2015): "A review of intelligence GWAS hits: their relationship to country IQ and the issue of spatial autocorrelation", _Intelligence_
http://dx.doi.org/10.6084/m9.figshare.1393160

Factor Analysis of Population Allele Frequencies as a Simple, Novel Method of Detecting Signals of Recent Polygenic Selection: The Example of Educational Attainment and IQ. D Piffer
D Piffer
Mankind Quarterly 54 (2), 168-200

Statistical associations between genetic polymorphisms modulating executive function and intelligence suggest recent selective pressure on cognitive abilities.
D Piffer
Mankind Quarterly 54 (1), 3-25

Opposite selection pressures on stature and intelligence across human populations
D Piffer
Open Behavioral Genetics

The genetic correlation between educational attainment, intracranial volume and IQ is due to recent polygenic selection on general cognitive ability
D Piffer, E Kirkegaard
Open Behavioral Genetics


Piffer, D. (2014). Estimating strength of polygenic selection with principal components analysis of spatial genetic variation. bioRxiv, 008011.

https://www.google.com/search?num=100&q=%22Date%20of%20Birth%22%20site%3Amy.pgp-hms.org%2Fprofile%2F eg https://my.pgp-hms.org/profile/hu594129 https://workbench.su92l.arvadosapi.com/collections/22d61dd43786c65cd175b04ad6954af0+3119/html/index.html 1252 people with registered birthdates according to `demographics.tsv`? but how many with genetics...
https://opensnp.org/phenotypes/148#users eg https://opensnp.org/users/3866
http://www.snpedia.com/index.php/Genomes
http://genomesunzipped.org/data

Rietveld et al 2013

0.1 = +1 month of schooling
SNP Allele freq Beta
--- ----   ---- ----
rs9320913  A 0.483 0.101
rs3783006  C 0.454 0.088
rs8049439 T 0.581 0.086
rs13188378 A 0.878 -0.097

> On average, subjects have 13.3 years of schooling

> For EduYears, the strongest effect identified (rs9320913) explains 0.022% of phenotypic variance in the replication sample. This R^2 corresponds to a difference of ~1 months of schooling per allele.

the 4 alleles have almost the same effect size (betas of 0.1), so the top 4 then have an R^2 of (0.0022*3)=0.0066?
then to get pearson's r:
r=sqrt(0.0022*3)=0.08124038405

so the r of the 4 rietveld hits with years of education is r=0.08.
Years of education, while g-loaded, is not IQ.

http://differentialclub.wdfiles.com/local--files/definitions-structure-and-measurement/Intelligence-Knowns-and-unknowns.pdf
> Correlations between IQ scores and total years of education are about .55, implying that differences in psychometric intelligence account for about 30% of the outcome variance.

so years of education and IQ are r=0.55 (this is not r^2 since we can see it's bigger than the other number: 0.55^2=0.30).
this makes years of education a decent but imperfect measure of IQ
if we assume a path model of genes->education->IQ with no other arrows, then the correlation of genes/IQ would be:

0.081 * 0.55 = 0.04455

We might ask about power in a _t_-test:

r->d = sqrt((4*r^2) / (1 - r^2))
sqrt((4*0.04455^2) / (1 - 0.04455^2))
d=0.08918855033
R> power.t.test(power=0.8, d=0.08918855033)

     Two-sample t test power calculation

              n = 1974.380528

for polygenic score predicting 2%:

r^2=0.02
r=sqrt(0.02)
r=0.1414213562
0.14*0.55=0.077
d=sqrt((4*0.077^2) / (1 - 0.077^2))=0.154

     Two-sample t test power calculation

              n = 659.4424804



rbivariate <- function(r, mean.x = 0, sd.x=1, mean.y=0, sd.y=1, n=1) {
 z1 <- rnorm(n)
 z2 <- rnorm(n)
 x <- sqrt(1-r^2)*sd.x*z1 + r*sd.x*z2 + mean.x
 y <- sd.y*z2 + mean.y
 return(list(x,y))
}

> To evaluate the combined explanatory power, we constructed a linear polygenic score (5) for each of our two education measures using the meta-analysis results (combining discovery and replication), excluding one cohort. We tested these scores for association with educational attainment in the excluded cohort. We constructed the scores using SNPs whose nominal p-values fall below a certain threshold, ranging from 5x10^-8 (only the genome-wide significant SNPs were included) to 1 (all SNPs were included)....To explore one of the many potential mediating endophenotypes, we examined how much the same polygenic scores (constructed to explain EduYears or College) could explain individual differences in cognitive function. While it would have been preferable to explore a richer set of mediators, this variable was available in STR, a dataset where we had access to the individual-level genotypic data. Cognitive function had been measured in a subset of males using the Swedish Enlistment Battery (used for conscription) (5, 17). The estimated R^2 ≈ 2.5% (p < 1.0x10^−8 ) for cognitive function is actually slightly larger than the fraction of variance in educational attainment captured by the score in the STR sample. One possible interpretation is that some of the SNPs used to construct the score matter for education through their stronger, more direct effects on cognitive function (5). A mediation analysis (table S24) provides tentative evidence consistent with this interpretation.

r^2=0.025
r=sqrt(0.025)=0.158113883
d=sqrt((4*0.158^2) / (1 - 0.158^2))=0.32
R> power.t.test(power=0.8, d=0.32)
     Two-sample t test power calculation

              n = 154.2646926

The dysgenics hypothesis argues that due to observed reproductive patterns where the highly educated or intelligent tend to have fewer offspring, genotypic IQ (the upper bound on phenotypic IQs set by genes and the sort of thing measured by a polygenic score).
Woodley summarizes a number of estimates:

> Early in the 20th century, negative correlations were observed between intelligence and fertility, which were taken to indicate a dysgenic fertility trend (e.g. Cattell, 1936; Lentz, 1927; Maller, 1933; Sutherland, 1929). Early predictions of the rate of dysgenesis were as high as between 1 and 1.5 IQ points per decade (Cattell, 1937, 1936)...In their study of the relationship between intelligence and both completed and partially completed fertility, van Court and Bean (1985) reported that the relationships were predominantly negative in cohorts born between the years 1912 and 1982...Vining (1982) was the first to have attempted an estimation of the rate of genotypic IQ decline due to dysgenesis with reference to a large national probability cohort of US women aged between 24 and 34 years in 1978. He identified significant negative correlations between fertility and IQ ranging from −.104 to −.221 across categories of sex, age and race, with an estimated genotypic IQ decline of one point a generation. In a 10year follow-up study using the same cohort, Vining (1995) re-examined the relationship between IQ and fertility, now that fertility was complete, finding evidence for a genotypic IQ decline of .5 points per generation. Retherford and Sewell (1988) examined the association between fertility and IQ amongst a sample of 9000 Wisconsin high-school graduates (graduated 1957). They found a selection differential that would have reduced the phenotypic IQ by .81 points per generation under the assumption of equal IQs for parents and children. With an estimate of .4 for the additive heritability of IQ, they calculated a more modest genotypic decline of approximately .33 points. The study of Ree and Earles (1991), which employed the NLSY suggests that once the differential fertility of immigrant groups is taken into consideration, the phenotypic IQ loss amongst the American population may be greater than .8 of a point per generation. Similarly, in summarizing various studies, Herrnstein & Murray (1994) suggest that "it would be nearly impossible to make the total [phenotypic IQ decline] come out to less than one point per generation. It might be twice that." (p. 364). Loehlin (1997) found a negative relationship between the fertility of American women aged 35–44 in 1992 and their educational level. By assigning IQ scores to each of six educational levels, Loehlin estimated a dysgenesis rate of .8 points in one generation. Significant contributions to the study of dysgenesis have been made by Lynn, 1996 (see also: 2011) whose book Dysgenics: Genetic deterioration in modern populations provided the first estimates of the magnitude of dysgenesis in Britain over a 90 year period, putting the phenotypic loss at .069 points per year (about 1.7 points a generation assuming a generational length of 25 years). In the same study, Lynn estimated that the genotypic IQ loss was 1.64 points per generation between 1920 and 1940, which reduced to .66 points between 1950 and the present. Subsequent work by Lynn has investigated dysgenesis in other populations. For example Lynn (1999) found evidence for dysgenic fertility amongst those surveyed in the 1994 National Opinion Research Center survey, which encompassed a representative sample of American adults, in the form of negative correlations between the intelligence of adults aged 40+ and the number of children and siblings. Lynn estimates the rate of dysgenesis amongst this cohort at .48 points per generation. In a more recent study, Lynn and van Court (2004) estimated that amongst the most recent US cohort for which fertility can be considered complete (i.e. those born in the years 1940–1949), IQ has declined by .9 points per generation. At the country level, Lynn and Harvey (2008) have found evidence of a global dysgenesis of around .86 points between 1950 and 2000, which is projected to increase to 1.28 points in the period from 2000 to 2050. This projection includes the assumption that 35% of the varience in cross-country IQ differences is due to the influence of genetic factors. A subsequent study by Meisenberg (2009), found that the fertility differential between developed and developing nations has the potential to reduce the phenotypic world population IQ mean by 1.34 points per decade (amounting to a genotypic decline of .47 points per decade assuming Lynn & Harvey's 35% estimate). This assumes present rates of fertility and pre-reproductive mortality within countries. Meisenberg (2010) and Meisenberg and Kaul (2010) have examined the factors through which intelligence influences reproductive outcomes. They found that amongst the NLSY79 cohort in the United States, the negative correlation between intelligence and fertility is primarily associated with g and is mediated in part by education and income, and to a lesser extent by more “liberal” gender attitudes. From this Meisenberg has suggested that in the absence of migration and with a constant environment, selection has the potential to reduce the average genotypic IQ of the US population by between .4, .8 and 1.2 points per generation.

All of these estimates are genetic selection estimates: indirect estimates inferred from IQ being a heritable trait and then treating it as a natural selection/breeding process, where a trait is selected against based on phenotype and how fast the trait decreases in each succeeding generation depends on how genetic the trait is and how harsh the selection is.
So variation in these estimates (quoted estimates per generation range from .3 to 3+) is due to sampling error, differences in populations or time periods, expressing the effect by year or generation, the estimate used for heritability, reliability of IQ estimates, and whether additional genetic effects are taken into account - for example, Woodley et al 2015 http://www.sciencedirect.com/science/article/pii/S0191886915003712 finds -.262 points per decade from selection, but in another paper argues that paternal mutation load must be affecting intelligence http://www.sciencedirect.com/science/article/pii/S0191886914006278 by ~-0.84 in the general population given a total of -1 per decade.

Dysgenics effects should be observable by looking at genomes & genotypes with known ages/birth-years and looking for increases in total mutations or decreases in intelligence-causing SNPs, respectively.

## Mutation load

The paternal mutation load should show up as a increase (70 new mutations per generation, 35 years per generation, so ~2 per year on average) over the past century, while the genetic selection will operate by reducing the frequency of variants which increase intelligence.

If there are ~70 new mutations per generation and 2 harmful, and there is no longer any purifying selection so that all 70 will tend to remain present,  http://journals.plos.org/plosone/article?id=10.1371/journal.pone.0090097 "The Effect of Paternal Age on Offspring Intelligence and Personality when Controlling for Parental Trait Levels", Arslan et al 2014
mutation load review "Estimating the mutation load in human genomes", Henn et al 2015 /docs/2015-henn.pdf
"The deleterious mutation load is insensitive to recent population history", Simons et al 2014 /docs/2014-simons.pdf wiki/docs/2014-simons-supplementary.pdf ,  using data from "Analysis of 6,515 exomes reveals the recent origin of most human protein-coding variants", Fu et al 2012 /docs/2012-fu.pdf ; figure 3, number of single-nucleotide variants per person over the European-American sample, split by estimates of harm from least to most likely: `21345 + 15231 + 5338 + 1682 + 1969 = 45565`.
The supplementary tables gives a count of all observed SNVs by category, which sum to `300209 + 8355 + 220391 + 7001 + 351265 + 10293 = 897514`, so the average frequency must be `45565/897514=0.05`, and then the biomial SD will be `sqrt(897514*0.05*(1-0.05))=206.47`.
Considering the two-sample case of 1950 vs 2015, that's an increase of 130 total SNVs (`65*2`), which is 0.63SDs,

~~~{.R}
power.t.test(d=(130/206), power=0.8)
#
#      Two-sample t test power calculation
#
#               n = 40.40035398
# ...
~~~

A total of _n_=80.
This particular set up for the two-sample test can be seen as a linear model with the optimum design of allocating half the sample to each extreme (see ["Optimal design in psychological research"](http://www2.psych.ubc.ca/~schaller/528Readings/McClelland1997.pdf), McClelland 1997).
If samples were evenly allocated over all levels, then this suffers a penalty of requiring twice the sample size, so if the genomes were uniformly distributed 1950-2015, then we would require >160 genomes.

> We first analyzed single-nucleotide variant (SNV) frequency data from a recent exome sequencing study of 2,217 African Americans (AAs) and 4,298 European Americans (EAs) sequenced at 15,336 protein-coding genes by Fu et al. 6 (the allele frequencies are available from the National Heart, Lung, and Blood Institute (NHLBI) Grand Opportunity (GO) Exome Variant Server). Additionally, we analyzed exome data from 88 Yoruba (YRI) and 81 European (CEU) individuals collected by the 1000 Genomes Project 21...Figure 3 summarizes the results from the data of Fu et al. 6 . The mean allele frequency declines with increasing functional severity 5 from 2.8% at noncoding SNVs to 0.6% at probably damaging SNVs, implying that there is selection against most SNVs with predicted damaging effects.

## Selection on SNPs

Without formally meta-analyzing all dysgenics studies, a good starting point seems like a genetic selection of 1 point per decade or 0.1 points per year or 0.007 standard deviations per year (or 0.7 standard deviations per century).

The most common available genetic data is genotyped SNPs, which sequence only the variants most common in the general population; SNP data can look at the effects of genetic selection but will not look at new mutations (since a new mutation would not be common enough to be worth putting onto a SNP chip).

Given a large sample of SNP data, a birth year (or age), and a set of binary SNP variables which cause intelligence (coded as 1 for the good variant, 0 for the others), we could formulate this as a multivariate regression: glm(cbind(SNP1, SNP2, ... SNPN) ~ Year, family=binomial) and see if the year variable has a negative sign (increasing passage of time predicts lower levels of the good genes); if it does, this is evidence for dysgenics.
Better yet, given information about the effect size of the SNPs, we could for each person's SNP sum the net effects and then regress on a single variable, giving more precision rather than looking for independent effects on each SNP: lm(Polygenic_score ~ Year). Again a negative sign on the year variable is evidence for dysgenics.

Directional predictions are weak, and in this case we have quantitative predictions of how big the effects should be.
Most of the public genomes I looked at seem to have the earliest birthdates in the 1950s or so; genomes can come from any age person (parents can give permission, and sequencing has been done prenatally) so the maximum effect is the difference between 1950 and 2015, which is 65*0.007=0.455 standard deviations (but most genomes will come from intermediate birth-dates, which are less informative about the tempoeral trend - in the optimal experimental design for measuring a linear trend, half the samples would be from 1950 and the other half from 2015).
If the genetic total is going down by 0.455sd, how much do the frequencies of all the good genes go down?

One simple model of genotypic IQ would be to treat it as a large number of alleles of equal binary effect: a binomial sum of n=10,000 1/0 variables with p=50% (population frequency) is reasonable. In such a model, the average value of the sum is of course n*p=5000, and the sd is sqrt(n*p*(1-p)) or sqrt(10000*0.5*0.5) or 50. Applying our estimate of dysgenic effect, we would expect the sum to fall by 0.455*50=22.75, so we would be comparing two populations, one with a mean of 5000 and a dysgenic mean of 4977.25.
If we were given access to all alleles from a sample of 1950 and 2015 genomes and so we could construct the sum, how hard would it be able to tell the difference? In this case, the sum is normally distributed as there are more than enough alleles to create normality, so we can just treat this as a two-sample normally-distributed comparison of means (a _t_-test), and we already have a directional effect size in mind, -0.445sd, so:

~~~{.R}
power.t.test(delta=0.455, power=0.8, alternative="one.sided")
#
#      Two-sample t test power calculation
#
#               n = 60.4155602
# ...
~~~

A total n=120 is doable, but it is unlikely that we will know all intelligence genes anytime soon; instead, we know a few.
A new mean of 4977 implies that since total number of alleles is the same but the mean has fallen, the frequencies must also fall and the average frequency falls from 0.5 to 4977.25/10000=0.497725.
To go to the other extreme, if we know only a single gene and we want to test a fall from a frequency of 0.50 to 0.4977, we need infeasibly more samples:

~~~{.R}
power.prop.test(p1=0.5, p2=0.497725, power=0.8, alternative="one.sided")
#
#      Two-sample comparison of proportions power calculation
#
#               n = 597272.2524
# ...
~~~

We know a number of genes, though. Rietveld gives 4 good hits, so we can look at a polygenic score from that. They are all of similar effect size and frequency, so we'll continue under the same assumptions of 1/0 and p=50%. The non-dysgenic average score is 4*0.5=2, sd=sqrt(4*0.5*0.5)=1. (Naturally, the SD is *much* larger than before because with so few random variables...) The predicted shift is from frequencies of 0.5 to 0.497, so the dysgenic scores should be 4*0.497=1.988, sd=sqrt(4*0.497*0.503)=0.999. The difference of 0.012 on the reduced polygenic score is d=((2-1.988) / 0.999)=0.012, giving a necessary power of:

~~~{.R}
power.t.test(delta=0.012006003, power=0.8)
#
#      Two-sample t test power calculation
#
#               n = 108904.194
# ...
~~~

So the 4 hits do reduce the necessary sample size to detect a decrease, but it's still not feasible to require 200k genotypes (unless you are 23andMe or SSGAC or an entity like that).

In the current GWAS literature, there are ~9 hits we could use, but the [upcoming SSGAC paper promises: "We identified 86 independent SNPs associated with EA (p < 5E-8)."](http://drjamesthompson.blogspot.com/2015/09/scholar-in-86-snps.html "'86 genomic sites associated with educational attainment provide insight into the biology of cognitive performance', James J Lee, (at least 200 co-authors)").
So how much would 86 improve over 4?

- mean old: 86*0.5=43
- sd old=sqrt(86*0.5*0.5)=4.6368;
- mean new: 86*0.497=42.742
- sd new=sqrt(86*0.497*(1-0.497))=4.6367
- so d=(43-42.742)/4.63675=0.0556

~~~{.R}
power.t.test(delta=((43-42.742)/4.63675), power=0.8)
#
#      Two-sample t test power calculation
#
#               n = 5071.166739
# ...
~~~

so with 75, it drops from 200k to 10.1k.

To work backwards: we know with 1 hit, we need almost a million genomes (infeasible for any but the largest databanks, who have no interest in studying this hypothesis), and with all hits we need more like 200 genomes (entirely doable with just publicly available datasets like PGP), but how many hits do we need to work with an in-between amount of data like the ~2k genomes with ages I guess may be publicly available now or in the near future?

~~~{.R}
power.t.test(n=1000, power=0.8)
#     Two-sample t test power calculation
#
#              n = 1000
#          delta = 0.1253508704
hits=437; mean1=hits*0.5; sd1=sqrt(hits*0.5*0.5); mean2=hits*0.497; sd2=sqrt(hits*0.497*(1-0.497)); d=(mean1-mean2)/mean(c(sd1,sd2)); d
# [1] 0.1254283986
~~~

With a polygenic score drawing on 437 hits, then a sample of 2k suffices to detect the maximum decrease.

This is pessimistic because the 10k alleles are not all the same effect size and GWAS studies inherently will tend to find the largest effects first. So the first 4 (or 86) hits are worth the most.
The distribution of effects is probably something like an inverse exponential distribution: many small near-zero effects and a few large ones.
Rietveld 2013 released the betas for all SNPs, and [the beta estimates can be plotted](http://emilkirkegaard.dk/en/?p=5574 "Polygenic traits and the distribution of effect sizes: years of education from Rietveld et al (2013)"); each estimate is imprecise and there are some artifacts in beta sizes (probably due to rounding somewhere in the analysis pipeline), but the distribution looks like a radioactive half-life graph, an inverse exponential distribution.
With a mean of 1, we can simulate creating a set of 10k effect sizes which are exponentially distributed and have mean 5000 and SD close to (but larger than) 50 and mimicks closely the binomial model:

~~~{.R}
effects <- sort(rexp(10000)/1, decreasing=TRUE)
genomeOld <- function() { ifelse(sample(c(FALSE,TRUE), prob=c(0.5, 0.5), 10000, replace = TRUE), 0, effects) }
mean(replicate(10000, sum(genomeOld())))
# [1] 5000.270218
sd(replicate(10000, sum(genomeOld())))
# [1] 69.82652816
genomeNew <- function() { ifelse(sample(c(FALSE,TRUE), prob=c(0.497, 1-0.497), 10000, replace = TRUE), 0, effects) }
~~~

With a dysgenic effect of -0.445sds, that's a fall of the sum of random exponentials of ~31, which agrees closely with the difference in polygenic genome scores:

~~~{.R}
mean(replicate(10000, sum(genomeOld() - genomeNew())))
# [1] 29.75354558
~~~

For each draw from the old and new populations, we can take the first 4 alleles, which were the ones assigned the largest effects, and build a weak polygenic score and compare means
For example:

~~~{.R}
polyNew <- replicate(1000, sum(genomeNew()[1:4]))
polyOld <- replicate(1000, sum(genomeOld()[1:4]))
t.test(polyOld, polyNew, alternative="greater")
#
#   Welch Two Sample t-test
#
# data:  polyOld and polyNew
# t = 0.12808985, df = 1995.8371, p-value = 0.8980908
# alternative hypothesis: true difference in means is not equal to 0
# 95 percent confidence interval:
#  -0.7044731204  0.8029267301
# sample estimates:
#   mean of x   mean of y
# 17.72741040 17.67818359
~~~

Or to mimic 86 hits:

~~~{.R}
t.test(replicate(1000, sum(genomeOld()[1:86])), replicate(1000, sum(genomeNew()[1:86])))
#
#     Welch Two Sample t-test
#
# t = 1.2268929, df = 1997.6307, p-value = 0.2200074
# alternative hypothesis: true difference in means is not equal to 0
# 95% confidence interval:
#  -0.8642674547  3.7525210076
# sample estimates:
#   mean of x   mean of y
# 244.5471658 243.1030390
~~~

Using the exponential simulation, we can do a parallelized power analysis: simulate draws (i=200) & tests for a variety of sample sizes to get an idea of what sample size we need to get decent power with 86 hits.

~~~{.R}
library(ggplot2)
library(parallel)
library(plyr)
hits <- 86
sampleSizes <- seq(500, 5000, by=100)
iters <- 300
genomeOld <- function(efft) { ifelse(sample(c(FALSE,TRUE), prob=c(0.5, 0.5), length(efft), replace = TRUE), 0, efft) }
genomeNew <- function(efft) { ifelse(sample(c(FALSE,TRUE), prob=c(0.497, 1-0.497), length(efft), replace = TRUE), 0, efft) }
simulateStudy <- function(n, hits) {
        effects <- sort(rexp(10000)/1, decreasing=TRUE)[1:hits]
        polyOld <- replicate(n, sum(genomeOld(effects)))
        polyNew <- replicate(n, sum(genomeNew(effects)))
        t <- t.test(polyOld, polyNew, alternative="greater")
        return(data.frame(N=n, P=t$p.value, PO.mean=mean(polyOld), PO.sd=sd(polyOld), PN.mean=mean(polyNew), PN.sd=sd(polyNew))) }
parallelStudies <- function(n, itr) { ldply(mclapply(1:itr, function(x) { simulateStudy(n, hits); })); }
powerExponential <- ldply(lapply(sampleSizes, function(n) { parallelStudies(n, iters) })); summary(powerExponential)
#        N              P                  PO.mean             PO.sd             PN.mean
#  Min.   : 500   Min.   :0.000000000   Min.   :222.5525   Min.   :23.84966   Min.   :221.2894
#  1st Qu.:1600   1st Qu.:0.002991554   1st Qu.:242.8170   1st Qu.:26.46606   1st Qu.:241.3242
#  Median :2750   Median :0.023639517   Median :247.2059   Median :27.04467   Median :245.7044
#  Mean   :2750   Mean   :0.093184735   Mean   :247.3352   Mean   :27.06300   Mean   :245.8298
#  3rd Qu.:3900   3rd Qu.:0.107997575   3rd Qu.:251.7787   3rd Qu.:27.64103   3rd Qu.:250.2157
#  Max.   :5000   Max.   :0.997322161   Max.   :276.2614   Max.   :30.67000   Max.   :275.7741
#      PN.sd
#  Min.   :23.04527
#  1st Qu.:26.45508
#  Median :27.04299
#  Mean   :27.05750
#  3rd Qu.:27.63241
#  Max.   :30.85065
powerExponential$Power <- powerExponential$P<0.05
powers <- aggregate(Power ~ N, mean, data=powerExponential); powers
# 1   500 0.2133333333
# 2   600 0.2833333333
# 3   700 0.2833333333
# 4   800 0.3133333333
# 5   900 0.3033333333
# 6  1000 0.3400000000
# 7  1100 0.4066666667
# 8  1200 0.3833333333
# 9  1300 0.4133333333
# 10 1400 0.4166666667
# 11 1500 0.4700000000
# 12 1600 0.4600000000
# 13 1700 0.4666666667
# 14 1800 0.4733333333
# 15 1900 0.5233333333
# 16 2000 0.5366666667
# 17 2100 0.6000000000
# 18 2200 0.5900000000
# 19 2300 0.5600000000
# 20 2400 0.6066666667
# 21 2500 0.6066666667
# 22 2600 0.6700000000
# 23 2700 0.6566666667
# 24 2800 0.7133333333
# 25 2900 0.7200000000
# 26 3000 0.7300000000
# 27 3100 0.7300000000
# 28 3200 0.7066666667
# 29 3300 0.7433333333
# 30 3400 0.7133333333
# 31 3500 0.7233333333
# 32 3600 0.7200000000
# 33 3700 0.7766666667
# 34 3800 0.7933333333
# 35 3900 0.7700000000
# 36 4000 0.8100000000
# 37 4100 0.7766666667
# 38 4200 0.8000000000
# 39 4300 0.8333333333
# 40 4400 0.8466666667
# 41 4500 0.8700000000
# 42 4600 0.8633333333
# 43 4700 0.8166666667
# 44 4800 0.8366666667
# 45 4900 0.8666666667
# 46 5000 0.8800000000
qplot(N, Power, data=powers)  + stat_smooth()
# https://i.imgur.com/Fpj8iKN.png
~~~

So for a well-powered two-group comparison of 1950 & 2015 genotypes using 86 SNPs, we would want ~4000 in each group for a total _n_=8000; we do have nontrivial power even at a total _n_=1000 (500 in each group means 21% power) but a non-statistically-significant result will be difficult to interpret and if one wanted to do that, reporting a Bayes factor from a Bayesian hypothesis test would make much more sense to express clearly whether the (non-definitive) data is evidence for or against dysgenics.

This is still too optimistic since we assumed the optimal scenario of only very old and very new genomes, while available genomes are more likely to be distributed fairly uniformly between 1950 and 2015.
Per McClelland 1997, we would expect our sample size requirement to at least double to around _n_=16000, but we can do a power simulation here as well.
To get the effect size for each year, we simply split the frequency decrease over each year and generate hypothetical genomes with less of a frequency decrease uniformly distributed 1950-2015, and do a linear regression to get a _p_-value for the year predictor:

~~~{.R}
library(ggplot2)
library(parallel)
library(plyr)
hits <- 86
sampleSizes <- seq(8000, 30000, by=1000)
iters <- 100
genome <- function(effects) {
  t <- sample(c(1:(2015-1950)), 1)
  decreasedFrequency <- 0.5 - (((0.5-0.497)/(2015-1950)) * t)
  geneFlips <- sample(c(FALSE,TRUE), prob=c(decreasedFrequency, 1-decreasedFrequency), replace = TRUE, length(effects))
  geneValues <- ifelse(geneFlips, effects, 0)
  return(data.frame(Year=1950+t,
                    PolygenicScore=sum(geneValues)))
  }
simulateStudy <- function(n, hits) {
        effects <- sort(rexp(10000)/1, decreasing=TRUE)[1:hits]
        d <- ldply(replicate(n, genome(effects), simplify=FALSE))
        l <- lm(PolygenicScore ~ Year, data=d)
        p <- anova(l)$`Pr(>F)`[1]
        return(data.frame(N=n, P=p, PO.mean=predict(l, newdata=data.frame(Year=1950)),
                                           PN.mean=predict(l, newdata=data.frame(Year=2015)))) }
parallelStudies <- function(n, itr) { ldply(mclapply(1:itr, function(x) { simulateStudy(n, hits); })); }
powerExponentialDistributed <- ldply(lapply(sampleSizes, function(n) { parallelStudies(n, iters) })); summary(powerExponential)
powerExponentialDistributed$Power <- powerExponentialDistributed$P<0.05
powers <- aggregate(Power ~ N, mean, data=powerExponentialDistributed); powers
#        N Power
# 1   8000  0.27
# 2   9000  0.32
# 3  10000  0.35
# 4  11000  0.33
# 5  12000  0.41
# 6  13000  0.34
# 7  14000  0.41
# 8  15000  0.48
# 9  16000  0.55
# 10 17000  0.62
# 11 18000  0.55
# 12 19000  0.60
# 13 20000  0.69
# 14 21000  0.61
# 15 22000  0.65
# 16 23000  0.63
# 17 24000  0.71
# 18 25000  0.67
# 19 26000  0.71
# 20 27000  0.74
# 21 28000  0.70
# 22 29000  0.79
# 23 30000  0.83
qplot(N, Power, data=powers)  + stat_smooth()
# https://i.imgur.com/BYG9qds.png
~~~

In this case, the power simulation suggestions the need for triple rather than double the data, a total of _n_=30,000 to be well-powered.

## Genetic data availability

https://my.pgp-hms.org/public_genetic_data/statistics includes genotyping though
https://my.pgp-hms.org/users sort by 'Whole genome datasets', total of ~222?
looked at the first and last 22 entries; 34 had ages/birth-years, so  ~75%, indicating ~166 usable genomes

how fast is PGP genome dataset increasing? ~0.25 genomes per day:
https://my.pgp-hms.org/public_genetic_data?utf8=%E2%9C%93&data_type=Complete+Genomics&commit=Search
xclip -o|g '^hu'|g 'Complete Genomics'|cut -f 2
dates <- as.Date(c("2015-10-12","2015-10-11","2015-10-09","2015-10-03","2015-10-03","2015-10-01","2015-10-01","2015-10-01","2015-10-01","2015-10-01","2015-10-01","2015-09-29","2015-09-01","2015-08-31","2015-08-29","2015-08-10","2015-05-07","2015-05-07","2015-05-07","2015-04-07","2015-04-07","2015-04-07","2015-04-06","2015-04-06","2015-04-06","2015-04-06","2015-04-06","2015-04-06","2015-04-06","2015-03-18","2015-02-17","2014-04-19","2014-04-17","2014-04-17","2014-03-16","2014-03-07","2014-03-07","2014-03-07","2014-03-07","2014-03-02","2014-02-25","2014-02-24","2014-02-17","2014-02-12","2014-02-06","2014-02-04","2014-02-04","2014-02-04","2014-02-04","2014-01-24","2014-01-24","2014-01-23","2014-01-23","2014-01-23","2014-01-19","2013-09-07","2013-08-29","2013-08-12","2013-08-12","2013-08-12","2013-08-12","2013-08-12","2013-08-12","2013-08-12","2013-08-12","2013-08-12","2013-08-12","2013-08-12","2013-08-12","2013-08-12","2013-08-12","2013-08-12","2013-08-12","2013-08-12","2013-08-12","2013-08-12","2013-08-12","2013-08-12","2013-08-12","2013-08-12","2013-08-12","2013-08-12","2013-08-12","2013-08-12","2013-08-12","2013-08-12","2013-08-12","2013-08-12","2013-08-12","2013-08-12","2013-08-12","2013-08-12","2013-08-12","2013-08-12","2013-08-12","2013-08-12","2013-08-12","2013-08-12","2013-08-12","2013-08-12","2013-08-12","2013-08-12","2013-08-12","2013-08-12","2013-08-12","2013-08-12","2013-08-12","2013-08-12","2013-08-11","2013-08-09","2013-08-09","2013-08-09","2013-08-09","2013-08-09","2013-08-09","2013-08-09","2013-08-09","2013-08-09","2013-08-09","2013-08-09","2013-08-09","2013-08-09","2013-08-09","2013-08-09","2013-08-09","2013-08-09","2013-08-09","2013-08-09","2013-08-09","2013-08-09","2013-08-09","2013-08-09","2013-08-09","2013-08-09","2013-08-09","2013-08-09","2013-08-09","2013-08-09","2013-08-09","2013-08-09","2013-08-09","2013-08-09","2013-08-09","2013-08-09","2013-08-09","2013-08-09","2013-08-09","2013-08-09","2013-08-09","2013-08-09","2013-08-09","2013-08-09","2013-08-09","2013-08-09","2013-08-09","2013-08-09",
    "2013-08-09","2013-08-07","2013-08-07","2013-08-07","2013-08-07","2013-08-07","2013-08-07","2013-08-07","2013-08-07","2013-08-07","2013-08-07","2013-08-07","2013-08-07","2013-08-07","2013-08-07","2013-08-07","2013-08-07","2013-08-07","2013-08-07","2013-08-07","2013-08-07","2013-08-07","2013-08-07","2013-08-07","2013-08-07","2013-08-07","2013-08-07","2013-08-07","2013-08-07","2013-08-07","2013-08-07","2013-08-07","2013-08-07","2013-08-07","2013-08-07","2013-08-07","2013-08-07","2013-08-07","2013-08-07","2013-08-07","2013-08-07","2013-08-07","2013-08-07","2013-08-07","2013-08-07","2013-08-07","2013-08-07","2013-08-07","2013-08-07","2013-08-07","2013-08-07","2013-08-07","2013-08-07","2013-08-07","2013-08-07","2013-04-25","2013-04-25","2013-04-25","2013-04-25","2013-04-25","2013-04-25","2013-04-25","2013-04-25","2013-04-25","2013-04-25","2013-04-25","2013-04-25","2013-04-25","2013-04-25","2013-04-25","2013-04-25","2013-04-25","2013-04-25","2013-04-25","2013-04-25","2013-04-25","2013-04-25","2013-04-25","2013-04-25","2013-04-25","2013-04-25","2013-04-25","2013-04-25","2013-04-25","2013-04-25","2013-04-25","2013-04-25","2013-04-25","2013-04-25","2013-04-25","2013-04-25","2013-04-25","2013-04-25","2013-04-25","2013-04-25","2013-04-25","2013-04-25","2013-04-25","2013-04-25","2013-04-25","2013-04-25","2013-04-25","2013-04-25","2013-04-25","2013-04-25","2013-04-25","2013-04-25","2013-04-25","2013-04-25","2013-04-25","2013-04-25","2013-04-25","2013-04-25","2013-04-25","2013-04-25","2013-04-25","2013-04-25","2013-04-25","2013-04-25","2013-04-25","2013-04-25","2013-04-25","2013-04-25","2013-04-25","2013-04-25","2013-04-25","2013-04-25","2013-04-25","2013-04-25","2013-04-25","2013-04-25","2013-04-25","2013-04-25","2013-04-25","2013-04-25","2013-04-25","2013-04-25","2013-04-25","2013-04-25","2013-04-25","2013-04-18","2013-04-06","2013-03-27","2013-03-27","2013-02-28","2013-02-26","2013-02-12","2013-02-10","2013-02-09","2013-02-08","2013-02-08","2013-02-08","2013-02-08","2013-02-04","2013-02-04","2013-01-29","2013-01-19","2013-01-15","2013-01-13","2013-01-10","2013-01-10","2013-01-10","2013-01-09","2013-01-09","2013-01-09","2012-12-14","2012-12-14","2012-12-14","2012-12-14","2012-12-14","2012-12-14","2012-12-14","2012-12-06","2012-11-13","2012-11-12","2012-11-09","2012-11-09","2012-11-09","2012-11-09","2012-10-30","2012-10-25","2012-10-22","2012-10-19","2012-10-19","2012-10-19","2012-10-19","2012-10-17","2012-10-16","2012-10-16","2012-10-15","2012-10-15","2012-09-18","2012-08-08","2012-08-08","2012-08-08","2012-08-08","2012-08-08","2012-07-17","2012-07-09","2012-07-08","2012-07-08","2012-07-07","2012-07-07","2012-07-07","2012-07-07","2012-07-01","2012-06-27","2012-02-03","2012-02-03","2011-09-16","2011-09-16","2011-09-16","2011-09-16","2011-09-16","2011-09-16","2011-09-16","2011-09-16","2011-09-16","2011-09-16","2011-09-16"))
length(dates) / (as.integer(max(dates)) - as.integer(min(dates)))
[1] 0.2528581036

So to let the birth-date set double, we need to wait 166=x*0.25*0.75=885 more days

What about SNPs? well, looking at the 23andMe sourced data:
https://my.pgp-hms.org/public_genetic_data?utf8=%E2%9C%93&data_type=23andMe&commit=Search
total of 656
between 2015-10-21 2011-01-06
 656 / (as.integer(as.Date("2015-10-21")) - as.integer(as.Date("2011-01-06")))
0.37 per day
for 30k, we will be waiting
(30000-(656*0.75))=x
(30000-(656*0.75))/(0.37*0.75)=x
106,335 days or 291 years

-->

# Bayesian Model Averaging

~~~{.R}
## original: "Bayesian model choice via Markov chain Monte Carlo methods" Carlin & Chib 1995 http://stats.ma.ic.ac.uk/~das01/MyWeb/SCBI/Papers/CarlinChib.pdf
## Kobe example & data from: "A tutorial on Bayes factor estimation with the product space method", Lodewyckx et al 2011 http://ejwagenmakers.com/2011/LodewyckxEtAl2011.pdf
## Lodewyckx code can be downloaded after registration & email from http://ppw.kuleuven.be/okp/software/scripts_tut_bfepsm/

## "Table 2: Observed field goals (y) and attempts (n) by Kobe Bryant during the NBA seasons of 1999 to 2006."
kobe <- read.csv(stdin(),header=TRUE)
Year, y,   n,    y.n
1999, 554, 1183, 0.47
2000, 701, 1510, 0.46
2001, 749, 1597, 0.47
2002, 868, 1924, 0.45
2003, 516, 1178, 0.44
2004, 573, 1324, 0.43
2005, 978, 2173, 0.45
2006, 399,  845, 0.47

library(runjags)
model1<-"model{
      # 1) MODEL INDEX
      # Model index is 1 or 2.
      # Prior probabilities based on argument prior1.
      # Posterior probabilities obtained by averaging
      # over postr1 and postr2.
      M ~ dcat(p[])
      p[1] <- prior1
      p[2] <- 1-prior1
      postr1 <- 2-M
      postr2 <- 1-postr1

      # 2) MODEL LIKELIHOOD
      # For each year, successes are Binomially distributed.
      # In M1, the success rate is fixed over years.
      # In M2, the success rate is year-specific.
      for (i in 1:n.years){
       successes[i] ~ dbin(pi[M,i], attempts[i])

       pi[1,i] <- pi.fixed
       pi[2,i] <- pi.free[i]
      }

      # 3) MODEL 1 (one single rate)
      # The fixed success rate is given a Beta prior and pseudoprior.
      # Whether it is a prior or pseudoprior depends on the Model index.
      pi.fixed ~ dbeta(alpha.fixed[M],beta.fixed[M])
      alpha.fixed[1] <- alpha1.prior
      beta.fixed[1] <- beta1.prior
      alpha.fixed[2] <- alpha1.pseudo
      beta.fixed[2] <- beta1.pseudo

      # 4) MODEL 2 (multiple independent rates)
      # The year-specific success rate is given a Beta prior and pseudoprior.
      # Whether it is a prior or pseudoprior depends on the Model index.
      for (i in 1:n.years){
       pi.free[i] ~ dbeta(alpha.free[M,i],beta.free[M,i])
       alpha.free[2,i] <- alpha2.prior
       beta.free[2,i] <- beta2.prior
       alpha.free[1,i] <- alpha2.pseudo[i]
       beta.free[1,i] <- beta2.pseudo[i]
      }
      # predictive interval for hypothetical 2007 data in which Kobe makes 1000 shots:
      successes.new.1 ~ dbin(pi.fixed, 1000)
      successes.new.2 ~ dbin(pi.free[n.years], 1000)

#      success.new.weighted ~ dcat(M
  }"
# 'prior1' value from paper
data <- list("prior1"=0.000000007451, "n.years"= length(kobe$Year), "successes"=kobe$y, "attempts"=kobe$n,
             "alpha1.prior"=1, "beta1.prior"=1, "alpha2.prior"=1, "beta2.prior"=1,
             "alpha1.pseudo"=1, "beta1.pseudo"=1, "alpha2.pseudo"=rep(1,8), "beta2.pseudo"=rep(1,8) )
# inits <- function() { list(mu=rnorm(1),sd=30,t=as.vector(apply(mailSim,1,mean))) }
params <- c("pi.free", "pi.fixed", "postr1", "postr2", "M", "successes.new.1", "successes.new.2")
j1 <- run.jags(model=model1, monitor=params, data=data, n.chains=8, method="rjparallel", sample=500000); j1
# JAGS model summary statistics from 4000000 samples (chains = 8; adapt+burnin = 5000):
#
#                  Lower95  Median Upper95    Mean      SD Mode      MCerr MC%ofSD SSeff
# pi.free[1]        0.3145 0.46864 0.98709 0.47383 0.11553   -- 0.00041958     0.4 75810
# pi.free[2]       0.10099 0.46447 0.77535 0.47005  0.1154   -- 0.00042169     0.4 74887
# pi.free[3]       0.19415  0.4692 0.86566  0.4741 0.11457   -- 0.00040171     0.4 81342
# pi.free[4]      0.020377 0.45146 0.69697 0.45867 0.11616   -- 0.00042696     0.4 74023
# pi.free[5]      0.024472 0.43846  0.7036 0.44749 0.11757   -- 0.00043352     0.4 73548
# pi.free[6]      0.076795 0.43325 0.74944 0.44318 0.11684   -- 0.00043892     0.4 70863
# pi.free[7]       0.06405 0.45033 0.73614 0.45748 0.11541   -- 0.00041715     0.4 76543
# pi.free[8]       0.30293 0.47267 0.97338 0.47708 0.11506   -- 0.00040938     0.4 79000
# pi.fixed        0.039931 0.45756 0.97903 0.49256 0.26498   -- 0.00099537     0.4 70868
# postr1                 0       0       1 0.15601 0.36287    0    0.15113    41.6     6
# postr2                 0       1       1 0.84399 0.36287    1    0.15113    41.6     6
# M                      1       2       2   1.844 0.36287    2    0.15113    41.6     6
# successes.new.1        0     463     940  492.57  265.28  454    0.99543     0.4 71019
# successes.new.2      300     473     971  477.05  116.03  473     0.4152     0.4 78094
getLogBF <- function(prior0, postr0) { log((postr0/(1-postr0)) / (prior0/(1-prior0))) }
getLogBF(0.000000007451, 0.15601)
# [1] 17.02669704
## analytic BF: 18.79; paper's MCMC estimate: 18.80; not sure where I lost 1.8 of the BF.
~~~

# Dealing with all-or-nothing unreliability of data

> Given two disagreeing polls, one small & imprecise but taken at face-value, and the other large & precise but with a high chance of being totally mistaken, what is the right Bayesian model to update on these two datapoints?
> I give ABC and MCMC implementations of Bayesian inference on this problem and find that the posterior is bimodal with a mean estimate close to the large unreliable poll's estimate but with wide credible intervals to cover the mode based on the small reliable poll's estimate.

A question was asked of me: what should one infer if one is given what would be definitive data if one could take it at face value - but one suspects this data might be totally incorrect?
An example would be if one wanted to know what fraction of people would answer 'yes' to a particular question, and one had a very small poll (_n_=10) suggesting 90% say yes, but then one was also given the results from a much larger poll (_n_=1000) saying 75% responded yes - but this poll was run by untrustworthy people, people that, for whatever reason, you believe might make something up half the time.
You should be able to learn *something* from this unreliable poll, but you can't learn *everything* from it because you would be burned half the time.

If not for this issue of unreliability, this would be an easy binomial problem: specify a uniform or Jeffreys prior on what percentage of people will say yes, add in the binomial data of 9/10, and look at the posterior.
But what do we do with the unreliability joker?

## Binomial

First let's try the simple case, just updating on a small poll of 9/10. We would expect it to be unimodally peaked around 80-90%, but broad (due to the small sample size) and falling sharply until 100% since being that high is a priori unlikely.

MCMC using [Bayesian First Aid](https://github.com/rasmusab/bayesian_first_aid):

~~~{.R}
## install.packages("devtools")
## devtools::install_github("rasmusab/bayesian_first_aid")
library(BayesianFirstAid)
b <- bayes.binom.test(oldData$Yes, oldData$N); b
# ...number of successes = 9, number of trials = 10
# Estimated relative frequency of success:
#   0.85
# 95% credible interval:
#   0.63 0.99
# The relative frequency of success is more than 0.5 by a probability of 0.994
# and less than 0.5 by a probability of 0.006
~~~

Which itself is a wrapper around calling out to JAGS doing something like this:

~~~{.R}
library(runjags)
model_string <- "model {
  x ~ dbinom(theta, n)
  theta ~ dbeta(1, 1) }"
model <- autorun.jags(model_string, monitor="theta", data=list(x=oldData$Yes, n=oldData$N)); model
# JAGS model summary statistics from 20000 samples (chains = 2; adapt+burnin = 5000):
#
#       Lower95  Median Upper95    Mean      SD Mode     MCerr MC%ofSD SSeff    AC.10   psrf
# theta 0.63669 0.85254  0.9944 0.83357 0.10329   -- 0.0007304     0.7 20000 0.011014 1.0004
~~~

Here is a simulation-based version of Bayesian inference using [ABC](!Wikipedia "Approximate Bayesian computation"):

~~~{.R}
oldData <- data.frame(Yes=9, N=10)
simulatePoll <- function(n, pr)  { rbinom(1, size=n, p=pr); }
poll_abc <- replicate(100000, {
    # draw from our uniform prior
    p <- runif(1,min=0,max=1)
    # simulate a hypothetical poll dataset the same size as our original
    newData <- data.frame(Yes=simulatePoll(oldData$N, p), N=oldData$N)
    # were they equal? if so, save sample as part of posterior
    if (all(oldData == newData)) { return(p) }
   }
  )
resultsABC <- unlist(Filter(function(x) {!is.null(x)}, poll_abc))
summary(resultsABC)
#      Min.   1st Qu.    Median      Mean   3rd Qu.      Max.
# 0.3260816 0.7750520 0.8508855 0.8336383 0.9117471 0.9991691
hist(resultsABC)
# https://i.imgur.com/fn3XYQW.png
~~~

They look identical, as they should.

### Binomial with binary unreliability

To implement our more complicated version: the original poll remains the same but we add in the complication of a very large poll which 50% of the time is a true measure of the poll response and 50% of the time is drawn uniformly at random.
(So if the true poll response is 90%, then half the time the large poll will yield accurate data like 905/1000 or 890/1000, and the rest it will yield 10/1000 or 400/1000 or 700/1000.)
This is different from the more common kinds of measurement-error models where it's generally assumed that the noisy measurements still have *some* informativeness to them; here there is none.

Specifically, this faux poll has yielded the data not 9/10, but 750/1000.

#### ABC

Using ABC again: we generate the reliable small poll as before, and we add in an faux poll where we flip a coin to decide if we are going to return a 'yes' count based on the population parameters or just a random number, then we combine the two datasets and check that it's identical to the actual data, saving the population probability if it is.

~~~{.R}
oldData2 <- data.frame(Yes=c(9,750), N=c(10,1000)); oldData2
#   Yes    N
# 1   9   10
# 2 750 1000
simulateHonestPoll <- function(n, pr)  { rbinom(1, size=n, p=pr); }
simulateFauxPoll <- function(n, pr, switchp) { if(sample(c(TRUE, FALSE), 1, prob=c(switchp, 1-switchp))) { rbinom(1, size=n, p=pr); } else { round(runif(1, min=0, max=n)); }}
poll_abc <- replicate(1000000, {
 priorp <- runif(1,min=0,max=1)
 switch <- 0.5
 n1 <- 10
 n2 <- 1000
 data1 <- data.frame(Yes=simulateHonestPoll(n1, priorp), N=n1)
 data2 <- data.frame(Yes=simulateFauxPoll(n2, priorp, switch), N=n2)
 newData <- rbind(data1, data2)
 if (all(oldData2 == newData)) { return(priorp) }
 }
)
resultsABC <- unlist(Filter(function(x) {!is.null(x)}, poll_abc))
summary(resultsABC)
#      Min.   1st Qu.    Median      Mean   3rd Qu.      Max.
# 0.5256471 0.7427098 0.7584650 0.7860109 0.8133581 0.9765648
hist(resultsABC)
# https://i.imgur.com/atMz0jg.png
~~~

The results are interesting and in this case the summary statistics are misleading: the median is indeed around 75% (as we would expect! since that's the result of the highly precise poll which has a 50% chance of being the truth) but we see the mean is being pulled away towards the original 90% estimate, and plotting the histogram, bimodality emerges.
The posterior reports that there's still a lot of credibility to the 90% point estimate, but between the original diffuseness of that posterior (leaving a lot of probability to lower responses including, say, 75%) and the high certainty that if accurate the responses will definitely be close to 75%, it winds up peaked at a little higher than 75% (since even if the larger poll is honest, the earlier poll did still find 9/10).
So it's not so much that we think the best estimate of true population rate really is 79% (indeed, the mode is more like 75%, but it could easily be far away from 75% and in the 90%s) as we would need to think more about what we want to do with this posterior before we decide how to summarize it.

#### Mixture

ABC is slow and would not scale to more hypothetical polls unless we abandoned exact ABC inference and began using approximate ABC (entirely possible in this case; instead of strict equality between the original and simulated data, we'd instead accept a sample of _p_ if the simulated dataset's fractions were within, say, 1% of the originals); and the simulation would need to be rewritten anyway.

MCMC can handle this if we think of our problem as a [mixture model](!Wikipedia): our problem is that we have poll data drawn from two clusters/distributions - one cluster is the true population distribution of opinion, and the other cluster just spits out noise. We have one observation which we know is from first reliable distribution (the 9/10 poll result), and one observation which we're not sure which of the two it came from (750/1000), but we do know that the indexing probability for mixing the two distributions is 50%.

In JAGS, we write down a model in which `dcat` flips between 1 and 2 if the cluster is not known, specifying which distribution a sample came from and its theta probability, and then we infer the thetas for both distributions.
Of course, we only care about the first distribution's theta since the second one is noise.

~~~{.R}
library(runjags)
model1 <- "model {
  for (i in 1:N) {
   y[i] ~ dbinom(theta[i], n[i])
   theta[i] <- thetaOfClust[ clust[i] ]
   clust[i] ~ dcat(pi[])
  }
  pi[1]  <- switch[1]
  pi[2]  <- switch[2]
  thetaOfClust[1] ~ dbeta(1,1)
  thetaOfClust[2] ~ dunif(0,1)
 }"
j1 <- autorun.jags(model1, monitor=c("theta"), data = list(N=nrow(oldData2), y=oldData2$Yes, n=oldData2$N, switch=c(0.5, 0.5), clust=c(1,NA))); j1
# ...      Lower95  Median Upper95    Mean       SD Mode      MCerr MC%ofSD SSeff    AC.10   psrf
# theta[1] 0.70582 0.75651 0.97263 0.77926  0.07178   --   0.001442       2  2478  0.12978 1.0011
# theta[2] 0.72446 0.75078 0.77814 0.75054 0.013646   -- 0.00009649     0.7 20000 0.009458      1
plot(j1)
# https://i.imgur.com/EaqR0dD.png
~~~

Sure enough, we get a good match with the ABC estimate: a mean estimate for the population distribution of 78% with a very wide 95% CI and a clearly bimodal distribution with a huge spike at 75%.
Since the MCMC mixture model looks completely different from the imperative simulation-based model, the consistency in estimates & distributions gives me some confidence in the results being right.

So we can see how we should update our beliefs - by a perhaps surprising amount towards the unreliable datapoint.
The original data was too weak to strongly resist the allure of that highly precise poll.

#### Weakening heuristic?

We might try to think of it this way: half the time, the large poll means nothing whatsoever, it contains 0% or no information about the population at all; While the other half of the time, it is exactly what it seems to be and 100% informative; so doesn't that mean that on average we should treat it as containing half the information we thought it did? And the information is directly based on the sample size: a sample 5x as big contains 5x as much information.
So perhaps in this case of all-or-nothing accuracy, we could solve it easily by simply weakening the weight put the unreliable information and shrinking the claimed sample size - instead of treating it as 750 of 1000, treat it as 375/500; and if it had been 75,000 of 100,000, convert it to 37,500 of 50,000.
This is a simple and intuitive shortcut, but if we think about what the binomial will return as the unreliable poll increases in size or if we look at the results...

~~~{.R}
switch <- 0.5
oldData3 <- data.frame(Yes=c(9,(750*switch)), N=c(10,(1000*switch)))
b2 <- bayes.binom.test(sum(oldData3$Yes), sum(oldData3$N)); b2
#
#   Bayesian First Aid binomial test
#
# data: sum(oldData3$Yes) and sum(oldData3$N)
# number of successes = 384, number of trials = 510
# Estimated relative frequency of success:
#   0.75
# 95% credible interval:
#   0.71 0.79
# The relative frequency of success is more than 0.5 by a probability of >0.999
# and less than 0.5 by a probability of <0.001
~~~

Unfortunately, this doesn't work because it doesn't preserve the bimodal aspect of the posterior, and we get a unimodal distribution ever concentrating on its mean, wiping out the existence of the 0.90 peak. If our untrustworthy poll had instead, say, reported 750,000 out of 1 million, that should only make the peak at 0.75 look like a needle - it should be unable to affect the mass around 0.9, because it doesn't matter if the data is 100 or 1 million or 1 billion, it still only has a 50% chance of being true.
It's a little hard to see this since the mean frequency of 0.75 is fairly close to the mean of 0.78 from the ABC and we might write this off as approximation error in either the ABC estimate or BFA's MCMC, but if we look at the 95% CI and note that 0.9 is not inside it or if we plot the posterior (`plot(b2)`), then the absence of bimodality jumps out.
So this trick doesn't work.

# RNN metadata for mimicking individual author style

idea: can we teach an RNN artistic style using inline metadata?
not quite supervised learning, not quite unsupervised
that is, instead of hardwiring into the architecture a few nodes corresponding to a 'style' indicator (which is work, would require deep modifications to char-rnn, and may not be flexible), or trying to hack the layers of the rnn directly like in the recent 'artistic style' cnn stuff, perhaps we could provide texts from multiple authors but embed in the text markers of authorship, which the RNN may gradually learn indicates different vocabularies and syntaxes.

If you told someone a quote was by Shakespeare, their expectations will be different than if you had told them, "As Charles Dickens once wrote...", and it seems reasonable to think that an RNN might be able to pick up on arbitrary prefixes and 'remember' them in its state-vector for hundreds of characters thereafter and adjust its prediction/style based on this memory.

So more concretely, we might provide in training the full corpus of Shakespeare, but with each line prefixed by 'SHAKESPEARE|', and the full texts of Dickens with each line prefixed with 'DICKENS|', etc; and then when we generate new text, we hint to the RNN which to emulate by specifying a seed 'DICKENS|'.
(Why the pipe character? Because it's rarely used in prose but isn't hard to type or work with.)
Depending on how well this works, you could imagine specifying lots of kinds of metadata in-band: not just author, but novel title and genre, orfor music RNNs, why not specify author, instrument, genre, and maybe a bunch of tags provided by music listeners ('energetic', 'kitties', 'for_running' etc)?
This would be especially helpful for musical applications of RNNs, which are limited in part by the difficulty of finding large textual musical datasets of a single consistent genre to train an RNN on - if you have a few hundred songs of Irish music written in ABC format and then you have a few dozen of rock or classical pieces written in MIDI, training an RNN on them all mixed together will simply yield gibberish output because you will get an 'average syntax' of ABC & MIDI and an 'average music' of Irish & Rock, which is of no interest to anyone.
With enough data, the RNN would *probably* learn to separate them on its own, since ABC and MIDI look very different but do you have that much data?
On the other hand, if you could make it easy on the RNN and included prefix keyword metadata like "MIDI|ROCK|..." or "ABC|IRISH|...", then perhaps it will be able to easily learn the different syntaxes & genres while still being able to benefit from the underlying similarities which makes it all music.

If we just download some complete works off Project Gutenberg (googling 'Project Gutenberg "complete works of"'), prefix each line with "$AUTHOR|", concatenate the complete works, and throw them into `char-rnn`, we should not expect good results: the author metadata will now make up something like 5% of the entire character count (because PG wraps them to short lines) and by training on 5M of exclusively Austen and then 5M of exclusively Churchill, we might run into overfitting problems and due to the lack of proximity of different styles, the RNN might not 'realize' that the author metadata isn't just some easily predicted & then ignored noise but can be used to predict far into the future.
We also don't want the PG headers explaining what PG is and make sure the files are all converted to ASCII.

So to deal with these 4 issues I'm going to process the PG collected works thusly:

1. delete the first 80 lines and filter out any line mentioning "Gutenberg"
2. convert to ASCII
3. delete all newlines and then rewrap to make lines which are 10000 bytes - long enough to have a great deal of internal structure and form a good batch to learn from, and thus can be randomly sorted with the others.

    But newlines *do* carry semantic information - think about dialogues - and does deleting them carry a cost? Perhaps we should map newlines to some rare character like tilde, or use the poetry convention of denoting newlines with forward-slashes.
4. prefix each long line with the author it was sampled from

I grabbed 7 authors, giving a good final dataset of 46M:

~~~{.Bash}
cd ~/src/char-rnn/data/
mkdir ./styles/ ; cd ./styles/

## "The Complete Project Gutenberg Works of Jane Austen" http://www.gutenberg.org/ebooks/31100
wget 'https://www.gutenberg.org/ebooks/31100.txt.utf-8' -O austen.txt
## "The Complete Works of Josh Billings" https://www.gutenberg.org/ebooks/36556
wget 'https://www.gutenberg.org/files/36556/36556-0.txt' -O billings.txt
## "Project Gutenberg Complete Works of Winston Churchill" http://www.gutenberg.org/ebooks/5400
wget 'https://www.gutenberg.org/ebooks/5400.txt.utf-8' -O churchill.txt
## "The Project Gutenberg Complete Works of Gilbert Parker" https://www.gutenberg.org/ebooks/6300
wget 'https://www.gutenberg.org/ebooks/6300.txt.utf-8' -O parker.txt
## "The Complete Works of William Shakespeare" http://www.gutenberg.org/ebooks/100
wget 'https://www.gutenberg.org/ebooks/100.txt.utf-8' -O shakespeare.txt
## "The Entire Project Gutenberg Works of Mark Twain" http://www.gutenberg.org/ebooks/3200
wget 'https://www.gutenberg.org/ebooks/3200.txt.utf-8' -O twain.txt
## "The Complete Works of Artemus Ward" https://www.gutenberg.org/ebooks/6946
wget 'https://www.gutenberg.org/ebooks/6946.txt.utf-8' -O ward.txt
du -ch *.txt; wc --char *.txt
# 4.2M  austen.txt
# 836K  billings.txt
# 9.0M  churchill.txt
# 34M   input.txt
# 12M   parker.txt
# 5.3M  shakespeare.txt
# 15M   twain.txt
# 12K   ward.txt
# 80M   total
#  4373566 austen.txt
#   849872 billings.txt
#  9350541 churchill.txt
# 34883356 input.txt
# 12288956 parker.txt
#  5465099 shakespeare.txt
# 15711658 twain.txt
#     9694 ward.txt
# 82932742 total
for FILE in *.txt; do
  dos2unix $FILE
  AUTHOR=$(echo $FILE | sed -e 's/\.txt//' | tr '[:lower:]' '[:upper:]')
  cat $FILE | tail -n +80 | grep -v -i 'Gutenberg' | iconv -c -tascii | tr '\n' ' ' | fold --spaces --bytes --width=10000 | sed -e "s/^/$AUTHOR\|/" > $FILE.transformed
done
rm input.txt
cat *.transformed | shuf > input.txt
cd ../../
th train.lua -data_dir data/styles/ -gpuid 0 -rnn_size 747 -num_layers 2 -seq_length 187
# using CUDA on GPU 0...
# loading data files...
# cutting off end of data so that the batches/sequences divide evenly
# reshaping tensor...
# data load done. Number of data batches in train: 4852, val: 256, test: 0
# vocab size: 96
# creating an lstm with 2 layers
# number of parameters in the model: 7066716
# cloning rnn
# cloning criterion
# 1/242600 (epoch 0.000), train_loss = 4.57489208, grad/param norm = 9.6573e-01, time/batch = 2.03s
# ...
# 15979/242600 (epoch 3.293), train_loss = 1.01393854, grad/param norm = 1.8754e-02, time/batch = 1.40s
~~~

started: 12:57PM
finished: ?

This gets us a corpus in which every line specifies its author and then switches authors, while still being long enough to have readable meaning.
After about 22 hours of training yielding a validation loss of 1.0402 (with little improvement evident after the first 7 hours), we can try out our best candidate and see if it knows Shakespeare versus Austen:

~~~{.Bash}
BEST=`ls cv/*.t7 | sort --field-separator="_" --key=4 --numeric-sort --reverse | tail -1`
th sample.lua $BEST -temperature 0.8 -length 500 -primetext "SHAKESPEARE|"
# SHAKESPEARE|is of no regular complexion.  The action of the plain chatter--"  "Alas, they have discovered what was to be afforded since then?"  "We can believe--for the signature of the Church."  "So they do, dear lord, do they their home?  Oh, no, to the devil which we have not written, the Church is not in the world; but not in this harmless way then to the captain of man--therefore while the praise of it was allurious he would not reflect on the curious man's hatch deemed that his life should be very con
th sample.lua $BEST -temperature 0.8 -length 500 -primetext "SHAKESPEARE|" -seed 105
# SHAKESPEARE|   CHAPTER VII  FROM A WESPERON IN STORY  "MOST INGURIFELLOWSELLES," Antoinette now looked at him a sharp pleasure in passing southward and again in portion of his mother's reach of it. Suddenly the thing was said.  "We'll sit down and find out," he inquired, with a pity to see Mr. Carvel driving beside the bedroom, which was almost as much as he could bear the potion.  "You say you're strong," said Mrs. Holy, indignantly, "you won't have to go away, about the doctor. What is it?"  "Why, we are"
th sample.lua $BEST -temperature 0.8 -length 500 -primetext "AUSTEN|"
# AUSTEN|business, and the gout--a constant and foolish figure in which Fellowes' ring is nearer to distemper than meek and steady interest and clean iron. The episode for the future and the war, and the seedy and effective sun-elogs and the others ventured its remote room, whose hair was a suffering man--that the work of the circumstance interested him. It had no long served to open the papers to answer up a quiet road, free from the long row of white to the lash called No. 14,000 to a sweet conversatio
th sample.lua $BEST -temperature 0.8 -length 500 -primetext "TWAIN|"
# TWAIN|quarrelling with a little book, and so on, considering its sensations as to whether it were not possible to eat it.  He thought that the leader of the conference with his own death would be recognized as a common expression.  The men that mounted from motive powers, how big the calf, commander of the rights of the new economic steamer, the English, a lass of manhood, will exhibit no praise or increase out of a sort of meaning in the senses, and send them back to such a winter as we can go into t
~~~

We can see that while the RNN is producing very English-sounding novelistic prose and produces its usual mix of flawless syntax and hilarious semantics (I particularly like the phrase "Oh, no, to the devil which we have not written, the Church is not in the world"), it has failed to learn the styles I was hoping for.
The Austen and Twain samples sound somewhat like themselves, but the Shakespeare samples are totally wrong.
And given the lack of improvements on the validation set, it seems unlikely that another 10 epochs will remedy the situation: it should be immediately obvious to the RNN how to use the metadata.

Since the style varies so little between the samples, I wonder if mimicking English uses up all the capacity in the RNN? I gave it only 747 neurons, but I could've given it much more.

So to try again:

- to better preserve the semantics, instead of deleting newlines, replace them with a slash
- try much shorter lines of 1000 bytes (increasing the relative density of the metadata)
- back off on the very long backprop through time, and instead, devote the GPU RAM to many more neurons.
- the default setting for the validation set is a bit excessive here and I'd rather use some of that text for training

~~~{.Bash}
rm input.txt *.transformed
for FILE in *.txt; do
  dos2unix $FILE
  AUTHOR=$(echo $FILE | sed -e 's/\.txt//' | tr '[:lower:]' '[:upper:]')
  cat $FILE | tail -n +80 | grep -v -i 'Gutenberg' | iconv -c -tascii | tr '\n' '/' | fold --spaces --bytes --width=1000 | sed -e "s/^/$AUTHOR\|/" > $FILE.transformed
done
cat *.transformed | shuf > input.txt
cd ../../
th train.lua -data_dir data/styles/ -gpuid 0 -rnn_size 2600 -num_layers 2 -val_frac 0.01
# ...data load done. Number of data batches in train: 18294, val: 192, test: 771
# vocab size: 96
# creating an lstm with 2 layers
# number of parameters in the model: 82409696
# cloning rnn
# cloning criterion
# 1/914700 (epoch 0.000), train_loss = 4.80300702, grad/param norm = 1.1946e+00, time/batch = 2.78s
# 2/914700 (epoch 0.000), train_loss = 13.66862074, grad/param norm = 1.5432e+00, time/batch = 2.63s
# ...
~~~

errored out of memory at 11:36AM the next day:

17889/914700 (epoch 0.978), train_loss = 1.22413840, grad/param norm = 2.2657e-03, time/batch = 2.58s
/home/gwern/src/torch/install/bin/luajit: ./util/misc.lua:9: cuda runtime error (2) : out of memory at /home/gwern/src/torch/extra/cutorch/lib/THC/THCStorage.cu:30
stack traceback:
    [C]: in function 'clone'
    ./util/misc.lua:9: in function 'clone_list'
    train.lua:259: in function 'opfunc'
    .../gwern/src/torch/install/share/lua/5.1/optim/rmsprop.lua:32: in function 'rmsprop'
    train.lua:293: in main chunk
    [C]: in function 'dofile'
    .../src/torch/install/lib/luarocks/rocks/trepl/scm-1/bin/th:131: in main chunk
    [C]: at 0x00406670

Still pretty meh, but at 1.1705, can't expect much:

$ th sample.lua cv/lm_lstm_epoch0.93_1.1705.t7 -temperature 0.8 -length 500 -primetext "SHAKESPEARE|"
seeding with SHAKESPEARE|
--------------------------
SHAKESPEARE|jung's own,/which is on the house again.  There is no endeavour to be dressed in the midst of the/present of Belle, who persuades himself to know to have a condition of/the half, but "The garnal she was necessary, but it was high, consecrets, and/excursions of the worst and thing and different honor to flew himself.  But/since the building closed the mass of inspiration of the children of French wind,/hurried down--but he was in the second farmer of the Cald endless figures, Mary/Maeaches, and t
$ th sample.lua cv/lm_lstm_epoch0.93_1.1705.t7 -temperature 0.8 -length 500 -primetext "AUSTEN|"
AUSTEN|mill./And now the good deal now be alone, there is no endeavour to be dreaming./In fact, what was the story of his state, must be a steady carriages of pointing out/both till he has walked at a long time, and not convinced that he remembers/her in this story of a purpose of this captain in stock. There was/no doubt of interest, that Mr. Crewe's mother could not be got the/loss of first poor sister, and who looked warm enough by a/great hay below and making a leaver and with laid with a murder to
$ th sample.lua cv/lm_lstm_epoch0.93_1.1705.t7 -temperature 0.8 -length 500 -primetext "TWAIN|"
TWAIN|nor contributed/she has filled on behind him.  He had been satisfied by little just as to/deliver that the inclination of the possession of a thousand expenses in the group of feeling had destroyed/him to descend.  The physical had he darted before him that he was worth a
PARKER|George Pasha, for instance?"//"Then it is not the marvel of laws upon Sam and the Sellers."  She said/he would ask himself to, one day standing from the floor, as he/stood for the capital.  He was no good of conversation

Increase diversity of styles: amping up to 38 authors, including modern SF/F fiction authors (Robert Jordan's _Wheel of Time_, Gene Wolfe, R.A. Lafferty, Ryukishi07's _Umineko no naku koro ni_, Kafka), poetry ancient and modern (_Iliad_, _Beowulf_, Dante, Keats, Coleridge, Poe, Whitman, Gilbert & Sullivan), ancient fiction (the Bible), miscellaneous nonfiction (Aristotle, Machiavelli, Paine) etc.
By adding in many more authors from many different genres and time periods, may force the RNN to realize that it needs to take seriously the metadata prefix.

~~~{.Bash}
wget 'http://dl.dropboxusercontent.com/u/182368464/umineko-compress.tar.xz'
untar umineko-compress.tar.xz && rm umineko-compress.tar.xz
mv umineko/umineko.txt  ryukishi07.txt; mv  umineko/wot.txt jordan.txt; rm -rf ./umineko/

cat /home/gwern/doc/fiction/lafferty/*.txt > lafferty.txt
cat /home/gwern/doc/fiction/wolfe/fiction/*.txt > wolfe.txt

wget 'https://www.gutenberg.org/ebooks/10031.txt.utf-8' -O poe.txt
wget 'https://www.gutenberg.org/ebooks/11.txt.utf-8' -O carroll.txt
wget 'https://www.gutenberg.org/ebooks/1232.txt.utf-8' -O machiavelli.txt
wget 'https://www.gutenberg.org/ebooks/12699.txt.utf-8' -O aristotle.txt
wget 'https://www.gutenberg.org/ebooks/1322.txt.utf-8' -O whitman.txt
wget 'https://www.gutenberg.org/ebooks/16328.txt.utf-8' -O beowulf.txt
wget 'https://www.gutenberg.org/ebooks/1661.txt.utf-8' -O doyle.txt
wget 'https://www.gutenberg.org/ebooks/23684.txt.utf-8' -O keats.txt
wget 'https://www.gutenberg.org/ebooks/2383.txt.utf-8' -O chaucer.txt
wget 'https://www.gutenberg.org/ebooks/2701.txt.utf-8' -O melville.txt
wget 'https://www.gutenberg.org/ebooks/30.txt.utf-8' -O bible.txt
wget 'https://www.gutenberg.org/ebooks/3090.txt.utf-8' -O maupassant.txt
wget 'https://www.gutenberg.org/ebooks/31270.txt.utf-8' -O paine.txt
wget 'https://www.gutenberg.org/ebooks/3253.txt.utf-8' -O lincoln.txt
wget 'https://www.gutenberg.org/ebooks/345.txt.utf-8' -O stoker.txt
wget 'https://www.gutenberg.org/ebooks/3567.txt.utf-8' -O bonaparte.txt
wget 'https://www.gutenberg.org/ebooks/3600.txt.utf-8' -O montaigne.txt
wget 'https://www.gutenberg.org/ebooks/4200.txt.utf-8' -O pepys.txt
wget 'https://www.gutenberg.org/ebooks/4361.txt.utf-8' -O sherman.txt
wget 'https://www.gutenberg.org/ebooks/4367.txt.utf-8' -O grant.txt
wget 'https://www.gutenberg.org/ebooks/6130.txt.utf-8' -O homer.txt
wget 'https://www.gutenberg.org/ebooks/7849.txt.utf-8' -O kafka.txt
wget 'https://www.gutenberg.org/ebooks/808.txt.utf-8' -O gilbertsullivan.txt
wget 'https://www.gutenberg.org/ebooks/8800.txt.utf-8' -O dante.txt
wget 'https://www.gutenberg.org/files/28289/28289-0.txt' -O eliot.txt
wget 'https://www.gutenberg.org/files/29090/29090-0.txt' -O coleridge.txt
wget 'https://www.gutenberg.org/files/5000/5000-8.txt' -O davinci.txt
~~~

Due to OOM crash, decreasing neuron count
with bigger neuron count, also necessary to have dropout enabled (default of 0 means progress seems to halt around a loss of 3.5 and makes no discernible progress for hours)

~~~{.Bash}
rm input.txt *.transformed *.t7
wc --char *.txt
# 100972224 total
for FILE in *.txt; do
  dos2unix $FILE;
  AUTHOR=$(echo $FILE | sed -e 's/\.txt//' | tr '[:lower:]' '[:upper:]')
  cat $FILE | tail -n +80 | grep -i -v -e 'Gutenberg' -e 'http' -e 'file://' -e 'COPYRIGHT' -e 'ELECTRONIC VERSION' -e 'ISBN' | tr -d '\f' | tr -d '^M' | iconv -c -tascii | sed -e ':a;N;$!ba;s/\n/ /g' -e 's/  */ /g' -e 's/ \/ \/ //g' | fold --spaces --bytes --width=1500 | sed -e "s/^/$AUTHOR\|/" > $FILE.transformed
done
cat *.transformed | shuf > input.txt
cd ../../
th train.lua -data_dir data/styles/ -gpuid 0 -rnn_size 2400 -num_layers 2 -val_frac 0.01 -dropout 0.5
# ...data load done. Number of data batches in train: 39862, val: 419, test: 1679
# vocab size: 98
# creating an lstm with 2 layers
# number of parameters in the model: 70334498
# cloning rnn
# cloning criterion
# 1/1993100 (epoch 0.000), train_loss = 4.68234798, grad/param norm = 7.4220e-01, time/batch = 2.53s
# 2/1993100 (epoch 0.000), train_loss = 13.00693768, grad/param norm = 1.7191e+00, time/batch = 2.35s
# ...
~~~

did OK but seemed to have difficulty improving past a loss of 1.14, had issues with exploding error (one exploding error up to a loss of 59 terminated an overnight training run) and then began erroring out everytime I tried to resume, so I began a third try, this time experimenting with deeper layers and increasing the data preprocessing steps to catch various control-characters and copyright/boilerplate which snuck in:

~~~{.Bash}
nice th train.lua -data_dir data/styles/ -gpuid 0 -rnn_size 1000 -num_layers 3 -val_frac 0.005 -seq_length 75 -dropout 0.7
~~~

This one eventually exploded too, having maxed out at a loss of 1.185.

for FILE in *.txt; do
  dos2unix $FILE;
  AUTHOR=$(echo $FILE | sed -e 's/\.txt//' | tr '[:lower:]' '[:upper:]')
  cat $FILE | fold --spaces --bytes --width=1500 | sed -e "s/^/$AUTHOR\|/" > $FILE.transformed
done
cat *.transformed | shuf > input.txt
cd ../../

After deleting even more control characters and constantly restarting after explosions (which had become a regular thing as the validation loss began bouncing around a range of 1.09-1.2, the RNN seeming to have severe trouble doing any better) I did some sampling.
The results are curious: the RNN has memorized the prefixes, of course, and at higher temperatures will spontaneously end with a newline and begin with a new prefix; many of the prefixes like 'BIBLE|' look nothing like the original source, but the 'JORDAN|' prefix performs extremely well in mimicking the _Wheel of Time_, dropping in many character names and WoT neologisms like 'Aiel' or (of course) 'Aes Sedai'.
This isn't too surprising since the WoT corpus makes up 20M or a sixth of the input; it's also not too surprising when WoT terms pop up with other prefixes, but they do so at a far lower rate.
So at least to some extent, the RNN has learned to use Jordan versus non-Jordan prefixes to decide whether to drop in WoT vocab.
The next largest author in the corpus is Mark Twain, and here too we see something similar: when generating Twain text, we see a lot of words that sound like Twain vocab (riverboats, 'America', 'the Constitution' etc), and while these sometimes pop up in the smaller prefix samples it's at a much lower rate.
So the RNN is learning that different prefixes indicate different vocabularies, but it's only doing this well on the largest authors.
Does this reflect that <2M of text from an author is too little to learn from and so the better-learned authors' material inherently pulls the weaker samples towards them (borrowing strength), that the other authors' differences are too subtle compared to the distinctly different vocab of Jordan & Twain (so the RNN focuses on the more predictively-valuable differences in neologisms etc), or that the RNN is too small to store the differences between so many authors?

deleted smallest 14mb of text:
rm  carroll.txt keats.txt beowulf.txt machiavelli.txt poe.txt kafka.txt ward.txt doyle.txt aristotle.txt eliot.txt dante.txt whitman.txt  billings.txt stoker.txt gilbertsullivan.txt homer.txt melville.txt davinci.txt chaucer.txt grant.txt
leaving 111MB

for fifth try,
rm sherman.txt paine.txt coleridge.txt maupassant.txt bonaparte.txt  montaigne.txt  lincoln.txt austen.txt  lafferty.txt

and used very deep 6-layer/500-nn rnn
asymptoted at ~1.1 after ~40 epoches

for sixth try,
rm shakespeare.txt ryukishi07.txt  pepys.txt  parker.txt
back to flat 1-layer 2600-nn rnn

got down to 1.0414 before validation loss began increasing a bit

OK, let's try just Bible & Jordan, deleting everything else
$ rm churchill.txt  twain.txt wolfe.txt
got down to 0.9763

question: how much do multiple authors penalize learning ability? let's try seeing what the flat rnn can do on solely the Robert Jordan corpus (but still formatted with prefixes etc).
got down to 0.9638

just Bible:
0.9420

so the penalty for the Bible for learning Jordan as well is 0.9763-0.9420=0.0343, and vice-versa is 0.9763-0.9638=0.0125.
presumably the reason the Bible RNN is hurt 2.7x more is because the Jordan corpus is 4.3x larger and more learning capacity goes to its vocabulary & style.

Can we learn multiple metadata prefixes? Like an author and then a transform of some sort - in music, a useful transform might be time signature or instrument set.

A simple transform we could apply here is upcasing and downcasing every character, so we might have a set of 6 prefixes like Bible+upcase, Bible+downcase, Bible+mix, etc, written as `BIBLE|U|`, `BIBLE|D|`, `BIBLE|M|`, and to help enforce abstraction, also reverse ordering like `U|BIBLE|`, giving 12 total prefixes (3x2x2).
The interesting question here is whether the RNN would be able to factor out the transformations and learn the up/mix/downcase transformation separately from the Bible/Jordan difference in styles. (If it thought that Jordan upcased was a different author, and to be learned differently, from Jordan downcased, then we would have to conclude that it was not seeing two pieces of metadata, Jordan+upcase, but seeing it as one JORDANUPCASE, and a failure of both learning and abstraction.)
But if we included each of the 12 prefixes, then we wouldn't know if it had managed to do this, since it could have learned each of the 12 separately, which might or might not show up as much worse performance. So we should leave out two prefixes: one to test out generalization of casing, and one to test out swapping (dropping 1 from Bible and 1 from Jordan to be fair).
At the end, we should get an RNN with a validation loss slightly worse than 0.9763 (the extra transformation & keyword must cost something), and one which will hopefully be able to yield the correct output for the prefixes `JORDAN|U|` and `C|BIBLE|`

~~~{.BASH}
rm *.t7 *.transformed input.txt
for FILE in *.txt; do
  AUTHOR=$(echo $FILE | sed -e 's/\.txt//' | tr '[:lower:]' '[:upper:]')
  TEXT=$(cat $FILE | tail -n +80 | grep -i -v -e 'Gutenberg' -e 'http' -e 'file://' -e 'COPYRIGHT' -e 'ELECTRONIC VERSION' -e 'ISBN' | tr -d '\f' | tr -d '^M' | iconv -c -tascii | sed -e ':a;N;$!ba;s/\n/ /g' -e 's/  */ /g' -e 's/ \/ \/ //g')
  echo $TEXT | fold --spaces --width=3000 |                              sed -e "s/^/$AUTHOR\|M\|/" >> $FILE.transformed
  echo $TEXT | fold --spaces --width=3000 | tr '[:lower:]' '[:upper:]' | sed -e "s/^/$AUTHOR\|U\|/" >> $FILE.transformed
  echo $TEXT | fold --spaces --width=3000 | tr '[:upper:]' '[:lower:]' | sed -e "s/^/$AUTHOR\|D\|/" >> $FILE.transformed
  echo $TEXT | fold --spaces --width=3000 |                              sed -e "s/^/M\|$AUTHOR\|/" >> $FILE.transformed
  echo $TEXT | fold --spaces --width=3000 | tr '[:lower:]' '[:upper:]' | sed -e "s/^/U\|$AUTHOR\|/" >> $FILE.transformed
  echo $TEXT | fold --spaces --width=3000 | tr '[:upper:]' '[:lower:]' | sed -e "s/^/D\|$AUTHOR\|/" >> $FILE.transformed
done
cat *.transformed | grep -v -e "JORDAN|U|" -e "M|BIBLE|" | shuf > input.txt
~~~
