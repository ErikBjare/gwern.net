---
description: Miscellaneous statistical stuff
tags: statistics
created: 17 July 2014
status: in progress
belief: possible
...

# Critiques

- moxibustion mouse study https://plus.google.com/103530621949492999968/posts/TisYM64ckLM
- criticism of teeth-removal experiment in rats http://lesswrong.com/r/discussion/lw/kfb/open_thread_30_june_2014_6_july_2014/b1u3
- criticism of small noopept self-experiment http://www.bluelight.org/vb/threads/689936-My-Paper-quot-Noopept-amp-The-Placebo-Effect-quot?p=11910708&viewfull=1#post11910708
- why Soylent is not a good idea http://lesswrong.com/lw/hht/link_soylent_crowdfunding/90y7
- misinterpretation of fluoridation meta-analysis and ignorance of VoI http://theness.com/neurologicablog/index.php/anti-fluoride-propaganda-as-news/#comment-76400
- http://lesswrong.com/lw/1lt/case_study_melatonin/8mgf
- Fulltext: https://dl.dropboxusercontent.com/u/280585369/2014-dubal.pdf is this possible? http://nextbigfuture.com/2014/05/kl-vs-gene-makes-up-six-iq-points-of.html#comment-1376748788 http://www.reddit.com/r/Nootropics/comments/25233r/boost_your_iq_by_6_points/chddd7f
- Facebook emotion study: http://www.reddit.com/r/psychology/comments/29vg9j/no_emotions_arent_really_contagious_over_facebook/cip7ln5 https://plus.google.com/103530621949492999968/posts/1PqPdLyzXhn
- tACS causes lucid dreaming: http://www.reddit.com/r/LucidDreaming/comments/27y7n6/no_brain_stimulation_will_not_get_you_lucid/ck6isgo
- Herbalife growth patterns: http://www.reddit.com/r/business/comments/24aoo2/what_unsustainable_growth_looks_like_herbalife/ch5hwtv
- Plausible correlate of Fairtrade: http://www.reddit.com/r/Economics/comments/26jb2d/surprise_fairtrade_doesnt_benefit_the_poor/chrx9s4
- slave whippings vs cotton production http://lesswrong.com/r/discussion/lw/kwc/open_thread_sept_17_2014/bajv
- whether a study on mental illness & violence shows schizophrenics are not more likely to murder but rather be murdered: http://www.reddit.com/r/psychology/comments/2fwjs8/people_with_mental_illness_are_more_likely_to_be/ckdq50k / http://www.nationalelfservice.net/publication-types/observational-study/people-with-mental-illness-are-more-likely-to-be-victims-of-homicide-than-perpetrators-of-homicide/#comment-95507 (see also http://slatestarscratchpad.tumblr.com/post/120950150581/psycholar-giraffepoliceforce-museicetc )
- Fortune analysis of higher female CEO returns http://lesswrong.com/r/discussion/lw/l3b/contrarian_lw_views_and_their_economic/bftw
- failed attempt at estimating P(causation|correlation) https://plus.google.com/103530621949492999968/posts/UzMMmPgyyaV
- algae/IQ: http://lesswrong.com/r/discussion/lw/l9v/open_thread_nov_17_nov_23_2014/bm7o
- synaesthesia/IQ: https://www.reddit.com/r/psychology/comments/2mryte/surprising_iq_boost_12_in_average_by_a_training/cm760v8
- misinterpretation: http://slatestarcodex.com/2014/12/08/links-1214-come-ye-to-bethlinkhem/#comment-165197
- unpowered/multiple-correction jobs program: http://slatestarcodex.com/2014/12/08/links-1214-come-ye-to-bethlinkhem/#comment-165197
- vitamin D/caffeine claim based on weak in vitro claims, inconsistent with more relevant in vivo results: https://plus.google.com/u/0/103530621949492999968/posts/AUg3udezXMS [already included in Nootropics.page]
- claimed fall in digit span backwards minuscule and non-statistically-significant, no evidence of heterogeneity beyond variability due to sample size http://drjamesthompson.blogspot.com/2015/04/digit-span-bombshell.html?showComment=1428096775425#c4097303932864318518
- Claimed randomized experiment of whether sushi tastes worse after freezing is not actually a randomized experiment https://www.reddit.com/r/science/comments/324xmf/randomized_doubleblind_study_shows_the_quality_of/cq8dmsb
- aerobic vs weightlifting exercise, multiple problems but primarily p-hacking, difference-in-statistical-significance-is-not-a-significant-difference, and controlling for intermediate variable: https://plus.google.com/103530621949492999968/posts/aeZqB8JWUiQ
- sexual openness result undermined by ceiling effect http://mindhacks.com/2015/04/28/when-society-isnt-judging-womens-sex-drive-rivals-mens/#comment-362749
- music study claiming WM interaction: possible ceiling effect? see FB PM

# Estimating censored test scores

An acquaintance asks the following question: he is applying for a university course which requires a certain minimum score on a test for admittance, and wonders about his chances and a possible trend of increasing minimum scores over time.
(He hasn't received his test results yet.)
The university doesn't provide a distribution of admittee scores, but it does provide the minimum scores for 2005-2013, unless all applicants were admitted because they all scored above an unknown cutoff - in which case it provides no minimum score.
This leads to the dataset:

~~~{.R}
2005,NA
2006,410
2007,NA
2008,NA
2009,398
2010,407
2011,417
2012,NA
2013,NA
~~~

A quick eyeball tells us that we can't conclude much: only 4 actual datapoints, with 5 hidden from us.
We can't hope to conclude anything about time trends, other than there doesn't seem to be much of one: the last score, 417, is not much higher than 410, and the last two scores are low enough to be hidden.
We might be able to estimate a mean, though.

We can't *simply* average the 4 scores and conclude the mean minimum is 410 because of those NAs: a number of scores have been 'censored' because they were too low, and while we don't know what they were, we do know they were \<398 (the smallest score) and so a bunch of \<398s will pull down the uncensored mean of 410.

On approach is to treat it as a [Tobit model](!Wikipedia) and estimate using something like the [`censReg`](http://cran.r-project.org/package=censReg) library ([overview](http://cran.r-project.org/web/packages/censReg/vignettes/censReg.pdf)).

But if we try a quick call to `censReg`, we are confounded: a Tobit model expects you to provide the cutoff below which the observations were censored, but that is something we don't know.
All we know is that it must be below 398, we weren't told it was exactly 395, 394, etc.
Fortunately, this is a solved problem. For example: ["The Tobit model with a non-zero threshold"](http://econweb.ucsd.edu/~rcarson/papers/TobitEJ07.pdf), Carson & Sun 2007 tells us:

> In this paper, we consider estimating the unknown censoring threshold by the minimum of the uncensored $y_i$'s. We show that the estimator $γ'$ of $γ$ is superconsistent and asymptotically exponentially distributed. Carson (1988, 1989) also suggests estimating the unknown censoring threshold by the minimum of the uncensored $y_i$'s. In a recent paper, [Zuehlke (2003)](/docs/statistics/20013-zuehlke.pdf "Estimation of a Tobit model with unknown censoring threshold") rediscovers these unpublished results and demonstrates via simulations that the asymptotic distribution of the maximum likelihood estimator does not seem to be affected by the estimation of the censoring threshold.

That seems to be almost *too* simple and easy, but it makes sense and reminds me a little of the [German tank problem](!Wikipedia): the minimum might not be that accurate a guess (it's unlikely you just happened to draw a sample right on the censoring threshold) and it definitely can't be wrong in the sense of being too low. (A Bayesian method might be able to do better with a prior like a exponential.)

With that settled, the analysis is straightforward: load the data, figure out the minimum score, set the NAs to 0, regress, and extract the model estimates for each year:

~~~{.R}
scores <- data.frame(Year=2005:2013,
                     MinimumScore=c(NA,410,NA,NA,398,407,417,NA,NA));
censorThreshold <- min(scores$MinimumScore, na.rm=T)
scores[is.na(scores)] <- 0

library(censReg)
# 'censorThreshold-1' because censReg seems to treat threshold as < and not <=
summary(censReg(MinimumScore ~ Year, left=censorThreshold-1, data=scores))
# Warning message:
# In censReg(MinimumScore ~ Year, left = censorThreshold - 1, data = scores) :
#   at least one value of the endogenous variable is smaller than the left limit
#
# Call:
# censReg(formula = MinimumScore ~ Year, left = censorThreshold -
#     1, data = scores)
#
# Observations:
#          Total  Left-censored     Uncensored Right-censored
#              9              5              4              0
#
# Coefficients:
#              Estimate Std. error t value Pr(> t)
# (Intercept) -139.9711        Inf       0       1
# Year           0.2666        Inf       0       1
# logSigma       2.6020        Inf       0       1
#
# Newton-Raphson maximisation, 37 iterations
# Return code 1: gradient close to zero
# Log-likelihood: -19.35 on 3 Df
-139.9711 + (0.2666 * scores$Year)
# [1] 394.6 394.8 395.1 395.4 395.6 395.9 396.2 396.4 396.7
~~~

With so little data the results aren't very reliable, but there is one observation we can make.

The fact that half the dataset is censored tells us that the uncensored mean may be a *huge* overestimate (since we're only looking at the 'top half' of the underlying data), and indeed it is.
The original mean of the uncensored scores was 410; however, the estimate including the censored data is much lower, 397 (*13* less)!

This demonstrates the danger of ignoring systematic biases in your data.

So, trying to calculate a mean or time effect is not helpful.
What might be better is to instead exploit the censoring directly: if the censoring happened because *everyone* got in, then if you showed up in a censored year, you have 100% chance of getting in; while in a non-censored year you have an unknown but \<100% chance of getting in; so the probability of a censored year sets a lower bound on one's chances, and this is easy to calculate as a simple binomial problem - 5 out of 9 years were censored years, so:

~~~{.R}
binom.test(c(5,4))
#
#   Exact binomial test
#
# data:  c(5, 4)
# number of successes = 5, number of trials = 9, p-value = 1
# alternative hypothesis: true probability of success is not equal to 0.5
# 95% confidence interval:
#  0.212 0.863
# sample estimates:
# probability of success
#                 0.5556
~~~

So we can tell him that he may have a \>55% chance of getting in.

# The Traveling Gerontologist problem

A quick probability exercise: [Wikipedia](!Wikipedia "Centenarian#Centenarian_populations_by_country") mentions Finland has 566 centenarians as of 2010.

That's few enough you could imagine visiting them all to research them and their longevity, in a sort of traveling salesman problem but with gerontologists instead.
Except, because of the [exponential increase in mortality](https://en.wikipedia.org/wiki/Gompertz%E2%80%93Makeham_law_of_mortality), centenarians have high annual mortality rates; it depends on the exact age but you could call it >30% (eg Finnish 99yos in 2012 had a death toll of 326.54/1000).
So you might well try to visit a centenarian and discover they'd died before you got there.

How bad a risk is this?
Well, if the risk per year is 30%, then one has a 70% chance of surviving a year.
To survive a year, you must survive all 365 days; by the multiplication rule, the risk is $x$ where $0.7 = x \cdot x \cdot x \cdot ... * x \text{[365.25 times]}$ or $0.7 = x^{365.25}$; solving, $x = 0.999024$.

It takes time to visit a centenarian - it wouldn't do to be abrupt and see them for only a few minutes, you ought to listen to their stories, and you need to get to a hotel or airport, so let's assume you visit 1 centenarian per day.

If you visit centenarian A on day 1, and you want to visit centenarian B on day 2, then you can count on a 99.9% chance B is still alive. So far so good.
And if you wanted to visit 566 centenarians (let's imagine you have a regularly-updated master list of centenarians from the Finnish population registry), then you only have to beat the odds 566 times in a row, which is not _that_ hard: $0.999024^{566} = 0.5754023437943274$.

But that's coldblooded of you to objectify those Finnish centenarians! "Any centenarian will do, I don't care."
What if you picked the *current* set of 566 centenarians and wanted to visit just them, specifically - with no new centenarians introduced to the list to replace any dead ones.

That's a little more complicated. When you visit the first centenarian, it's the same probability: 0.999024.
When you visit the second centenarian the odds change since now she (and it's more often 'she' than 'he', since remember the exponential and males having shorter mean lifetimes) has to survive 2 days, so it's $0.999024 \cdot 0.999024$ or $0.999024^2$; for the third, it's $0.999024^3$, and so on to #566 who has been patiently waiting and trying to survive a risk of $0.999024^566$, and then you need to multiply to get your odds of beating every single risk of death and the centenarian not leaving for a more permanent rendezvous: $0.999024 \cdot 0.999024^2 \cdot 0.999024^3 \cdot ... \cdot 0.999024^{566}$, which would be $\prod_{n=1}^{566} 0.999024^n$, or in Haskell:

~~~{.Haskell}
product (map (\x -> 0.999024**x) [1..566])
~> 8.952743340164081e-69
~~~

(A little surprisingly, Wolfram Alpha can [solve the TeX expression](https://www.wolframalpha.com/input/?i=\prod_{n%3D1}^{566}+0.999024^n) too.)

Given the use of floating point in that function (567 floating point exponentiations followed by as many multiplications) and the horror stories about floating point, one might worry the answer is wrong & the real probability is much larger.
We can retry with an implementation of computable reals, [`CReal`](https://hackage.haskell.org/package/numbers-2009.8.9/docs/Data-Number-CReal.html#t:CReal), which can be very slow but should give more precise answers:

~~~{.Haskell}
:module + Data.Number.CReal
showCReal 100 (product (map (\x -> 0.999024**x) [1..566]))
~> 0.0000000000000000000000000000000000000000000000000000000000000000000089527433401308585720915431195262
~~~

Looks good - agrees with the floating point version up to the 11th digit:

    8.9527433401 64081e-69
    8.9527433401 308585720915431195262

We can also check by rewriting the product equation to avoid all the exponentiation and multiplication (which might cause issues) in favor of a single exponential:

1. $p^1 * p^2 * ... p^n$ (as before)
2. = $p^{1+2+...+n}$ (since $(x^m) * (x^n) = x^(m + n)$)
3. = $p^{\frac{n \cdot (1 + n)}{2}}$ (by [arithmetic progression](!Wikipedia)/[Gauss's famous classroom trick](http://mathworld.wolfram.com/ArithmeticSeries.html) since $\sum_1^n = n \cdot \frac{a_1 + a_n}{2}$)
4. = $0.999024^{\frac{566 \cdot (1 + 566)}{2}}$ (start substituting in specific values)
5. = $0.999024^{\frac{320922}{2}}$
6. = $0.999024^{160461}$

So:

~~~{.Haskell}
0.999024^160461
~> 8.95274334014924e-69
~~~

Or to go back to the longer version:

~~~{.Haskell}
0.999024**((566*(1 + 566)) / 2)
~> 8.952743340164096e-69
~~~

Also close. All probabilities of success are minute.

How fast would you have to be if you wanted to at least *try* to accomplish the tour with, say, a 50-50 chance?

Well, that's easy: you can consider the probability of all of them surviving one day and as we saw earlier, that's $0.999024^{566} = 0.58$, and two days would be $(0.999024 ^ {566}) ^ 2 = 0.33$
So you can only take a little over a day before you've probabilistically lost & one of them has died; if you hit all 566 centenarians in 24 hours, that's ~24 centenarians per hour or ~2 minutes to chat with each one and travel to the next. If you're trying to collect DNA samples, better hope they're all awake and able to give consent!

So safe to say, you will probably not be able to manage the Traveling Gerontologist's tour.


# Bayes nets
## Daily weight data graph

As the datasets I'm interested in grow in number of variables, it becomes harder to justify doing analysis by simply writing down a simple linear model with a single dependent variable and throwing in the independent variables and maybe a few transformations chosen by hand.
I can instead write down some simultaneous-equations/structural-equation-models, but while it's usually obvious what to do for _k_<4 and if it's not I can compare the possible variants, 4 variables is questionable what the right SEM is, and >5, it's hopeless.
Factor analysis to extract some latent variables is a possibility, but the more general solution here seems to be probabilistic graphical models such as Bayesian networks.

I thought I'd try out some Bayes net inference on some of my datasets.
In this case, I have ~150 daily measurements from my Omron body composition scale, measuring total weight, body fat percentage, and some other things (see [an Omron manual](http://ecx.images-amazon.com/images/I/B1v0aFQLGFS.pdf)):

1. Total weight
2. BMI
3. Body fat percentage
4. Muscle percentage
5. Resting metabolism in calories
6. "Body age"
7. Visceral fat index

The 7 variables are interrelated, so this is definitely a case where a simple `lm` is not going to do the trick.
It's also not 100% clear how to set up a SEM; some definitions are obvious (the much-criticized BMI is going to be determined solely by total weight, muscle and fat percentage might be inversely related) but others are not (how does "visceral fat" relate to body fat?).
And it's not a hopelessly small amount of data.

The Bayes net R library I'm trying out is [`bnlearn`](http://www.bnlearn.com) ([paper](http://www.jstatsoft.org/v35/i03/paper)).

~~~{.R}
library(bnlearn)
# https://www.dropbox.com/s/4nsrszm85m47272/2015-03-22-gwern-weight.csv
weight <- read.csv("selfexperiment/weight.csv")
weight$Date <- NULL; weight$Weight.scale <- NULL
# remove missing data
weightC <- na.omit(weight)
# bnlearn can't handle integers, oddly enough
weightC <- as.data.frame(sapply(weightC, as.numeric))
summary(weightC)
#   Weight.Omron        Weight.BMI        Weight.body.fat    Weight.muscle
#  Min.   :193.0000   Min.   : 26.90000   Min.   :27.00000   Min.   :32.60000
#  1st Qu.:195.2000   1st Qu.: 27.20000   1st Qu.:28.40000   1st Qu.:34.20000
#  Median :196.4000   Median : 27.40000   Median :28.70000   Median :34.50000
#  Mean   :196.4931   Mean   : 28.95409   Mean   :28.70314   Mean   :34.47296
#  3rd Qu.:197.8000   3rd Qu.: 27.60000   3rd Qu.:29.10000   3rd Qu.:34.70000
#  Max.   :200.6000   Max.   : 28.00000   Max.   :31.70000   Max.   :35.50000
#  Weight.resting.metabolism Weight.body.age    Weight.visceral.fat
#  Min.   :1857.000          Min.   :52.00000   Min.   : 9.000000
#  1st Qu.:1877.000          1st Qu.:53.00000   1st Qu.:10.000000
#  Median :1885.000          Median :53.00000   Median :10.000000
#  Mean   :1885.138          Mean   :53.32704   Mean   : 9.949686
#  3rd Qu.:1893.000          3rd Qu.:54.00000   3rd Qu.:10.000000
#  Max.   :1914.000          Max.   :56.00000   Max.   :11.000000
cor(weightC)
#                             Weight.Omron     Weight.BMI Weight.body.fat  Weight.muscle
# Weight.Omron               1.00000000000  0.98858376919    0.1610643221 -0.06976934825
# Weight.BMI                 0.98858376919  1.00000000000    0.1521872557 -0.06231142104
# Weight.body.fat            0.16106432213  0.15218725566    1.0000000000 -0.98704369855
# Weight.muscle             -0.06976934825 -0.06231142104   -0.9870436985  1.00000000000
# Weight.resting.metabolism  0.96693236051  0.95959140245   -0.0665001241  0.15621294274
# Weight.body.age            0.82581939626  0.81286141659    0.5500409365 -0.47408608681
# Weight.visceral.fat        0.41542744168  0.43260100665    0.2798756916 -0.25076619829
#                           Weight.resting.metabolism Weight.body.age Weight.visceral.fat
# Weight.Omron                           0.9669323605    0.8258193963        0.4154274417
# Weight.BMI                             0.9595914024    0.8128614166        0.4326010067
# Weight.body.fat                       -0.0665001241    0.5500409365        0.2798756916
# Weight.muscle                          0.1562129427   -0.4740860868       -0.2507661983
# Weight.resting.metabolism              1.0000000000    0.7008354776        0.3557229425
# Weight.body.age                        0.7008354776    1.0000000000        0.4840752389
# Weight.visceral.fat                    0.3557229425    0.4840752389        1.0000000000

## create alternate dataset expressing the two percentage variables as pounds, since this might fit better
weightC2 <- weightC
weightC2$Weight.body.fat <- weightC2$Weight.Omron * (weightC2$Weight.body.fat / 100)
weightC2$Weight.muscle   <- weightC2$Weight.Omron * (weightC2$Weight.muscle / 100)
~~~

Begin analysis:

~~~{.R}
pdap <- hc(weightC)
pdapc2 <- hc(weightC2)
## bigger is better:
score(pdap, weightC)
[1] -224.2563072
score(pdapc2, weightC2)
[1] -439.7811072
## stick with the original, then
pdap
#   Bayesian network learned via Score-based methods
#
#   model:
#    [Weight.Omron][Weight.body.fat][Weight.BMI|Weight.Omron]
#    [Weight.resting.metabolism|Weight.Omron:Weight.body.fat]
#    [Weight.body.age|Weight.Omron:Weight.body.fat]
#    [Weight.muscle|Weight.body.fat:Weight.resting.metabolism][Weight.visceral.fat|Weight.body.age]
#   nodes:                                 7
#   arcs:                                  8
#     undirected arcs:                     0
#     directed arcs:                       8
#   average markov blanket size:           2.57
#   average neighbourhood size:            2.29
#   average branching factor:              1.14
#
#   learning algorithm:                    Hill-Climbing
#   score:                                 BIC (Gauss.)
#   penalization coefficient:              2.534452101
#   tests used in the learning procedure:  69
#   optimized:                             TRUE
plot(pdap)
## https://i.imgur.com/nipmqta.png
~~~

This inferred graph is obviously wrong in several respects, violating prior knowledge about some of the relationships.

More specifically, my prior knowledge:

- `Weight.Omron` == total weight; should be influenced by `Weight.body.fat` (%), `Weight.muscle` (%), & `Weight.visceral.fat`
- `Weight.visceral.fat`: ordinal variable, <=9 = normal; 10-14 = high; 15+ = very high; from the Omron manual:

    > Visceral fat area (0 - approx. 300 cm , 1 inch=2.54 cm) distribution with 30 levels. NOTE: Visceral fat levels are relative and not absolute values.
- `Weight.BMI`: BMI is a simple function of total weight & height (specifically `BMI = round(weight / height^2)`), so it should be influenced only by `Weight.Omron`, and influence nothing else
- `Weight.body.age`: should be influenced by `Weight.Omron`, `Weight.body.fat`, and `Weight.muscle`, based on the description in the manual:

    > Body age is based on your resting metabolism. Body age is calculated by using your weight, body fat percentage and skeletal muscle percentage to produce a guide to whether your body age is above or below the average for your actual age.
- `Weight.resting.metabolism`: a function of the others, but I'm not sure which exactly; manual talks about what resting metabolism is generically and specifies it has the range "385 to 3999 kcal with 1 kcal increments"; https://en.wikipedia.org/wiki/Basal_metabolic_rate suggests the Omron may be using one of several approximation equations based on age/sex/height/weight, but it might also be using lean body mass as well.

Unfortunately, bnlearn doesn't seem to support any easy way of encoding the prior knowledge - for example, you can't say 'no outgoing arrows from node X' - so I iterate, adding bad arrows to the blacklist.

Which arrows violate prior knowledge?

- `[Weight.visceral.fat|Weight.body.age]` (read backwards, as `Weight.body.age ~> Weight.visceral.fat`)
- `[Weight.muscle|Weight.resting.metabolism]`

Retry, blacklisting those 2 arrows:

~~~{.R}
pdap2 <- hc(weightC, blacklist=data.frame(from=c("Weight.body.age", "Weight.resting.metabolism"), to=c("Weight.visceral.fat","Weight.muscle")))
~~~

New violations:

- `[Weight.visceral.fat|Weight.BMI]`
- `[Weight.muscle|Weight.Omron]`

~~~{.R}
pdap3 <- hc(weightC, blacklist=data.frame(from=c("Weight.body.age", "Weight.resting.metabolism", "Weight.BMI", "Weight.Omron"), to=c("Weight.visceral.fat","Weight.muscle", "Weight.visceral.fat", "Weight.muscle")))
~~~

New violations:

- `[Weight.visceral.fat|Weight.Omron]`
- `[Weight.muscle|Weight.BMI]`

~~{.R}
pdap4 <- hc(weightC, blacklist=data.frame(from=c("Weight.body.age", "Weight.resting.metabolism", "Weight.BMI", "Weight.Omron", "Weight.Omron", "Weight.BMI"), to=c("Weight.visceral.fat","Weight.muscle", "Weight.visceral.fat", "Weight.muscle", "Weight.visceral.fat", "Weight.muscle")))
~~~

One violation:

- `[Weight.muscle|Weight.body.age]`

~~~{.R}
pdap5 <- hc(weightC, blacklist=data.frame(from=c("Weight.body.age", "Weight.resting.metabolism", "Weight.BMI", "Weight.Omron", "Weight.Omron", "Weight.BMI", "Weight.body.age"), to=c("Weight.visceral.fat","Weight.muscle", "Weight.visceral.fat", "Weight.muscle", "Weight.visceral.fat", "Weight.muscle", "Weight.muscle")))
#   Bayesian network learned via Score-based methods
#
#   model:
#    [Weight.body.fat][Weight.muscle|Weight.body.fat][Weight.visceral.fat|Weight.body.fat]
#    [Weight.Omron|Weight.visceral.fat][Weight.BMI|Weight.Omron]
#    [Weight.resting.metabolism|Weight.Omron:Weight.body.fat]
#    [Weight.body.age|Weight.Omron:Weight.body.fat]
#   nodes:                                 7
#   arcs:                                  8
#     undirected arcs:                     0
#     directed arcs:                       8
#   average markov blanket size:           2.57
#   average neighbourhood size:            2.29
#   average branching factor:              1.14
#
#   learning algorithm:                    Hill-Climbing
#   score:                                 BIC (Gauss.)
#   penalization coefficient:              2.534452101
#   tests used in the learning procedure:  62
#   optimized:                             TRUE
plot(pdap5)
## https://i.imgur.com/nxCfmYf.png

## implementing all the prior knowledge cost ~30:
score(pdap5, weightC)
# [1] -254.6061724
~~~

No violations, so let's use the network and estimate the specific parameters:

~~~{.R}
fit <- bn.fit(pdap5, weightC); fit
#   Bayesian network parameters
#
#   Parameters of node Weight.Omron (Gaussian distribution)
#
# Conditional density: Weight.Omron | Weight.visceral.fat
# Coefficients:
#         (Intercept)  Weight.visceral.fat
#       169.181651376          2.744954128
# Standard deviation of the residuals: 1.486044472
#
#   Parameters of node Weight.BMI (Gaussian distribution)
#
# Conditional density: Weight.BMI | Weight.Omron
# Coefficients:
#   (Intercept)   Weight.Omron
# -0.3115772322   0.1411044216
# Standard deviation of the residuals: 0.03513413381
#
#   Parameters of node Weight.body.fat (Gaussian distribution)
#
# Conditional density: Weight.body.fat
# Coefficients:
# (Intercept)
# 28.70314465
# Standard deviation of the residuals: 0.644590085
#
#   Parameters of node Weight.muscle (Gaussian distribution)
#
# Conditional density: Weight.muscle | Weight.body.fat
# Coefficients:
#     (Intercept)  Weight.body.fat
#   52.1003347352    -0.6141270921
# Standard deviation of the residuals: 0.06455478599
#
#   Parameters of node Weight.resting.metabolism (Gaussian distribution)
#
# Conditional density: Weight.resting.metabolism | Weight.Omron + Weight.body.fat
# Coefficients:
#     (Intercept)     Weight.Omron  Weight.body.fat
#   666.910582196      6.767607964     -3.886694779
# Standard deviation of the residuals: 1.323176507
#
#   Parameters of node Weight.body.age (Gaussian distribution)
#
# Conditional density: Weight.body.age | Weight.Omron + Weight.body.fat
# Coefficients:
#     (Intercept)     Weight.Omron  Weight.body.fat
#  -32.2651379176     0.3603672788     0.5150134225
# Standard deviation of the residuals: 0.2914301529
#
#   Parameters of node Weight.visceral.fat (Gaussian distribution)
#
# Conditional density: Weight.visceral.fat | Weight.body.fat
# Coefficients:
#     (Intercept)  Weight.body.fat
#    6.8781100009     0.1070118125
# Standard deviation of the residuals: 0.2373649058
## residuals look fairly good, except for Weight.resting.metabolism, where there are some extreme residuals in what looks a bit like a sigmoid sort of pattern, suggesting nonlinearities in the Omron scale's formula?
bn.fit.qqplot(fit)
## https://i.imgur.com/mSallOv.png
~~~

We can double-check the estimates here by turning the Bayes net model into a SEM and seeing how the estimates compare, and also seeing if the p-values suggest we've found a good model:

~~~{.R}
library(lavaan)
Weight.model1 <- '
    Weight.visceral.fat ~ Weight.body.fat
    Weight.Omron ~ Weight.visceral.fat
    Weight.BMI ~ Weight.Omron
    Weight.body.age ~ Weight.Omron + Weight.body.fat
    Weight.muscle ~ Weight.body.fat
    Weight.resting.metabolism ~ Weight.Omron + Weight.body.fat
                   '
Weight.fit1 <- sem(model = Weight.model1,  data = weightC)
summary(Weight.fit1)
# lavaan (0.5-16) converged normally after 139 iterations
#
#   Number of observations                           159
#
#   Estimator                                         ML
#   Minimum Function Test Statistic               71.342
#   Degrees of freedom                                 7
#   P-value (Chi-square)                           0.000
#
# Parameter estimates:
#
#   Information                                 Expected
#   Standard Errors                             Standard
#
#                    Estimate  Std.err  Z-value  P(>|z|)
# Regressions:
#   Weight.visceral.fat ~
#     Weight.bdy.ft     0.107    0.029    3.676    0.000
#   Weight.Omron ~
#     Wght.vscrl.ft     2.745    0.477    5.759    0.000
#   Weight.BMI ~
#     Weight.Omron      0.141    0.002   82.862    0.000
#   Weight.body.age ~
#     Weight.Omron      0.357    0.014   25.162    0.000
#     Weight.bdy.ft     0.516    0.036   14.387    0.000
#   Weight.muscle ~
#     Weight.bdy.ft    -0.614    0.008  -77.591    0.000
#   Weight.resting.metabolism ~
#     Weight.Omron      6.730    0.064  104.631    0.000
#     Weight.bdy.ft    -3.860    0.162  -23.837    0.000
#
# Covariances:
#   Weight.BMI ~~
#     Weight.body.g    -0.000    0.001   -0.116    0.907
#     Weight.muscle    -0.000    0.000   -0.216    0.829
#     Wght.rstng.mt     0.005    0.004    1.453    0.146
#   Weight.body.age ~~
#     Weight.muscle     0.001    0.001    0.403    0.687
#     Wght.rstng.mt    -0.021    0.030   -0.700    0.484
#   Weight.muscle ~~
#     Wght.rstng.mt     0.007    0.007    1.003    0.316
#
# Variances:
#     Wght.vscrl.ft     0.056    0.006
#     Weight.Omron      2.181    0.245
#     Weight.BMI        0.001    0.000
#     Weight.body.g     0.083    0.009
#     Weight.muscle     0.004    0.000
#     Wght.rstng.mt     1.721    0.193
~~~

Comparing the coefficients by eye, they tend to be quite close (usually within 0.1) and the p-values are all statistically-significant.

The network itself looks right, although some of the edges are surprises: I didn't know visceral fat was predictable from body fat (I thought they were measuring separate things), and the relative independence of muscle suggests that in any exercise plan I might be better off focusing on the body fat percentage rather than the muscle percentage since the former may be effectively determining the latter.

So what did I learn here?

- learning network structure and direction of arrows is hard; even with only 7 variables and  _n_=159 (accurate clean data), the hill-climbing algorithm will learn at least 7 wrong arcs.

    - and the derived graphs depend disturbingly heavily on choice of algorithm; I used the `hc` hill-climbing algorithm (since I'm lazy and didn't want to specify arrow directions), but when I try out the alternatives like `iamb` on the same data & blacklist, the found graph looks rather different
- Gaussians are, as always, sensitive to outliers: I was surprised the first graph didn't show BMI connected to anything, so I took a closer look and found I had miscoded a BMI of 28 as *280*!
- bnlearn, while not as hard to use as I expected, could still use usability improvements: I should not need to coerce integer data into exactly equivalent numeric types just because bnlearn doesn't recognize integers; and blacklisting/whitelisting needs to be more powerful - iteratively generating graphs and manually inspecting and manually blacklisting is tedious and does not scale

    - hence, it may make more sense to find a graph using `bnlearn` and then convert it into simultaneous-equations and manipulate it using more mature SEM libraries

## Zeo sleep data

Here I look at my Zeo sleep data; more variables, more complex relations, and more unknown ones, but on the positive side, ~12x more data to work with.

~~~{.R}
zeo <- read.csv("~/wiki/docs/zeo/gwern-zeodata.csv")
zeo$Sleep.Date <- as.Date(zeo$Sleep.Date, format="%m/%d/%Y")

## convert "05/12/2014 06:45" to "06:45"
zeo$Start.of.Night <- sapply(strsplit(as.character(zeo$Start.of.Night), " "), function(x) { x[2] })
## convert "06:45" to 24300
interval <- function(x) { if (!is.na(x)) { if (grepl(" s",x)) as.integer(sub(" s","",x))
                                           else { y <- unlist(strsplit(x, ":")); as.integer(y[[1]])*60 + as.integer(y[[2]]); }
                                         }
                          else NA
                        }
zeo$Start.of.Night <- sapply(zeo$Start.of.Night, interval)
## correct for the switch to new unencrypted firmware in March 2013;
## I don't know why the new firmware subtracts 15 hours
zeo[(zeo$Sleep.Date >= as.Date("2013-03-11")),]$Start.of.Night <- (zeo[(zeo$Sleep.Date >= as.Date("2013-03-11")),]$Start.of.Night + 900) %% (24*60)

## after midnight (24*60=1440), Start.of.Night wraps around to 0, which obscures any trends,
## so we'll map anything before 7AM to time+1440
zeo[zeo$Start.of.Night<420 & !is.na(zeo$Start.of.Night),]$Start.of.Night <- (zeo[zeo$Start.of.Night<420 & !is.na(zeo$Start.of.Night),]$Start.of.Night + (24*60))

zeoSmall <- subset(zeo, select=c(ZQ,Total.Z,Time.to.Z,Time.in.Wake,Time.in.REM,Time.in.Light,Time.in.Deep,Awakenings,Start.of.Night,Morning.Feel))
zeoClean <- na.omit(zeoSmall)
# bnlearn doesn't like the 'integer' class that most of the data-frame is in
zeoClean <- as.data.frame(sapply(zeoClean, as.numeric))
~~~

Prior knowledge:

- `Start.of.Night` is temporally first, and cannot be caused
- `Time.to.Z` is temporally second, and can be influenced by `Start.of.Night` (likely a connection between how late I go to bed and how fast I fall asleep) & `Time.in.Wake` (since if it takes 10 minutes to fall asleep, I must spend >=10 minutes in wake) but not others
- `Morning.Feel` is temporally last, and cannot cause anything
- `ZQ` is a synthetic variable invented by Zeo according to an opaque formula, which cannot cause anything but is determined by others
- `Total.Z` should be the sum of `Time.in.Light`, `Time.in.REM`, and `Time.in.Deep`
- `Awakenings` should have an arrow with `Time.in.Wake` but it's not clear which way it should run

~~~{.R}
library(bnlearn)
## after a bunch of iteration, blacklisting arrows which violate the prior knowledge
bl <- data.frame(from=c("Morning.Feel", "ZQ", "ZQ", "ZQ", "ZQ", "ZQ", "ZQ", "Time.in.REM", "Time.in.Light", "Time.in.Deep", "Morning.Feel", "Awakenings", "Time.in.Light", "Morning.Feel", "Morning.Feel","Total.Z", "Time.in.Wake", "Time.to.Z", "Total.Z", "Total.Z", "Total.Z"),
                 to=c("Start.of.Night", "Total.Z", "Time.in.Wake", "Time.in.REM", "Time.in.Deep", "Morning.Feel","Start.of.Night", "Start.of.Night","Start.of.Night","Start.of.Night", "Time.to.Z", "Time.to.Z", "Time.to.Z", "Total.Z", "Time.in.Wake","Time.to.Z","Time.to.Z", "Start.of.Night", "Time.in.Deep", "Time.in.REM", "Time.in.Light"))

zeo.hc <- hc(zeoClean, blacklist=bl)
zeo.iamb         <- iamb(zeoClean, blacklist=bl)
## problem: undirected arc: Time.in.Deep/Time.in.REM; since hc inferred [Time.in.Deep|Time.in.REM], I'll copy that for iamb:
zeo.iamb <- set.arc(zeo.iamb, from = "Time.in.REM", to = "Time.in.Deep")
zeo.gs <- gs(zeoClean, blacklist=bl)
## same undirected arc:
zeo.gs <- set.arc(zeo.gs, from = "Time.in.REM", to = "Time.in.Deep")

## Bigger is better:
score(zeo.iamb, data=zeoClean)
# [1] -44776.79185
score(zeo.gs, data=zeoClean)
# [1] -44776.79185
score(zeo.hc, data=zeoClean)
# [1] -44557.6952
## hc scores best, so let's look at it:
zeo.hc
#   Bayesian network learned via Score-based methods
#
#   model:
#    [Start.of.Night][Time.to.Z|Start.of.Night][Time.in.Light|Time.to.Z:Start.of.Night]
#    [Time.in.REM|Time.in.Light:Start.of.Night][Time.in.Deep|Time.in.REM:Time.in.Light:Start.of.Night]
#    [Total.Z|Time.in.REM:Time.in.Light:Time.in.Deep][Time.in.Wake|Total.Z:Time.to.Z]
#    [Awakenings|Time.to.Z:Time.in.Wake:Time.in.REM:Time.in.Light:Start.of.Night]
#    [Morning.Feel|Total.Z:Time.to.Z:Time.in.Wake:Time.in.Light:Start.of.Night]
#    [ZQ|Total.Z:Time.in.Wake:Time.in.REM:Time.in.Deep:Awakenings]
#   nodes:                                 10
#   arcs:                                  28
#     undirected arcs:                     0
#     directed arcs:                       28
#   average markov blanket size:           7.40
#   average neighbourhood size:            5.60
#   average branching factor:              2.80
#
#   learning algorithm:                    Hill-Climbing
#   score:                                 BIC (Gauss.)
#   penalization coefficient:              3.614556939
#   tests used in the learning procedure:  281
#   optimized:                             TRUE

plot(zeo.hc)
## https://i.imgur.com/nD3LXND.png

fit <- bn.fit(zeo.hc, zeoClean); fit
#
#   Bayesian network parameters
#
#   Parameters of node ZQ (Gaussian distribution)
#
# Conditional density: ZQ | Total.Z + Time.in.Wake + Time.in.REM + Time.in.Deep + Awakenings
# Coefficients:
#    (Intercept)         Total.Z    Time.in.Wake     Time.in.REM    Time.in.Deep      Awakenings
# -0.12468522173   0.14197043518  -0.07103211437   0.07053271816   0.21121000076  -0.56476256303
# Standard deviation of the residuals: 0.3000223604
#
#   Parameters of node Total.Z (Gaussian distribution)
#
# Conditional density: Total.Z | Time.in.Wake + Start.of.Night
# Coefficients:
#    (Intercept)    Time.in.Wake  Start.of.Night
# 907.6406157850   -0.4479377278   -0.2680771514
# Standard deviation of the residuals: 68.90853885
#
#   Parameters of node Time.to.Z (Gaussian distribution)
#
# Conditional density: Time.to.Z | Start.of.Night
# Coefficients:
#    (Intercept)  Start.of.Night
# -1.02898431407   0.01568450832
# Standard deviation of the residuals: 13.51606719
#
#   Parameters of node Time.in.Wake (Gaussian distribution)
#
# Conditional density: Time.in.Wake | Time.to.Z
# Coefficients:
#   (Intercept)      Time.to.Z
# 14.7433880499   0.3289378711
# Standard deviation of the residuals: 19.0906685
#
#   Parameters of node Time.in.REM (Gaussian distribution)
#
# Conditional density: Time.in.REM | Total.Z + Start.of.Night
# Coefficients:
#      (Intercept)           Total.Z    Start.of.Night
# -120.62442964234     0.37864195651     0.06275760841
# Standard deviation of the residuals: 19.32560757
#
#   Parameters of node Time.in.Light (Gaussian distribution)
#
# Conditional density: Time.in.Light | Total.Z + Time.in.REM + Time.in.Deep
# Coefficients:
#   (Intercept)        Total.Z    Time.in.REM   Time.in.Deep
#  0.6424267863   0.9997862624  -1.0000587988  -1.0001805537
# Standard deviation of the residuals: 0.5002896274
#
#   Parameters of node Time.in.Deep (Gaussian distribution)
#
# Conditional density: Time.in.Deep | Total.Z + Time.in.REM
# Coefficients:
#   (Intercept)        Total.Z    Time.in.REM
# 15.4961459056   0.1283622577  -0.1187382535
# Standard deviation of the residuals: 11.90756843
#
#   Parameters of node Awakenings (Gaussian distribution)
#
# Conditional density: Awakenings | Time.to.Z + Time.in.Wake + Time.in.REM + Time.in.Light + Start.of.Night
# Coefficients:
#     (Intercept)        Time.to.Z     Time.in.Wake      Time.in.REM    Time.in.Light
# -18.41014329148    0.02605164827    0.05736596152    0.02291139969    0.01060661963
#  Start.of.Night
#   0.01129521977
# Standard deviation of the residuals: 2.427868657
#
#   Parameters of node Start.of.Night (Gaussian distribution)
#
# Conditional density: Start.of.Night
# Coefficients:
# (Intercept)
# 1413.382886
# Standard deviation of the residuals: 64.43144125
#
#   Parameters of node Morning.Feel (Gaussian distribution)
#
# Conditional density: Morning.Feel | Total.Z + Time.to.Z + Time.in.Wake + Time.in.Light + Start.of.Night
# Coefficients:
#     (Intercept)          Total.Z        Time.to.Z     Time.in.Wake    Time.in.Light
# -0.924662971061   0.004808652252  -0.010127269154  -0.008636841492  -0.002766602019
#  Start.of.Night
#  0.001672816480
# Standard deviation of the residuals: 0.7104115719

## some issues with big residuals at the extremes in the variables Time.in.Light, Time.in.Wake, and Time.to.Z;
## not sure how to fix those
bn.fit.qqplot(fit)
# https://i.imgur.com/fmP1ca0.png

library(lavaan)
Zeo.model1 <- '
    Time.to.Z ~ Start.of.Night
    Time.in.Wake ~ Total.Z + Time.to.Z
    Awakenings ~ Time.to.Z + Time.in.Wake + Time.in.REM + Time.in.Light + Start.of.Night
    Time.in.Light ~ Time.to.Z + Start.of.Night
    Time.in.REM ~ Time.in.Light + Start.of.Night
    Time.in.Deep ~ Time.in.REM + Time.in.Light + Start.of.Night
    Total.Z ~ Time.in.REM + Time.in.Light + Time.in.Deep
    ZQ ~ Total.Z + Time.in.Wake + Time.in.REM + Time.in.Deep + Awakenings
    Morning.Feel ~ Total.Z + Time.to.Z + Time.in.Wake + Time.in.Light + Start.of.Night
                   '
Zeo.fit1 <- sem(model = Zeo.model1,  data = zeoClean)
summary(Zeo.fit1)
# lavaan (0.5-16) converged normally after 183 iterations
#
#   Number of observations                          1379
#
#   Estimator                                         ML
#   Minimum Function Test Statistic               22.737
#   Degrees of freedom                                16
#   P-value (Chi-square)                           0.121
#
# Parameter estimates:
#
#   Information                                 Expected
#   Standard Errors                             Standard
#
#                    Estimate  Std.err  Z-value  P(>|z|)
# Regressions:
#   Time.to.Z ~
#     Start.of.Nght     0.016    0.006    2.778    0.005
#   Time.in.Wake ~
#     Total.Z          -0.026    0.007   -3.592    0.000
#     Time.to.Z         0.314    0.038    8.277    0.000
#   Awakenings ~
#     Time.to.Z         0.026    0.005    5.233    0.000
#     Time.in.Wake      0.057    0.003   16.700    0.000
#     Time.in.REM       0.023    0.002   10.107    0.000
#     Time.in.Light     0.011    0.002    6.088    0.000
#     Start.of.Nght     0.011    0.001   10.635    0.000
#   Time.in.Light ~
#     Time.to.Z        -0.348    0.085   -4.121    0.000
#     Start.of.Nght    -0.195    0.018  -10.988    0.000
#   Time.in.REM ~
#     Time.in.Light     0.358    0.018   19.695    0.000
#     Start.of.Nght     0.034    0.013    2.725    0.006
#   Time.in.Deep ~
#     Time.in.REM       0.081    0.012    6.657    0.000
#     Time.in.Light     0.034    0.009    3.713    0.000
#     Start.of.Nght    -0.017    0.006   -3.014    0.003
#   Total.Z ~
#     Time.in.REM       1.000    0.000 2115.859    0.000
#     Time.in.Light     1.000    0.000 2902.045    0.000
#     Time.in.Deep      1.000    0.001  967.322    0.000
#   ZQ ~
#     Total.Z           0.142    0.000  683.980    0.000
#     Time.in.Wake     -0.071    0.000 -155.121    0.000
#     Time.in.REM       0.071    0.000  167.090    0.000
#     Time.in.Deep      0.211    0.001  311.454    0.000
#     Awakenings       -0.565    0.003 -178.407    0.000
#   Morning.Feel ~
#     Total.Z           0.005    0.001    8.488    0.000
#     Time.to.Z        -0.010    0.001   -6.948    0.000
#     Time.in.Wake     -0.009    0.001   -8.592    0.000
#     Time.in.Light    -0.003    0.001   -2.996    0.003
#     Start.of.Nght     0.002    0.000    5.414    0.000
~~~

Again no major surprises, but one thing I notice is that `ZQ` does not seem to connect to `Time.in.Light`, though `Time.in.Light` does connect to `Morning.Feel`; I've long suspected that `ZQ` is a flawed summary and thought it was insufficiently taking into account wakes or something else, so it looks like it's `Time.in.Light` specifically which is missing.
`Start.of.night` also is more highly connected than I had expected.

Comparing graphs from the 3 algorithms, they don't seem to differ as badly as the weight ones did. Is this thanks to the much greater data or the constraints?

# Genome sequencing costs

~~~{.R}
# http://www.genome.gov/pages/der/sequencing_costs_apr2014.xls
# converted to CSV & deleted cost per base (less precision); CSV looks like:
# https://dl.dropboxusercontent.com/u/182368464/sequencing_costs_apr2014.csv
## Date, Cost per Genome
## Sep-01,"$95,263,072"
## ...
sequencing <- read.csv("sequencing_costs_apr2014.csv")
sequencing$Cost.per.Genome <- as.integer(gsub(",", "", sub("\\$", "", as.character(sequencing$Cost.per.Genome))))
# interpret month-years as first of month:
sequencing$Date <- as.Date(paste0("01-", as.character(sequencing$Date)), format="%d-%b-%y")
head(sequencing)
##         Date Cost.per.Genome
## 1 2001-09-01        95263072
## 2 2002-03-01        70175437
## 3 2002-09-01        61448422
## 4 2003-03-01        53751684
## 5 2003-10-01        40157554
## 6 2004-01-01        28780376

l <- lm(log(Cost.per.Genome) ~ Date, data=sequencing); summary(l)
##
## Coefficients:
##                 Estimate   Std. Error  t value   Pr(>|t|)
## (Intercept) 50.969823683  1.433567932  35.5545 < 2.22e-16
## Date        -0.002689621  0.000101692 -26.4486 < 2.22e-16
##
## Residual standard error: 0.889707 on 45 degrees of freedom
## Multiple R-squared:  0.939559,   Adjusted R-squared:  0.938216
## F-statistic: 699.528 on 1 and 45 DF,  p-value: < 2.22e-16
plot(log(Cost.per.Genome) ~ Date, data=sequencing)
## https://i.imgur.com/3XK8i0h.png
# as expected: linear in log (Moore's law) 2002-2008, sudden drop, return to Moore's law-ish ~December 2011?
# but on the other hand, maybe the post-December 2011 behavior is a continuation of the curve
library(segmented)
# 2 break-points / 3 segments:
piecewise <- segmented(l, seg.Z=~Date, psi=list(Date=c(13970, 16071)))
summary(piecewise)
## Estimated Break-Point(s):
##             Est. St.Err
## psi1.Date 12680 1067.0
## psi2.Date 13200  279.8
##
## t value for the gap-variable(s) V:  0 0 2
##
## Meaningful coefficients of the linear terms:
##                 Estimate   Std. Error  t value   Pr(>|t|)
## (Intercept) 35.841699121  8.975628264  3.99322 0.00026387
## Date        -0.001504431  0.000738358 -2.03754 0.04808491
## U1.Date      0.000679538  0.002057940  0.33020         NA
## U2.Date     -0.002366688  0.001926528 -1.22847         NA
##
## Residual standard error: 0.733558 on 41 degrees of freedom
## Multiple R-Squared: 0.962565,  Adjusted R-squared:   0.958
with(sequencing, plot(Date, log(Cost.per.Genome), pch=16)); plot(piecewise, add=T)
## https://i.imgur.com/HSRqkJO.png
# The first two segments look fine, but the residuals are clearly bad for the third line-segment:
# it undershoots (damaging the second segment's fit), overshoots, then undershoots again. Let's try again with more breakpoints:

lots <- segmented(l, seg.Z=~Date, psi=list(Date=NA), control=seg.control(stop.if.error=FALSE, n.boot=0))
summary(segmented(l, seg.Z=~Date, psi=list(Date=as.Date(c(12310, 12500, 13600, 13750,  14140,  14680,  15010, 15220), origin = "1970-01-01", tz = "EST"))))
# delete every breakpoint below t-value of ~|2.3|, for 3 breakpoints / 4 segments:
piecewise2 <- segmented(l, seg.Z=~Date, psi=list(Date=as.Date(c("2007-08-25","2008-09-18","2010-03-12"))))
with(sequencing, plot(Date, log(Cost.per.Genome), pch=16)); plot(piecewise2, add=T)

# the additional break-point is used up on a better fit in the curve. It looks like an exponential decay/asymptote,
# so let's work on fitting that part of the graph, the post-2007 curve:
sequencingRecent <- sequencing[sequencing$Date>as.Date("2007-10-01"),]
lR <- lm(log(Cost.per.Genome) ~ Date, data=sequencingRecent); summary(lR)
piecewiseRecent <- segmented(lR, seg.Z=~Date, psi=list(Date=c(14061, 16071))); summary(piecewiseRecent)
## Estimated Break-Point(s):
##             Est. St.Err
## psi1.Date 14290  36.31
## psi2.Date 15290  48.35
##
## t value for the gap-variable(s) V:  0 0
##
## Meaningful coefficients of the linear terms:
##                 Estimate   Std. Error   t value   Pr(>|t|)
## (Intercept)  1.13831e+02  6.65609e+00  17.10182 2.0951e-13
## Date        -7.13247e-03  4.73332e-04 -15.06865 2.2121e-12
## U1.Date      4.11492e-03  4.94486e-04   8.32161         NA
## U2.Date      2.48613e-03  2.18528e-04  11.37668         NA
##
## Residual standard error: 0.136958 on 20 degrees of freedom
## Multiple R-Squared: 0.995976,  Adjusted R-squared: 0.994971

with(sequencingRecent, plot(Date, log(Cost.per.Genome), pch=16)); plot(piecewiseRecent, add=T)

lastPiece <- lm(log(Cost.per.Genome) ~ Date, data=sequencingRecent[as.Date(15290, origin = "1970-01-01", tz = "EST")<sequencingRecent$Date,]); summary(lastPiece)
## Coefficients:
##                 Estimate   Std. Error  t value   Pr(>|t|)
## (Intercept) 17.012409648  1.875482507  9.07095 1.7491e-05
## Date        -0.000531621  0.000119056 -4.46528  0.0020963
##
## Residual standard error: 0.0987207 on 8 degrees of freedom
## Multiple R-squared:  0.71366,    Adjusted R-squared:  0.677867
with(sequencingRecent[as.Date(15290, origin = "1970-01-01", tz = "EST")<sequencingRecent$Date,], plot(Date, log(Cost.per.Genome), pch=16)); abline(lastPiece)

predictDays <- seq(from=sequencing$Date[1], to=as.Date("2030-12-01"), by="month")
lastPiecePredict <- data.frame(Date = predictDays, Cost.per.Genome=c(sequencing$Cost.per.Genome, rep(NA, 305)), Cost.per.Genome.predicted = exp(predict(lastPiece, newdata = data.frame(Date = predictDays))))

nlmR <- nls(log(Cost.per.Genome) ~ SSasymp(as.integer(Date), Asym, r0, lrc), data=sequencingRecent); summary(nlmR)
##
## Parameters:
##          Estimate   Std. Error    t value Pr(>|t|)
## Asym  7.88908e+00  1.19616e-01   65.95328   <2e-16
## r0    1.27644e+08  1.07082e+08    1.19203   0.2454
## lrc  -6.72151e+00  5.05221e-02 -133.04110   <2e-16
##
## Residual standard error: 0.150547 on 23 degrees of freedom
with(sequencingRecent, plot(Date, log(Cost.per.Genome))); lines(sequencingRecent$Date, predict(nlmR), col=2)

# side by side:
with(sequencingRecent, plot(Date, log(Cost.per.Genome), pch=16))
plot(piecewiseRecent, add=TRUE, col=2)
lines(sequencingRecent$Date, predict(nlmR), col=3)
# as we can see, the 3-piece linear fit and the exponential decay fit identically;
# but exponential decay is more parsimonious, IMO, so I prefer that.

predictDays <- seq(from=sequencingRecent$Date[1], to=as.Date("2020-12-01"), by="month")
data.frame(Date = predictDays, Cost.per.Genome.predicted = exp(predict(nlmR, newdata = data.frame(Date = predictDays))))
~~~

http://www.unz.com/gnxp/the-intel-of-sequencing/#comment-677904

# When Does The Mail Come?

Consider a question of burning importance for all nerds: what time does the mailman come, bearing gifts?
We could measure by going out in the morning at a random time to see if the mail has come yet, and estimate.
(No one wants to sit around all morning to spot the *exact* time the mailman comes. At least, I don't.)

Given a set of data like "2015-06-20 11:00AM: mail has not come yet; 2015-06-21 11:59AM: mail had come", how can we estimate?
This is not a normal setup where we estimate a mean but our data is interestingly messed up: censored or truncated or an interval somehow.

ground truth from USPS delivery emails & personal observation:

- 2015-06-10 10:34AM
- 2015-06-24 10:53AM
- 2015-06-25 10:55AM
- 2015-06-23 10:58AM
- 2015-06-20 11:02AM
- 2014-09-29 11:15AM
- 2015-03-09 11:19AM
- 2010-05-12 11:31AM
- 2010-04-29 11:33AM
- 2014-12-15 12:02PM
- 2014-08-20 12:14PM

[Survival analysis](!Wikipedia) seems like the appropriate paradigm.
This is not a simple survival analysis with "right-censoring" where each individual is followed up to a censoring time and the exact time of 'failure' is observed.
(This would be right-censoring if instead we had gone out to the mailbox early in the morning and sat there waiting for the mailman to record when she came, occasionally getting bored around 10AM or 11AM and wandering off without seeing when the mail comes.)
This isn't "left-censoring" either (for left-censoring, we'd go out to the mailbox late in the morning when the mail might already be there, and if it isn't, then wait until it does come).
I don't think this is left or right truncation either, since each day data is collected and there's no sampling biases at play.
What this is is interval censoring: when we go out to the mailbox at 11AM and discover the mail is there, we learn that the mail was delivered today sometime in the interval midnight-10:59AM, or if the mail isn't there, we learn it will be delivered later today sometime during the interval 11:01AM-midnight (hopefully closer to the first end than the second).
Interval censoring comes up in biostatistics for situations like periodic checkups for cancer, which does resemble our mail situation.

http://www.ctspedia.org/do/view/CTSpedia/IntervalCensoredAnalysis

## ML

> Surv(time, time2, event, type='interval')
>
> 1. time: For interval data, the first argument is the starting time for the interval.
> 2. time2: ending time of the interval for interval censored or counting process data only. Intervals are assumed to be open on the left and closed on the right, (start, end]. For counting process data, event indicates whether an event occurred at the end of the interval.
> 3. event: The status indicator, normally 0=alive, 1=dead....For interval censored data, the status indicator is 0=right censored, 1=event at time, 2=left censored, 3=interval censored.
>
> Interval censored data can be represented in two ways. For the first use type = "interval" and the codes shown above. In that usage the value of the time2 argument is ignored unless event=3. The second approach is to think of each observation as a time interval with (-infinity, t) for left censored, (t, infinity) for right censored, (t,t) for exact and (t1, t2) for an interval. This is the approach used for type = interval2. Infinite values can be represented either by actual infinity (Inf) or NA. The second form has proven to be the more useful one.

> a subject's data for the pair of columns in the dataset (time1, time2) is (t_e, t_e) if the event time t_e is known exactly; (t_l, NA) if right censored (where t_l is the censoring time); and (t_l, t_u) if interval censored (where t_l is the lower and t_u is the upper bound of the interval).

midnight=0
11AM=660
noon=720
midnight=1440

~~~{.R}
set.seed(2015-06-21)
# simulate a scenario in which the mailman tends to come around 11AM (660) and I tend to check around then,
# and generate interval data for each time, bounded by end-of-day/midnight below & above, collecting ~1 month of data:
simulateMailbox <- function(n, time) {
    deliveryTime <- round(rnorm(n, mean = time, sd = 30))
    checkTime <- round(rnorm(n, mean = time, sd = 60))
    simulates <- mapply(function (ck, dy) { if(ck>dy) { return(c(0,ck)) } else { return(c(ck,1440)) }},
                         checkTime, deliveryTime)
    return(data.frame(Time1=simulates[1,], Time2=simulates[2,])) }
mail <- simulateMailbox(30, 660); mail
##        Time1 Time2
## 1    569  1440
## 2      0   664
## 3    592  1440
## 4    581  1440
## 5    596  1440
## 6      0   703
## 7      0   705
## 8    667  1440
## ...
library(ggplot2)
png(file="~/wiki/images/maildelivery-simulated.png", width = 800, height = 500)
ggplot(mail) + geom_segment(aes(x=Time1, xend=Time2, y=1:nrow(mail), yend=1:nrow(mail))) +
    geom_vline(xintercept=660, color="blue") + ylab("Day") + xlab("Time")
invisible(dev.off())
~~~

Inferring the mean time of delivery might sound difficult with such extremely crude data of intervals 700 minutes wide or worse, but plotting the little simulated dataset and marking the true mean time of 660, we see it's not *that* bad - the mean time is probably whatever line passes through the most intervals:

![The simulated overlapping-intervals data, with the true mean time drawn in blue](/images/maildelivery-simulated.png)

And also with aoursimulated dataset, we can see if the standard R survival library and a interval-censored model written in JAGS can recover the 660:

~~~{.R}
surv <- Surv(mail$Time1, mail$Time2, type="interval2")
s <- survfit(surv ~ 1, data=mail); summary(s)
##  time   n.risk   n.event survival   std.err lower 95% CI upper 95% CI
## 633.0 30.00000  7.503147 0.749895 0.0790680     0.609889     0.922041
## 651.0 22.49685 12.188431 0.343614 0.0867071     0.209546     0.563458
## 663.5 10.30842  0.611086 0.323245 0.0853927     0.192604     0.542496
## 685.0  9.69734  9.697336 0.000000       NaN           NA           NA
plot(s)
## https://i.imgur.com/nzOHQT0.png
sr <- survreg(surv ~ 1, dist="gaussian", data=mail); summary(sr)
##                 Value Std. Error        z           p
## (Intercept) 656.21819   7.324758 89.58906 0.00000e+00
## Log(scale)    2.93664   0.444159  6.61169 3.79957e-11
##
## Scale= 18.8524
##
## Gaussian distribution
## Loglik(model)= -7.1   Loglik(intercept only)= -7.1
## Number of Newton-Raphson Iterations: 10
## n= 30
~~~

## MCMC

More Bayesianly, we can write an interval-censoring model in JAGS, which gives us the opportunity to use an informative prior about the mean time the mailman comes.

They work normal 9-5 hours as far as I know, so we can rule out anything outside 540-1020.
From past experience, I expect the mail to show up not before 10AM (600) and not after 1PM (780), with those extremes being rare and sometime around 11AM (660) being much more common; so not a uniform distribution over 600-780 but a normal one centered on 660 and then somewhat arbitrarily saying that 600-700 represent 3 SDs out from the mean of delivery times to get SD=~30 minutes so in all, `dnorm(660, pow(30, -2))`.
The SD itself seems to me like it could range anywhere from a few minutes to an hour, but much beyond that is impossible (if the SD was over an hour, then every so often the mailman would have to come at 8AM!).

~~~{.R}
library(R2jags)
model1 <- "model { for (i in 1:n){
           y[i] ~ dinterval(t[i], dt[i,])
           t[i] ~ dnorm(mu,tau)
           }

           mu ~ dnorm(660, pow(30, -2))
           sd ~ dunif(0, 60)
           tau <- pow(1/sd, 2)

           y.new ~ dnorm(mu, tau)
           }"


# y=1 means the same thing as Event=3 for `Surv`; it means the event is hidden inside the interval, not observed or left- or right-censored
data <- list("dt"=mail, "n"=nrow(mail), "y"=rep(1, nrow(mail)))
inits <- function() { list(mu=rnorm(1),sd=30,t=as.vector(apply(mail,1,mean))) }
params <- c("mu","tau", "y.new")
j1 <- jags(data,inits, params, textConnection(model1), n.iter=1000000); j1
## Inference for Bugs model at "13", fit using jags,
##  3 chains, each with 2000 iterations (first 1000 discarded)
##  n.sims = 3000 iterations saved
##          mu.vect sd.vect    2.5%     25%     50%     75%   97.5%  Rhat n.eff
## mu       657.668   7.985 642.789 652.612 657.204 662.418 674.584 1.008   390
## tau        0.003   0.003   0.000   0.001   0.002   0.004   0.010 1.060    44
## deviance   0.000   0.000   0.000   0.000   0.000   0.000   0.000 1.000     1
1/sqrt(0.003)
# [1] 18.25741858
~~~

Both approaches' point-value mean time of 656/657 (10:54AM) come close to the true simulation value of 660 (11AM), validating the models.
The estimated standard deviation isn't as accurate (18 vs 30) but the credible interval reflects that it's a harder parameter to estimate and the estimate is still vague with only _n_=30.

~~~{.R}
set.seed(2015-06-24)
library(lubridate)
clockS <- function(t){hour(t)*60 + minute(t) + second(t)/60}

mail <- data.frame(Time=clockS(as.POSIXct(c("2015-06-20 11:00", "2015-06-21 11:06", "2015-06-23 11:03", "2015-06-24 11:05", "2015-06-25 11:00"), "EDT")),
    Delivered=c(FALSE, FALSE, TRUE, TRUE, TRUE))

mail <- data.frame(Time1 = ifelse(mail$Delivered, 0, mail$Time),
                           Time2 = ifelse(mail$Delivered,  mail$Time, 1440))

library(R2jags)
model1 <- "model { for (i in 1:n){
           y[i] ~ dinterval(t[i], dt[i,])
           t[i] ~ dnorm(mu,tau)
           }

           mu ~ dnorm(660, pow(30, -2))
           sd ~ dunif(0, 60)
           tau <- pow(1/sd, 2)

           y.new ~ dnorm(mu, tau)
           }"

data <- list("dt"=mail, "n"=nrow(mail), "y"=rep(1, nrow(mail)))
inits <- function() { list(mu=rnorm(1),sd=30,t=as.vector(apply(mail,1,mean))) }
params <- c("mu","tau", "y.new")
j1 <- jags(data, inits, params, textConnection(model1), n.iter=1000000); j1
##          mu.vect sd.vect    2.5%     25%     50%     75%   97.5%  Rhat n.eff
## mu       655.412  17.808 617.206 645.052 656.344 666.095 689.965 1.001  2000
## tau        0.002   0.005   0.000   0.000   0.001   0.001   0.009 1.001  3000
## y.new    656.285  45.388 564.130 630.034 656.955 681.479 750.348 1.001  3000

1/sqrt(j1$BUGSoutput$mean$tau)
# [1] 24.24244989

lowerMean <- j1$BUGSoutput$summary[c(2),][3]
medianMean  <- j1$BUGSoutput$mean$mu
upperMean <- j1$BUGSoutput$summary[c(2),][7]

lowerPredictive <- j1$BUGSoutput$summary[c(4),][3]
# medianMean ~= predictive mean in this case, so don't bother
upperPredictive <- j1$BUGSoutput$summary[c(4),][7]

reformat <- function (time) { paste0(as.character(round((time %/% 60))), ":",  as.character(round((time %% 60)))) }
sapply(c(lowerMean, medianMean, upperMean), reformat)
sapply(c(lowerPredictive, upperPredictive), reformat)

library(ggplot2)

png(file="~/wiki/images/maildelivery-real.png", width = 800, height = 500)
ggplot(mail) + geom_segment(aes(x=Time1, xend=Time2, y=1:nrow(mail), yend=1:nrow(mail))) +
    ylab("Day") + xlab("Time") +
    geom_vline(xintercept=medianMean, color="blue") +
    geom_vline(xintercept=lowerMean, color="green") +
    geom_vline(xintercept=upperMean, color="green") +
    geom_vline(xintercept=lowerPredictive, color="red") +
    geom_vline(xintercept=upperPredictive, color="red")
invisible(dev.off())
~~~

![Overlapping-intervals data, with the estimated mean time in blue, 95% CI around the mean time in green, and 95% predictive intervals as to when the delivery is made](/images/maildelivery-real.png)

TODO: maybe create an animated plot showing how the estimates change with each datapoint using http://rforpublichealth.blogspot.com/2014/12/animations-and-gifs-using-ggplot2.html http://cran.r-project.org/web/packages/animation/index.html ?

## ABC

Because JAGS provides an interval-censored distribution in the form of `dinterval()` with a [likelihood function](!Wikipedia), we can use MCMC for backward inference (reasoning from data to the underlying process)
But if it didn't, I sure wouldn't know how to write one down for it and then the MCMC wouldn't work; but I was able to write a little simulation of how the underlying process of delivery-and-checking works, which, given a set of parameters, spits out simulated results generated by the process, which is forward inference (reasoning from a version of an underlying process to see what it creates).
This is a common situation: you can write a good simulation, but you can't write a likelihood function.

[ABC](!Wikipedia "Approximate Bayesian computation") (exemplified in the fun example ["Tiny Data, Approximate Bayesian Computation and the Socks of Karl Broman"](http://www.sumsar.net/blog/2014/10/tiny-data-and-the-socks-of-karl-broman/)) is a remarkably simple and powerful idea which lets us take a forward simulation and use it to run backwards inference.

The simplest ABC goes like this:
You sample possible parameters from your prior, feed the set of parameters into your simulation, and if the result is identical to your data, you save that set of parameters.
At the end, you're left with a bunch of sets and that's your posterior distribution which you can look at the histograms of and calculate 95% densities etc.

So for the mail data, ABC goes like this:

~~~{.R}
simulateMailbox <- function(n, dTime, dSD) {
    deliveryTime <- round(rnorm(n, mean = dTime, sd = dSD))
    checkTime <- round(rnorm(n, mean = dTime, sd = dSD))
    simulates <- mapply(function (ck, dy) { if(ck>dy) { return(c(0,ck)) } else { return(c(ck,1440)) }},
                         checkTime, deliveryTime)
    return(data.frame(Time1=simulates[1,], Time2=simulates[2,])) }

# if both dataframes are sorted, comparison is easier
mailSorted <- mail[order(mail$Time1),]

mail_sim <- replicate(10000000, {
    # mu ~ dnorm(660, 30)
    mu <- rnorm(n=1, mean=660, sd=30)
    # sd ~ dunif(0, 60)
    sd <- runif(n=1, min=0, max=60)

    newData <- simulateMailbox(nrow(mailSorted), mu, sd)
    newDataSorted <- newData[order(newData$Time1),]

    if (all(newDataSorted == mailSorted)) { return(c(Mu=mu, SD=sd)) }
   }
  )
results <- Filter(function(x) {!is.null(x)}, mail_sim)
results <- data.frame(t(sapply(results,c)))
summary(results)
~~~

The first thing to note is efficiency: I can get reasonable number of samples in reasonable amount of time for _n_=1-3, but at 4 datapoints, it becomes slow.
There's so many possible datasets when 4 checks are simulated that almost all get rejected because they are not identical to the real dataset and it takes millions of samples and hours to run.
And this problem only gets worse for _n_=5 and higher

To run ABC more efficiently, you relax the requirement that the simulated data == real data and instead accept the pair of parameters if the simulated data is 'close enough' in some sense to the real data, close in terms of some summary statistic (hopefully sufficient) like the mean.
I don't know what are the sufficient statistics for a set of interval-censored data, but I figure that if the means of the pairs of times are similar in both datasets, then they are probably close enough for ABC to work, so I can use that as a rejection tolerance; implementing that and playing around, it seems I can make the difference in means as tight as <2 while still running fast.

~~~{.R}
mail_abc <- function(samples) {
    results <- list()
    n <- 0
    while (n<samples) {

      # Priors:
      ## mu ~ dnorm(660, 30)
      mu <- rnorm(n=1, mean=660, sd=30)
      ## sd ~ dunif(0, 60)
      sd <- runif(n=1, min=0, max=60)

      # generate new data set based on a pair of possible parameters:
      newData <- simulateMailbox(nrow(mail), mu, sd)

      # see if some summaries of the new data were within tolerance ϵ of the real data:
      if (abs(mean(newData$Time1) - mean(mail$Time1)) < 2 &&
          abs(mean(newData$Time2) - mean(mail$Time2)) < 2)
        { results <- list(c(Mu=mu, SD=sd), results); n <- n+1; }
    }
   return(results)
  }
sims <- mail_abc(1000)
results <- matrix(unlist(sims), ncol=2, byrow=TRUE)
summary(results)
##        V1                 V2
##  Min.   :624.7535   Min.   : 0.148301
##  1st Qu.:659.2788   1st Qu.: 2.806508
##  Median :661.7225   Median : 6.937644
##  Mean   :661.2441   Mean   :12.997532
##  3rd Qu.:663.9442   3rd Qu.:18.276974
##  Max.   :699.1050   Max.   :59.746685
~~~

The mean value here is acceptable but the SD is considerably off from the JAGS estimate (which I assume to be correct).
Post hoc, this makes some sense since my summary statistic *is* just means; it might make more sense to check the SD of each column as well (at the cost of more runtime).

Another way to summarize the dataset occurs to me while looking at the graphs: the most striking visual feature of the interval-censored data is how the 'needles' overlap slightly and it is this slight overlap which determines where the mean is; the most informative set of data would be balanced exactly between needles that fall to the left and needles that fall to the right, leaving as little room as possible for the mean to 'escape' out into the wider intervals and be uncertain.
(Imagine a set of data where all the needles fall to the left, because I only checked the mail at 2PM; I would then be extremely certain that the mail is not delivered after 2PM but I would have little more idea than when I started about when the mail is actually delivered in the morning and my posterior would repeat the prior.)
So I could use the count of left or right intervals (it doesn't matter if I use `sum(Time1 == 0)` or `sum(Time2 == 1440)` since they are mutually exclusive) as the summary statistic.

~~~{.R}
mail_abc <- function(samples) {
    results <- list()
    n <- 0
    while (n<samples) {

      # Priors:
      ## mu ~ dnorm(660, 30)
      mu <- rnorm(n=1, mean=660, sd=30)
      ## sd ~ dunif(0, 60)
      sd <- runif(n=1, min=0, max=60)

      # generate new data set based on a pair of possible parameters:
      newData <- simulateMailbox(nrow(mail), mu, sd)

      # see if a summary of the new data matches the old:
      if (sum(mail$Time1 == 0) == sum(newData$Time1 == 0))
        { results <- list(c(Mu=mu, SD=sd), results); n <- n+1; }
    }
   return(results)
  }
sims <- mail_abc(10000)
results <- matrix(unlist(sims), ncol=2, byrow=TRUE)
summary(results)
##        V1                 V2
##  Min.   :550.2925   Min.   : 0.08477393
##  1st Qu.:639.5443   1st Qu.:16.10635360
##  Median :660.1267   Median :31.09839041
##  Mean   :660.4666   Mean   :30.95371197
##  3rd Qu.:681.0658   3rd Qu.:45.92970616
##  Max.   :775.1745   Max.   :59.99744545
~~~

This summary, simple as it is, does much better in replicating the JAGS estimates.
