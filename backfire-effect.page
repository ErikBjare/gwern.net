---
title: Biased information as anti-information
description: Filtered data for a belief can rationally push you away from that belief
created: 19 Oct 2012
tags: psychology, statistics, politics
status: notes
belief: unlikely
...

> The backfire effect is a recently-discovered bias where arguments contrary to a person's belief leads to them believing even more strongly in that belief; this is taken as obviously "irrational". The "rational" update can be statistically modeled as a shift in the estimated mean of a normal distribution where each randomly distributed datapoint is an argument: new datapoints below the mean cause a shift of the inferred mean downward and likewise if above. When this model is changed to include the "censoring" of datapoints, then the valid inference changes and a datapoint below the mean can lead to a shift of the mean upwards. This suggests that providing a person with anything less than the best data contrary to, or decisive refutations of, one of their beliefs may result in them becoming even more certain of that belief. If it is enjoyable or profitable to argue with a person while one does less than one's best, it is bad to hold false beliefs, and this badness is not shared between both parties, then arguing online may constitute a negative externality: an activity whose benefits are gained by one party but whose full costs are not paid by the same party. In many moral systems, negative externalities are considered selfish and immoral; hence, lazy or half-hearted arguing may be immoral because it internalizes any benefits while possibly leaving the other person epistemically worse off.

> Although some scholars view such bias as irrational behavior, 48 it is perfectly rational if the goal is not to get at the "truth" of a given issue in order to be a better voter, but to enjoy the psychic benefits of being a political "fan."...See, e..g, Taber and Lodge, "Motivated Skepticism,"; Shenkman, Just How Stupid Are We?, ch. 3.
["Opening the Political Mind? The effects of self-affirmation and graphical information on factual misperceptions"](http://www.dartmouth.edu/~nyhan/opening-political-mind.pdf "Nyhan & Reifler 2011")

> In some cases, unwelcome information can backfire and strengthen their previous beliefs (e.g. Redlawsk 2002, Nyhan and Reifler 2010).
> - Redlawsk, David. "Implications of Motivated Reasoning for Voter Information Processing." International Society of Political Psychology, 2001

> However, Berinsky (N.d.) conducted several studies of myths about President Obama's health care plan and found that corrections were generally effective in reducing misperceptions among Republicans (the partisans most likely to hold those beliefs), particularly when those corrections were attributed to a Republican source.
>
> - Berinsky, Adam. N.d. "Rumors, Truth, and Reality: A Study of Political Misinformation." Unpublished manuscript.

> Only a few studies have directly tested the effects of providing citizens with correct information about political issues or topics about which they may be misinformed (see Nyhan and Reifler 2012 for a review). Most of these focus on the effects of false information on policy attitudes. Kuklinski et al. (2000; study 1), Gilens (2001), Berinsky (2007), Sides and Citrin (2007), Howell and West (2009), Mettler and Guardino (2011) and Sides (N.d.) all provided experimental participants with correct factual information about an issue and then asked about their policy preferences on the issue. The results have been mixed - Gilens, Howell and West, Mettler and Guardino, and Sides found that correct factual information changed participants' policy preferences, but Kuklinski et al., Sides and Citrin, and Berinsky did not.
>
> - Kuklinski, James H., Paul J. Quirk, Jennifer Jerit, David Schweider, and Robert F. Rich. 2000. "Misinformation and the Currency of Democratic Citizenship." The Journal of Politics, 62(3):790-816
> - Gilens, Martin. 2001. "Political Ignorance and Collective Policy Preferences." American Political Science Review, 95(2): 379-396
> - Sides, John and Jack Citrin. 2007. "How Large the Huddled Masses? The Causes and Consequences of Public Misperceptions about Immigrant Populations." Paper presented at the 2007 annual meeting of the Midwest Political Science Association, Chicago, IL
> - Howell, William G. and Martin R. West. 2009. "Educating the Public." Education Next 9(3): 41-47
> - Mettler, Suzanne and Matt Guardino, "From Nudge to Reveal," in Suzanne Mettler, 2011, The Submerged State: How Invisible Government Policies Undermine American Democracy. University of Chicago Press
> - Sides, John. N.d. "Stories, Science, and Public Opinion about the Estate Tax." Unpublished manuscript

*not* this:

> Wogalter, Marwitz, and Leonard (1992) presented another argument against selecting fillers on the basis of their resemblance to the suspect: The ''backfire effect'' refers to the idea that, somewhat ironically, the suspect might stand out if he or she was the basis for selecting the fillers in the lineup, because the suspect represents the central tendency or origin of the lineup. Clark and Tunnicliff (2001) reported evidence for the backfire effect. However, eyewitnesses' descriptions of the target are often sparse and sometimes do not even match the characteristics of the suspect (Lindsay, Martin, & Webber, 1994; Meissner, Sporer, & Schooler, in press; Sporer, 1996, in press).
>
> - Wogalter, M.S., Marwitz, D.B., & Leonard, D.C. (1992). Suggestiveness in photospread lineups: Similarity induces distinctiveness. Applied Cognitive Psychology, 6, 443-453.

["Part 3, Bayesian Updating of Political Beliefs: Normative and Descriptive Properties"](http://dl.dropbox.com/u/85192141/2007-bullock-excerpt.pdf), from [Bullock 2007](http://dl.dropbox.com/u/85192141/2007-bullock.pdf "Experiments on partisanship and public opinion: Party cues, false beliefs, and Bayesian updating")


Arguing is a negative externality, per the backfire effect:
http://www.motherjones.com/kevin-drum/2008/09/backfire-effect
http://www.nd.edu/~ghaeffel/Lilienfeld2009%20Perspectives%20on%20Psychological%20Science.pdf
http://www.jstor.org/stable/10.1086/656252?ai=rvmi=0af=R
http://www.desmogblog.com/want-sway-climate-change-skeptics-ask-about-their-personal-strengths-and-show-pictures
http://www.nytimes.com/2012/09/18/opinion/balanced-news-reports-may-only-inflame.html

Remember simple mode: arguer selects best argument he has; experimental confirmation? http://www.overcomingbias.com/2012/10/we-add-near-average-far.html
http://lesswrong.com/lw/aq2/fallacies_as_weak_bayesian_evidence/
https://en.wikipedia.org/wiki/Censoring_%28statistics%29

"damning with faint praise"

= 11:23 <@gwern> just an endless loop playing in my head. 'It may seem strange and prima facie irrational that receiving information in support of a belief may actually increase your belief in its opposite, but...'
= 11:24 <@gwern> 'Let us play a little game I call the Debate Game. You are trying to guess a real number P between 0 and 1; you opponent periodically tells you a real which is drawn from a normal distribution around P...'
= 11:25 <@gwern> 'Let us play a different game called the Random Debate Game, where the real is drawn randomly from said normal distribution; you hold no opinion on what P is, and in the first round are told 0.75. what do you conclude?'
= 11:26 <@gwern> 'Now let us play a third game, called the Worst Debate Game; now the real is not drawn randomly from said normal, but is at least one standard deviation between P; you are told 0.55, what do you conclude about P?'

22:50:07 < nshepperd> if you're talking to a proud supporter of position Â¬X, and they only give you one bad argument against X, that's evidence for X
22:50:58 <@gwern> nshepperd: note the crucial assumption there: you're assuming something about the distribution of the ~X arguments
22:52:06 <@gwern> after all, what if they're biased to giving you only bad arguments? are you really going to conclude that them providing a bad argument is
                  evidence for X?
22:52:41 < nshepperd> yeah, the assumption is that proud supporters will know all the good arguments
22:53:29 <@gwern> nshepperd: anyway, so that's the core argument: the naive belief that the backfire effect is 'perverse' is assuming the arguments are selected randomly so any argument of non-zero value ought to move them that direction, whereas the reality is you expect them to select their best argument, in which case you get an upper bound which can easily them the opposite direction
22:55:30 < nshepperd> I think this was mentioned in one of the sequence posts on filtered evidence
22:55:37 <@gwern> nshepperd: the math part is treating beliefs as reals 0-1, assuming a normal distribution of sampling arguments around the underlying belief, and showing that these different distributions yield the actual effects we claim (random selection = any arguments worth >0.5 increases confidence and vice-versa, biased selection toward max argument can decreases confidence even if >0.5)
22:57:38 < nialo> gwern:  does that still preserve expected evidence = 0?  that is, sum(p(argument) * confidenceChange(arguement)) still zero?
22:58:04 < nialo> (I ask cause it seems like it might and I can't prove it either way quickly)
22:58:11 < nialo> (er, might not*)
22:58:19 <@gwern> nialo: I think it does, it's just shifting a huge amount of probability mass toward the upper ranges
22:59:17 <@gwern> nialo: my intuition is that, let's say your estimate of their belief P is currently 0.5; if they produce a 0.5, you take this as an upper bound and conclude it's <0.5, but if they produce a 0.9, then you will be shocked and revise massively upward
22:59:38 < nshepperd> gwern: also if you expect them to list off a large number of arguments, then they only come up with two, then that is evidence they have less arguments on total
22:59:54 <@gwern> nshepperd: yeah. there's a clear connection to the hope function too, although I'm not sure what it is
23:00:05 <@gwern> it's really a kinda nifty model of political arguing
23:01:15 < clone_of_saturn> someone should see what happens when someone states a weak argument first, then a much stronger one
23:01:22 <@gwern> if only I were e.t. Jaynes, I could probably write this all out in an hour
23:02:21 <@gwern> clone_of_saturn: an interesting question. going weak then strong sort of implies that they either aren't good at judging arguments since then they would start with the strong, or it's not actually strong in some way you aren't judging right and that's why they left it for later
23:03:31 < nshepperd> or maybe they're listing their arguments in alphabetical order
23:04:32 < nialo> or in order by length
23:04:58 <@gwern> well, there's always alternative models available...
23:05:09 < Namegduf> People like to start with arguments they feel emotionally attached to
23:05:19 < Namegduf> Snappy ones, "heavy" ones, etc
23:05:28 < Namegduf> These are not always ones which are perceived as stronger
23:06:26 < Namegduf> That's my observation, anyway.
23:07:40 < nialo> also gwern, this seems to intersect with Yvain's thing about mainstream ideas have worse average arguments: http://squid314.livejournal.com/333353.html  (I'm not sure how precisely, but I think there's a useful distribution for expected argument quality in there somewhere)
23:12:31 <@gwern> the best I can think of is that beliefs which are really wrong will have a relatively narrow spread past the underlying belief - if you're a Klansman, it's hard to make an argument even worse than the underlying falsity of Klanism
23:12:40 < nshepperd> well, to do this properly, you need a distribution over the set of arguments the clever arguer is likely to possess
23:12:44 <@gwern> analogous to Gould's complexity argument
23:12:53 < nshepperd> given which hypothesis is actually true
23:13:00 <@gwern> you can't be less complex than viruses - there's a censorship of the data
23:13:26 <@gwern> so Klan arguments in practice will come from the right part of the Klan bell curve and be better than expected
23:13:57 <@gwern> if Klanism is at 0.2, I probably won't see the 0-0.2 arguments, but the 0.2-0.4 arguments, if you follow me
23:15:04 < nialo> that roughly tracks what I'd expect also, yes
23:15:16 <@gwern> but my brain is shutting down from all the statistics I've been learning today, so maybe that is not a very good point to extract from Yvain's post into my backfire model
23:17:20 < nshepperd> Well, suppose there's a fixed supply of arguments for each side, whose number and strength depends only on whether Klanism is true
23:17:42 < nshepperd> If Klanism is true there should be more and better arguments in favor of it
23:18:44 < nshepperd> A particular person possess some subset of this supply of arguments.
23:19:34 < nshepperd> However, Klansman, being non-mainstream, will have spent more time "mining" this argument supply for good arguments
23:21:00 < nshepperd> This affects your distribution over arguments you expect the Klansman you're talking with to have.
23:22:23 < nshepperd> Which in turn affects the arguments they'll actually tell you. So when you update on what they've said you have to sort of propagate that information back through each layer of the model to the factual truth of Klanism
23:25:05 < nshepperd> (Klansman says only one good argument, meaning he probably doesn't have many good arguments because he would select all his best, meaning there probably aren't many good arguments because Klansman spend a lot of time looking for arguments, meaning Klanism is probably false because otherwise there'd be more good arguments for it)
23:26:38 <@gwern> so how many good Klan arguments will finally be evidence for it?
23:30:33 < nshepperd> gwern: approximately, more arguments than you'd expect there to be if Klanism were wrong
23:33:04 <@gwern> nshepperd: which would be?
23:33:25 < nialo> insufficient data
23:33:47 <@gwern> nialo: my challenge wasn't for a real number but pseudocode for estimating it
23:33:57 < nshepperd> depends on your distributions of P(existing arguments | K)
23:34:28 <@gwern> nshepperd: that sounds like a boring model then
23:36:23 < nshepperd> you could make a toy model with gaussians with different means, maybe
23:36:59 < nialo> I think what nshepperd is saying is approximately equivalent to Klan arguments coming from the right half of the bell curve, as above
23:38:26 < nshepperd> say if Klanism is true there exist a large number of arguments distributed in quality according to Norm(u_true, sigma), otherwise the arguments are distributed according to Norm(u_false, sigma) where u_true > u_false
23:39:21 < nshepperd> then the Klansmen draw a bunch of arguments from whichever distribution is correct, and give you the top N they find
23:40:08 <@gwern> how does that differ from my model?
23:40:29 <@gwern> aside from collapsing multiple steps
23:41:46 < nialo> I think you end up with a slightly different distribution at the end?
23:41:58 < nshepperd> aside from being mathematically precise
23:47:02 <@gwern> (believing in conspiracy theories means always being hopeful - that at least there's someone to blame)
...
00:06:28 < nshepperd> toy example I just calculated: if P(argument quality | H) ~ Normal(mu=1.05, sigma=0.2) and P(argument quality | Â¬H) ~ Normal(mu=0.95, sigma=0.2) and the clever arguer possesses ten arguments drawn from whichever distribution is correct, and tells you his four best, which are [1.1, 1.09, 1.08, 1.07]
00:07:26 < nshepperd> then the likelihood ratio P(E|H)/P(E|Â¬H) for that evidence is 0.40, and it's evidence against H
00:08:50 < nshepperd> even though the arguments are on the high side of both distributions
00:11:35 < nshepperd> but if you make the clever arguer work less hard to find arguments, and say he only has 6 arguments, of which he tells you the best four, then that same set becomes evidence for H (likelihood ratio 1.29)
00:12:00 < clone_of_saturn> p(a|b) is pretty intuitive if you read the | as "given"
00:12:28 < nshepperd> because then you no longer expect that he would have found much better arguments

http://doingbayesiandataanalysis.blogspot.com/2012/01/complete-example-of-right-censoring-in.html

Frederic Bastiat: "The worst thing that can happen to a good cause is not to be skillfully attacked, but to be ineptly defended."
