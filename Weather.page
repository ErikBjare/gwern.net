---
title: Weather and My Productivity
description: Rain or shine affect my mood? Seems not.
created: 19 Mar 2013
tags: statistics, psychology
status: in progress
belief: likely
...

> `Touji:`   Oh, yes. the view of the world that one can have is quite small. \
> `Hikari:`  Yes, you measure things only by your own small measure. \
> `Asuka:`   One sees things with the truth, given by others. \
> `Misato:`  Happy on a sunny day. \
> `Rei:`     Gloomy on a rainy day. \
> `Asuka:`   If you're taught that, you always think so. \
> `Ritsuko:` But, you can enjoy rainy days.^[_Neon Genesis Evangelion_, episode 26 ["The Beast that Shouted 'I' at the Heart of the World"](http://www.oocities.org/gorene/text/Episode26.txt) (Literal Translation Project)]

The scientific literature has found that [lighting & temperature](http://blog.bufferapp.com/the-science-of-how-room-temperature-and-lighting-affects-our-productivity) & [the seasons](!Wikipedia "Seasonal affective disorder") matter to one's mood, focus, sleep and other things. Certainly, these things having large effects makes a lot of sense - who doesn't feel gloomy on a rainy day or happier on a sunny day?

But things everyone knows often turn out to be wrong; and in my case, it seems like my morning use of [vitamin D](Nootropics#vitamin-d) and evening [melatonin](Melatonin) supplementation may be screening off the environmental effects. So one rainy and then sunny day in March 2013, I wondered if I could find an influence of rain or darkness on my own mood or productivity.

As it happens, since 16 February 2012, I have been daily writing down my impression of whether my mood & productivity (MP) that day was average, below-average, or above; this gave me 399 ratings to work with. I didn't record any weather data, but weather data ought to be available online. So I can find it, and see what aspects of the weather influence my self-rating, if any.

<!-- TODO: schedule a followup in a year or three?; post to LW open thread; post to Data Analysis -->

# Data

A convenient source of data seems to be [Weather Underground](!Wikipedia "Weather Underground (weather service)"). Going via [API](http://allthingsr.blogspot.com/2012/04/getting-historical-weather-data-in-r.html) is overkill, since we just need data from [Islip](http://www.wunderground.com/history/airport/KISP/2013/3/19/MonthlyHistory.html?MR=1) for 16 February 2012 - 08 July 2012 ([CSV](http://www.wunderground.com/history/airport/KISP/2012/2/16/CustomHistory.html?dayend=8&monthend=7&yearend=2012&req_city=NA&req_state=NA&req_statename=NA&format=1)) and from [Pax River](http://www.wunderground.com/history/airport/KNHK/2012/7/11/CustomHistory.html?dayend=9&monthend=3&yearend=2013&req_city=NA&req_state=NA&req_statename=NA&MR=1) 11 July 2012 - 22 March 2013 ([CSV](http://www.wunderground.com/history/airport/KNHK/2012/7/11/CustomHistory.html?dayend=22&monthend=3&yearend=2013&req_city=NA&req_state=NA&req_statename=NA&format=1)). I'm using the nearest airport, rather than the nearest volunteer weather station, to get data on cloud cover; cloud cover should be a good proxy for brightness or sunniness. The [insol](http://cran.r-project.org/web/packages/insol/index.html) library calculates the length of day, another potentially relevant variable.

~~~{.R}
# gather and clean data
weather1 <- read.csv("http://www.wunderground.com/history/airport/KISP/2012/2/16/CustomHistory.html?dayend=8&monthend=7&yearend=2012&req_city=NA&req_state=NA&req_statename=NA&format=1")
weather2 <- read.csv("http://www.wunderground.com/history/airport/KNHK/2012/7/11/CustomHistory.html?dayend=22&monthend=3&yearend=2013&req_city=NA&req_state=NA&req_statename=NA&format=1")
weather1$PrecipitationIn <- as.numeric(as.character(weather1$PrecipitationIn)) # delete weird "T" factor; ???
# mood/productivity (MP) sourced from personal log
weather1$MP <- c(1,3,4,4,4,4,3,4,2,3,4,4,2,3,2,3,3,2,3,2,3,4,3,2,3,2,3,1,2,4,3,3,3,3,3,
                 3,3,2,3,4,3,3,4,4,4,4,2,3,4,3,3,3,4,3,3,1,3,2,3,3,4,3,2,3,2,4,4,4,3,3,
                 4,4,3,4,3,3,2,3,3,2,2,4,3,2,4,4,4,3,3,2,4,4,4,3,3,3,3,2,3,4,3,2,2,4,3,
                 3,2,2,2,2,3,4,2,4,4,3,3,3,2,4,2,2,2,2,2,3,3,4,2,1,3,3,2,3,3,4,2,3,2,3,
                 3,2,4,4)
weather2$MP <- c(2,4,4,4,3,4,3,3,4,3,2,3,4,4,4,3,2,3,3,2,3,3,3,2,2,2,2,2,3,4,3,4,2,4,3,
                 3,2,2,2,3,3,3,3,4,3,3,3,4,3,3,3,2,4,2,3,3,4,4,3,3,3,4,3,3,4,3,4,2,3,3,
                 4,4,3,3,4,4,3,4,3,2,3,3,3,4,3,2,3,2,2,2,3,3,3,4,4,3,4,4,3,3,2,3,3,3,3,
                 4,4,3,4,2,2,2,3,4,3,4,3,4,3,4,4,3,3,2,3,2,4,4,3,4,2,3,4,2,3,3,2,2,2,3,
                 2,3,3,4,2,3,4,3,4,3,3,2,2,3,4,4,3,4,2,2,3,2,3,2,2,2,4,3,3,4,2,2,3,3,3,
                 4,4,3,2,3,2,2,2,3,3,3,4,3,4,3,3,3,2,2,3,3,3,4,4,3,2,2,2,3,3,4,3,4,3,4,
                 3,2,4,4,3,3,2,4,3,3,3,2,3,3,2,3,2,3,3,3,3,2,3,3,3,3,3,3,4,2,3,2,4,3,3,
                 2,2,3,3,3,2,4,3,3,3)
# compute length of day for each location
library(insol)
weather1$DayLength <- daylength(40,73, julian(as.Date(as.character(weather1$EDT))), 1)[,3]
weather2$DayLength <- daylength(38,76, julian(as.Date(as.character(weather2$EDT))), 1)[,3]
# combine & start cleaning
weather <- rbind(weather1,weather2)
weather$EDT <- NULL # we don't use the date anywhere & it causes problems with all the models, so delete it
weather$WindDirDegrees.br... <- sub("<br />", "", weather$WindDirDegrees.br...)
weather$WindDirDegrees.br... <- as.integer(weather$WindDirDegrees.br...)
weather$Events <- as.integer(weather$Events)
weather$Events[weather$Events>1] <- 2
# something of a hack but we'll impute any missing rain precipitation values as 0
weather[is.na(weather)] <- 0
~~~

# Analysis
## Exploratory

After cleanup, the data looks reasonable and as expected, with each variable spanning the ranges one would expect of temperatures and precipitation:

~~~{.R}
summary(weather)
 Max.TemperatureF Mean.TemperatureF Min.TemperatureF Max.Dew.PointF MeanDew.PointF Min.DewpointF
 Min.   : 26.0    Min.   :21.0      Min.   :14.0     Min.   :12.0   Min.   : 1.0   Min.   :-5.0
 1st Qu.: 50.0    1st Qu.:44.0      1st Qu.:36.0     1st Qu.:38.0   1st Qu.:31.2   1st Qu.:24.0
 Median : 63.0    Median :55.0      Median :46.0     Median :51.0   Median :45.0   Median :37.5
 Mean   : 64.5    Mean   :56.5      Mean   :48.4     Mean   :50.5   Mean   :44.8   Mean   :38.8
 3rd Qu.: 78.0    3rd Qu.:69.0      3rd Qu.:62.0     3rd Qu.:64.0   3rd Qu.:60.0   3rd Qu.:54.0
 Max.   :100.0    Max.   :87.0      Max.   :79.0     Max.   :79.0   Max.   :73.0   Max.   :71.0
  Max.Humidity   Mean.Humidity   Min.Humidity  Max.Sea.Level.PressureIn Mean.Sea.Level.PressureIn
 Min.   : 42.0   Min.   :28.0   Min.   :12.0   Min.   :29.4             Min.   :29.1
 1st Qu.: 82.0   1st Qu.:58.2   1st Qu.:37.0   1st Qu.:30.0             1st Qu.:29.9
 Median : 89.0   Median :68.0   Median :47.5   Median :30.1             Median :30.0
 Mean   : 85.6   Mean   :67.6   Mean   :48.5   Mean   :30.1             Mean   :30.0
 3rd Qu.: 93.0   3rd Qu.:77.0   3rd Qu.:60.0   3rd Qu.:30.2             3rd Qu.:30.2
 Max.   :100.0   Max.   :97.0   Max.   :93.0   Max.   :30.6             Max.   :30.5
 Min.Sea.Level.PressureIn Max.VisibilityMiles Mean.VisibilityMiles Min.VisibilityMiles
 Min.   :28.7             Min.   : 6.00       Min.   : 2.00        Min.   : 0.00
 1st Qu.:29.8             1st Qu.:10.00       1st Qu.: 9.00        1st Qu.: 3.00
 Median :29.9             Median :10.00       Median :10.00        Median :10.00
 Mean   :29.9             Mean   : 9.97       Mean   : 9.05        Mean   : 6.95
 3rd Qu.:30.1             3rd Qu.:10.00       3rd Qu.:10.00        3rd Qu.:10.00
 Max.   :30.5             Max.   :10.00       Max.   :10.00        Max.   :10.00
 Max.Wind.SpeedMPH Mean.Wind.SpeedMPH Max.Gust.SpeedMPH PrecipitationIn   CloudCover
 Min.   : 6.0      Min.   : 1.00      Min.   : 0.0      Min.   :0.000   Min.   :0.00
 1st Qu.:13.0      1st Qu.: 6.00      1st Qu.: 0.0      1st Qu.:0.000   1st Qu.:3.00
 Median :16.0      Median : 8.00      Median :22.0      Median :0.000   Median :5.00
 Mean   :16.9      Mean   : 8.16      Mean   :19.1      Mean   :0.123   Mean   :4.47
 3rd Qu.:21.0      3rd Qu.:10.00      3rd Qu.:28.0      3rd Qu.:0.030   3rd Qu.:6.00
 Max.   :40.0      Max.   :27.00      Max.   :56.0      Max.   :6.000   Max.   :8.00
     Events     WindDirDegrees.br... MP
 Min.   :1.00   Min.   :  1          1:  4
 1st Qu.:1.00   1st Qu.:143          2: 98
 Median :1.00   Median :214          3:182
 Mean   :1.46   Mean   :208          4:102
 3rd Qu.:2.00   3rd Qu.:289
 Max.   :2.00   Max.   :360
~~~

`CloudCover` being 0-8 looks a bit odd, since one might've expected a decimal or percentage or some quantification of thickness but turns out to be a standard measurement, the "[okta](!Wikipedia)". We might expect some seasonal effects, but graphing a sensitive [LOESS](!Wikipedia) [moving average](!Wikipedia) and then a per-week spineplot suggests just 1 anomaly, that there were some days I rated "1" but never subsequently (this has an explanation: as I began keeping the series, my car caught on fire, was totaled; and I was then put through the wringer with the insurance & junkyard, and felt truly miserable at multiple points):

~~~{.R}
par(mfrow=c(2,1))
scatter.smooth(x=weather$EDT, y=weather$MP, span=0.3, col="#CCCCCC", xlab="Days", ylab="MP rating")
weeks <- c(seq(from=1,to=length(weather$MP),by=7), length(weather$MP))
spineplot(c(1:(length(weather$MP))), as.factor(weather$MP), breaks=weeks, xlab="Weeks", ylab="MP rating ratios")
~~~

![MP from February 2012 to March 2013](/images/2013-weather-seasonal.png)

A formal check for [autocorrelation](!Wikipedia) in MP ratings (most methods assume independence) turns up little enough[^autocorrelation] that I feel free to ignore them.

[^autocorrelation]: A test using `acf` shows no autocorrelation worth mentioning:

    ~~~{.R}
    acf(weather$MP, main="Do days predict subsequent days at various distances?")
    ~~~

    ![MP series shows almost no autocorrelation at any timelag](/images/2013-weather-autocorrelation-mp.png)

    For comparison, here is `acf` for a data series where one would expect a great deal of autocorrelation - my daily weight - and one does indeed observe it:

    ~~~{.R}
    acf(c(205,214,216,213,213,218,218,214,215,216,218,216,210,219,217,
          219,217,215,215,217,219,216,215,218,215,219,219,219,218,218,
          220,220,219,219,219,221,220,219,216,220,220,218,218,220,219,
          220,215,215,218,218,215,216,216,218,218,220,219,216,217,220,
          215,215,218,216,214,213,216,215,214,213,214,216,216,212,209,
          212,214,213,210,211,210,213,211,215,211,212,212,216,212,215,
          216,215,215,212,216,213,212,211,215,214,215,216,214,212,212,
          213,212,213,211,214,215,210,211,211,211,212,210,210,212,211,
          214,213,214,212,215,214,213,215,211,214,214,216,215,213,215,
          213,213,212,215,211,212,212,211,212,211,212,210,211,213,218,
          217,212,214,216,213,212,211,211,214,212,211,216,218,216,214,
          215,216,216,213,216,214,215,219,218,216,215,218,217,219,219,
          219,219,219,218,217,216,216,215,218,219,217,216,219,217,216,
          216,219,216,218,215,216,215,215,213,214,215,217,216,215,215,
          216,214,215,215,214,216,211,214,213,214,211,212,211,210,212,
          211,212,214,212,211,214,212,211,212,211,212,213,210,212,210,
          210,211,210))
    ~~~

    ![Weight series showing autocorrelation at every timelag](/images/2013-weather-autocorrelation-weight.png)

So the data looks clean and tame; now we can begin real interpretation. The first and most obvious thing to do is to see what the overall correlation matrix with MP looks like:

~~~{.R}
# all weather correlations with mood/productivity
round(cor(weather, use="complete.obs")[23,], digits=3)
         Max.TemperatureF         Mean.TemperatureF          Min.TemperatureF
                    0.012                     0.004                     0.000
           Max.Dew.PointF            MeanDew.PointF             Min.DewpointF
                    0.002                    -0.007                    -0.019
             Max.Humidity             Mean.Humidity              Min.Humidity
                    0.015                    -0.025                    -0.052
 Max.Sea.Level.PressureIn Mean.Sea.Level.PressureIn  Min.Sea.Level.PressureIn
                    0.007                    -0.009                    -0.008
      Max.VisibilityMiles      Mean.VisibilityMiles       Min.VisibilityMiles
                    0.078                     0.021                     0.000
        Max.Wind.SpeedMPH        Mean.Wind.SpeedMPH         Max.Gust.SpeedMPH
                    0.046                     0.016                     0.030
          PrecipitationIn                CloudCover                    Events
                   -0.051                     0.004                    -0.052
     WindDirDegrees.br...                                           DayLength
                    0.043                                               0.025

~~~

All the _r_ values seem very small; only 2 are >0.1, and not by much. But some of the more plausible correlations may be statistically significant, so we'll look for that:

~~~{.R}
sapply(c("Events", "Min.Humidity", "Min.VisibilityMiles", "PrecipitationIn", "CloudCover", "Events", "DayLength"),
       function(x) cor.test(weather$MP, weather[[x]]))
...
            Events                                 Min.Humidity
p.value     0.2993                                 0.2964
estimate    -0.05209                               -0.0524

            Min.VisibilityMiles                    PrecipitationIn
p.value     0.9945                                 0.3086
estimate    -0.0003455                             -0.0511

            CloudCover                             Events
p.value     0.9437                                 0.2993
estimate    0.003549                               -0.05209

            DayLength
p.value     0.6254
estimate    0.02451
~~~

These specific variables turn out to be failures too, with large _p_-values.

One useful technique is to convert metrics into standardized units (of standard deviations), sum them all into a single composite variable, and then test that; this can reveal influences not obvious if one looked at the metrics individually. (For example, in my [potassium sleep experiments](Zeo#potassium), where I was interested in an overall measure of reduced sleep quality rather than single metrics like sleep latency.) Perhaps this would work here?

~~~{.R}
# construct a z-score for all of them to see if it does any better
cor.test(weather$MP, (weather$Events + scale(weather$Min.Humidity) + scale(weather$Min.VisibilityMiles)
                      + scale(weather$PrecipitationIn) + scale(weather$CloudCover) + scale(weather$DayLength)))
...
t = -0.8365, df = 397, p-value = 0.4034
alternative hypothesis: true correlation is not equal to 0
95 percent confidence interval:
 -0.13955  0.05646
sample estimates:
     cor
-0.04195
~~~

## Modeling
### Continuous MP
#### Linear model

The correlations show individually little value, so we'll move on to building modeling and assessing their fit. Can we accurately predict MP if we use all the parameters? We'll start with a linear model/regression, where we treat the categorical MP variable as a continuous variable for simplicity:

~~~{.R}
model1 <- lm(MP ~ ., data=weather); summary(model1)
...
Residuals:
    Min      1Q  Median      3Q     Max
-2.0809 -0.7307  0.0203  0.6627  1.4204

Coefficients:
                           Estimate Std. Error t value Pr(>|t|)
(Intercept)               -2.73e+00   7.04e+00   -0.39     0.70
Max.TemperatureF           3.70e-02   3.91e-02    0.95     0.34
Mean.TemperatureF         -6.47e-02   7.36e-02   -0.88     0.38
Min.TemperatureF           3.56e-02   3.85e-02    0.93     0.36
Max.Dew.PointF            -4.22e-04   1.66e-02   -0.03     0.98
MeanDew.PointF            -8.03e-03   3.04e-02   -0.26     0.79
Min.DewpointF             -6.85e-03   1.58e-02   -0.43     0.66
Max.Humidity               6.31e-03   9.79e-03    0.65     0.52
Mean.Humidity              9.16e-05   1.58e-02    0.01     1.00
Min.Humidity               2.80e-03   9.38e-03    0.30     0.77
Max.Sea.Level.PressureIn   1.31e+00   1.05e+00    1.24     0.22
Mean.Sea.Level.PressureIn -1.78e+00   1.76e+00   -1.01     0.31
Min.Sea.Level.PressureIn   5.50e-01   9.10e-01    0.60     0.55
Max.VisibilityMiles        1.94e-01   1.50e-01    1.29     0.20
Mean.VisibilityMiles      -2.82e-03   4.39e-02   -0.06     0.95
Min.VisibilityMiles       -8.46e-03   2.28e-02   -0.37     0.71
Max.Wind.SpeedMPH          1.32e-02   1.50e-02    0.88     0.38
Mean.Wind.SpeedMPH        -8.96e-03   1.87e-02   -0.48     0.63
Max.Gust.SpeedMPH         -1.45e-03   5.34e-03   -0.27     0.79
PrecipitationIn           -1.51e-01   1.00e-01   -1.51     0.13
CloudCover                 3.22e-02   2.72e-02    1.18     0.24
Events                    -1.82e-01   1.14e-01   -1.61     0.11
WindDirDegrees.br...       2.53e-04   4.62e-04    0.55     0.58
DayLength                  7.64e-02   5.51e-02    1.39     0.17

Residual standard error: 0.752 on 375 degrees of freedom
Multiple R-squared: 0.0391, Adjusted R-squared: -0.0198
F-statistic: 0.664 on 23 and 375 DF,  p-value: 0.88
~~~

The variance explained is extremely minimal, even with 23 different variables in the linear model, and suggests overfitting (as one would expect). By Occam's razor, most of the variables should be scrapped for not carrying their weight; the `step` function uses a [complexity penalty](!Wikipedia "Akaike information criterion") to choose which variable to eliminate while still fitting the data reasonably well. It choose to keep only 2 variables:

~~~{.R}
smodel1 <- step(model1); summary(smodel1)
...
lm(formula = MP ~ 1, data = weather)

Residuals:
    Min      1Q  Median      3Q     Max
-1.9875 -0.9875  0.0125  1.0125  1.0125

Coefficients:
            Estimate Std. Error t value Pr(>|t|)
(Intercept)   2.9875     0.0373    80.1   <2e-16

Residual standard error: 0.745 on 398 degrees of freedom
~~~

The model with *no* data variables is much simpler, and appears to fit as well or better. We can compare the models' prediction accuracy on the dataset (lower is better in mean-squared-error):

~~~{.R}
mean((weather$MP - weather$MP)^2) # perfect
[1] 0
mean((weather$MP - model1$fitted.values)^2) # original
[1] 0.5321
mean((weather$MP - smodel1$fitted.values)^2) # simpler model
[1] 0.5537
~~~

### Random forests regression

Linear modeling having failed to reveal any interesting relationships, we'll take one last crack at it: if there's any interesting predictive power in the weather data at all, a high-powered machine learning technique like [random forests](!Wikipedia) ought to build a model which outperforms the linear model at least a little on mean-squared-error

~~~{.R}
library(randomForest)
rmodel <- randomForest(MP ~ ., data=weather, proximity=TRUE); rmodel
...
                     Number of trees: 500
No. of variables tried at each split: 7

          Mean of squared residuals: 0.5608
                    % Var explained: -1.29
~~~

The error with all variables is a little higher than the simpler linear model. Troublingly, a random forests with the simpler variables (the last two variables left in the `step` procedure before it deleted all of them) increases the error:

~~~{.R}
srmodel <- randomForest(MP ~ Max.Wind.SpeedMPH + PrecipitationIn, data=weather, proximity=TRUE); srmodel
...
               Type of random forest: regression
                     Number of trees: 500
No. of variables tried at each split: 1

          Mean of squared residuals: 0.6108
                    % Var explained: -9.17
~~~

Perhaps treating MP as a continuous variable was a bad idea. Let's start over.

### Categorical MP

We turn the MP data into an ordered factor:

~~~{.R}
weather$MP <- ordered(weather$MP)
~~~

#### Logistic model

A straight [logistic regression](!Wikipedia) (`glm(MP ~ ., data=weather, family="binomial")`) is not appropriate because MP is not a binary outcome; we use the [MASS](http://cran.r-project.org/web/packages/MASS/index.html) library to do an ordinal logistic regression; as with the linear model, the coefficients don't seem to differ very much

~~~{.R}
library(MASS)
lmodel <- polr(as.ordered(MP) ~ ., data = weather); summary(lmodel)
...
Coefficients:
                              Value Std. Error   t value
Max.TemperatureF           0.081294    0.09662   0.84134
Mean.TemperatureF         -0.144929    0.18183  -0.79708
Min.TemperatureF           0.078656    0.09563   0.82247
Max.Dew.PointF             0.000131    0.04109   0.00318
MeanDew.PointF            -0.017673    0.07449  -0.23725
Min.DewpointF             -0.017735    0.03790  -0.46796
Max.Humidity               0.018943    0.02391   0.79216
Mean.Humidity             -0.000704    0.03922  -0.01795
Min.Humidity               0.004597    0.02341   0.19637
Max.Sea.Level.PressureIn   3.427293    0.93423   3.66859
Mean.Sea.Level.PressureIn -4.817837    0.06750 -71.37750
Min.Sea.Level.PressureIn   1.613441    0.99164   1.62705
Max.VisibilityMiles        0.537582    0.38427   1.39896
Mean.VisibilityMiles      -0.008728    0.10912  -0.07998
Min.VisibilityMiles       -0.017014    0.05670  -0.30008
Max.Wind.SpeedMPH          0.031269    0.03715   0.84171
Mean.Wind.SpeedMPH        -0.026754    0.04763  -0.56172
Max.Gust.SpeedMPH         -0.001229    0.01337  -0.09194
PrecipitationIn           -0.400005    0.24630  -1.62403
CloudCover                 0.088216    0.06536   1.34966
Events                    -0.450000    0.28126  -1.59994
WindDirDegrees.br...       0.000900    0.00115   0.78007
DayLength                  0.204261    0.13844   1.47545

Intercepts:
    Value   Std. Error t value
1|2  11.112   0.023    481.489
2|3  14.735   0.501     29.434
3|4  16.876   0.514     32.818

Residual Deviance: 858.75
AIC: 910.75

# relative risk or odds; how much difference does each variable make, per its units?
exp(coef(lmodel))
         Max.TemperatureF         Mean.TemperatureF          Min.TemperatureF
                 1.084690                  0.865084                  1.081832
           Max.Dew.PointF            MeanDew.PointF             Min.DewpointF
                 1.000131                  0.982483                  0.982421
             Max.Humidity             Mean.Humidity              Min.Humidity
                 1.019123                  0.999296                  1.004607
 Max.Sea.Level.PressureIn Mean.Sea.Level.PressureIn  Min.Sea.Level.PressureIn
                30.793160                  0.008084                  5.020058
      Max.VisibilityMiles      Mean.VisibilityMiles       Min.VisibilityMiles
                 1.711863                  0.991310                  0.983130
        Max.Wind.SpeedMPH        Mean.Wind.SpeedMPH         Max.Gust.SpeedMPH
                 1.031763                  0.973601                  0.998771
          PrecipitationIn                CloudCover                    Events
                 0.670317                  1.092224                  0.637628
     WindDirDegrees.br...                 DayLength
                 1.000900                  1.226618
~~~

Another use of `step`; this time, it builds something more interesting than a constant predictor:

~~~{.R}
slmodel <- step(lmodel); summary(slmodel)
...
Coefficients:
                      Value Std. Error t value
Min.DewpointF       -0.0240     0.0120   -2.00
Max.Humidity         0.0169     0.0106    1.60
Max.VisibilityMiles  0.5424     0.3395    1.60
DayLength            0.1839     0.1012    1.82

Intercepts:
    Value  Std. Error t value
1|2  3.452  3.598      0.959
2|3  7.061  3.603      1.959
3|4  9.165  3.613      2.536

Residual Deviance: 868.85
AIC: 882.85

exp(coef(slmodel))
      Min.DewpointF        Max.Humidity Max.VisibilityMiles           DayLength
             0.9763              1.0171              1.7202              1.2019
~~~

These seem like reasonable variables; the [dew point](!Wikipedia "Dew point#Relationship to human comfort") temperature reflects the barometric pressure, so perhaps that variable is tapping into some sort of rain variable.

#### Random forests classification

Random forests can be used to classify (predict categorical outcomes) as well as regressions; having turned the response variable into a factor, the type switches automatically:

~~~{.R}
rmodel <- randomForest(MP ~ ., data=weather, proximity=TRUE); rmodel
...
               Type of random forest: classification
                     Number of trees: 500
No. of variables tried at each split: 4

        OOB estimate of  error rate: 57.89%
Confusion matrix:
  1  2   3  4 class.error
1 0  0   4  0      1.0000
2 0 11  84  6      0.8911
3 0 21 142 27      0.2526
4 0  7  82 15      0.8558
~~~

We can't `step` through random forests since it doesn't have a complexity measure like AIC to use, so we'll reuse the variables from the simplified ordinal:

~~~{.R}
srmodel <- randomForest(MP ~ Min.DewpointF + Max.Humidity + Max.VisibilityMiles + DayLength, data=weather, proximity=TRUE)
srmodel
...
               Type of random forest: classification
                     Number of trees: 500
No. of variables tried at each split: 2

        OOB estimate of  error rate: 66.92%
Confusion matrix:
  1  2   3  4 class.error
1 0  0   3  1      1.0000
2 0 14  72 15      0.8614
3 0 45 100 45      0.4737
4 0 14  72 18      0.8269
~~~

No improvement; perhaps the forests were benefiting from the other data variables.

#### Ordinal vs Random forests

We can now compare the fraction of days that are incorrectly predicted by the constant predictor, the ordinal logistic regression (full & simplified), and the random forests (full & simplified):

~~~{.R}
1 - (sum(weather$MP==3) / length(weather$MP))
[1] 0.5238

1 - (sum(weather$MP == as.integer(predict(lmodel))) / length(weather$MP))
[1] 0.5038
1 - (sum(as.integer(weather$MP) == predict(slmodel)) / length(weather$MP))
[1] 0.5188

1 - (sum(as.integer(weather$MP) == as.integer(predict(rmodel))) / length(weather$MP))
[1] 0.5815
1 - (sum(as.integer(weather$MP) == as.integer(predict(srmodel))) / length(weather$MP))
[1] 0.6692
~~~

It would seem that the ordinal logistic regressions are equivalent or better than the constant predictor, but the random forests does worse than either.

### Model checking
#### Error rate

Before concluding that the ordinal logistic regression is better than the constant predictor, it might be a good idea to check how robust this result holds up. The difference in correctly classified days is very small, and it might represent minimal advantage. We'll [bootstrap](!Wikipedia "Bootstrapping (statistics)") a large number of logistic regressions on samples of the full dataset, and see what fraction of them incur a higher classification error rate than the constant predictor:

~~~{.R}
library(boot)
classify <- function(dt, indices) {
  d <- dt[indices,] # allows boot to select subsample
  lmodel <- polr(as.ordered(MP) ~ ., data = d) # train new regression model on subsample
  return(1 - (sum(d$MP == predict(lmodel)) / length(d$MP)))
}
bs <- boot(data=weather, statistic=classify, R=10000); bs; bs$t
ORDINARY NONPARAMETRIC BOOTSTRAP
...
Bootstrap Statistics :
    original   bias    std. error
t1*   0.5038 -0.01347     0.02866

boot.ci(bs)
...
Intervals :
Level      Normal              Basic
95%   ( 0.4610,  0.5734 )   ( 0.4586,  0.5739 )

Level     Percentile            BCa
95%   ( 0.4336,  0.5489 )   ( 0.4612,  0.5767 )
Calculations and Intervals on Original Scale


hist(bs$t, xlab="Error rate", ylab="Number of samples",
           main="Bootstrap check of ordinal logistic regression accuracy")
sum(bs$t > (1 - (sum(weather$MP==3) / length(weather$MP)))) / length(bs$t)
[1] 0.1129
~~~

![Distribution of logistic regression classification rates](/images/2013-weather-bootstrap.png)

There's substantial uncertainty in the classification rate, as evidence by the potentially wide confidence intervals, but in ~11% of the new logistic regressions, the classification rate was worse than the constant predictor. This is not too bad but the logistic regression is not outperforming random by very much, so the reality of the result is open to question.

#### Power simulation

An earlier version of this analysis with ~13 fewer days yielded a simplified logistic regression which was a constant predictor; adding the data then yielded a real model using 2 temperature parameters. But this simplified model still performed worse. This raises a question: perhaps there is a barely legible signal in the data which adding more data would allow clear inference of. 399 days may seem like a lot, but is not much when split over 20+ data variables; we might not expect the effects to be very large, and the response variable is a ternary variable and so at best a rough measure of the underlying reality. We may simply lack [statistical power](!Wikipedia).

Unfortunately, a post hoc power analysis for a ordinal logistic regression is not easily done and I cannot find any R libraries or functions which provide power analysis along the lines of `power.t.test` or `power.anova.test`. The [best advice I found](http://stats.stackexchange.com/a/22410) on the topic was to essentially bootstrap with a known effect and test empirically how often one's regression had the power to detect the known effect.

library(rms)
tmpfun <- function(n, beta0, beta1, beta2) {
    x <- runif(n, 0, 10)
    eta1 <- beta0 + beta1*x
    eta2 <- eta1 + beta2
    p1 <- exp(eta1)/(1+exp(eta1))
    p2 <- exp(eta2)/(1+exp(eta2))
    tmp <- runif(n)
    y <- (tmp < p1) + (tmp < p2)
    fit <- lrm(y~x)
    fit$stats[5]
}

out <- replicate(1000, tmpfun(100, -1/2, 1/4, 1/4))
mean( out < 0.05 )

~~~{.R}
found <- function(dt, indices) {
  d <- dt[indices,]
  # uniform number 0=0.5; the bias (>0.5 is implausibly large)
  r <- round(runif(1, max=0.5), digits=2)
  # build up a sample with identical distribution to MP
  d$Z <- sample(4, length(d$MP), prob = c(4/399, 101/399, 190/399, (104/399)), replace = T)
  # with probability r, replace each item of Z with a copy of the corresponding MP
  d$Z <- ifelse((runif(length(d$Z))<=r), d$MP, d$Z)
  # train simplified regression model on subsample
  slmodel <- try(step(multinom(MP ~ ., data = d, maxit=200, trace=FALSE), trace=0))
  slmodel
  # did we error out due to weird sporadic `step` bug? and was Z selected in the simplified model?
  print(c(r, !is.na(match("Z", slmodel$coefnames))))
  if("try-error" %in% class(slmodel)) return(c(NA,NA)) else return(c(r, !is.na(match("Z", slmodel$coefnames))))
}
# the easy way?
bs <- boot(data=weather, statistic=found, R=10); bs
bs <- boot(data=weather, statistic=function(x,y) { tryCatch(found(x,y),error = function(){return(NA,NA)})}, R=2); bs



# the hard way
lappend <- function(lst, obj) {
    lst[[length(lst)+1]] <- obj
    return(lst)
}
results <- NULL
for (i in 1:1000) { res <- boot(data=weather, statistic=found, R=1); results <- lappend(results, res); }
bs <- matrix(unlist(lapply(results, function(x) x[["t"]])), ncol=2, byrow=T)

found <- function(dt, indices) {
  d <- dt[indices,]
  # uniform number 0=0.5; the bias (>0.5 is implausibly large)
  r <- round(runif(1, max=0.5), digits=2)
  # build up a sample with identical distribution to MP
  d$Z <- sample(4, length(d$MP), prob = c(4/399, 101/399, 190/399, (104/399)), replace = T)
  # with probability r, replace each item of Z with a copy of the corresponding MP
  d$Z <- ifelse((runif(length(d$Z))<=r), d$MP, d$Z)
  # train simplified regression model on subsample
  slmodel <- try(step(multinom(MP ~ ., data = d, maxit=200, trace=FALSE), trace=0))
  slmodel
  # did we error out due to weird sporadic `step` bug? and was Z selected in the simplified model?
  print(c(r, !is.na(match("Z", slmodel$coefnames))))
  if("try-error" %in% class(slmodel)) return(c(NA,NA)) else return(c(r, !is.na(match("Z", slmodel$coefnames))))
}
# the easy way?
bs <- boot(data=weather, statistic=found, R=10); bs
bs <- boot(data=weather, statistic=function(x,y) { tryCatch(found(x,y),error = function(){return(NA,NA)})}, R=2); bs


set.seed(666)
x1 = rnorm(1000)           # some continuous variables
x2 = rnorm(1000)
z = 1 + 2*x1 + 3*x2        # linear combination with a bias
pr = 1/(1+exp(-z))         # pass through an inv-logit function
y = rbinom(1000,1,pr)      # bernoulli response variable
foo = rbinom(1000,4,0.5) # distractor
#now feed it to glm:
df = data.frame(y=y,x1=x1,x2=x2,foo=foo)
step(glm(y~x1+x2+foo,data=df,family="binomial"))

library(nnet)

included <- function(dt, indices) {
d <- weather
l <- length(d$MP)
x1 <- rnorm(l) # continuous variables
x2 <- rnorm(l)
coef1 <- 1.1 # runif(1, max=3,min=1.0)
coef2 <- 1.1 # runif(1, max=3,min=1.0)
z <- 1 + coef1*x1 + coef2*x2 # bias
pr <- 1/(1+exp(-z))  # pass through an inv-logit function
y <- 1 + rbinom(l,3,pr)  # bernoulli response variable
d$MP <- y; d$X1 <- x1; d$X2 <- x2 # force a signal and provide its predictor
slmodel <- try(step(multinom(MP ~ ., data = d, maxit=200, trace=FALSE),trace=0))
c(coef, !is.na(match("X", slmodel$coefnames))); cor(d$MP,d$X1); cor(d$MP,d$X2)


if("try-error" %in% class(slmodel)) return(c(NA,NA)) else return(c(r, !is.na(match("X", slmodel$coefnames))))
}
bs <- boot(data=weather, statistic=included, R=2); bs


found <- function(dt, indices) {
d <- dt[indices,]
# borrowed from
# http://sas-and-r.blogspot.com/2009/06/example-72-simulate-data-from-logistic.html
l <- length(d$MP)
intercept = 0
beta = runif(1,max=0.4) # we know an effect as shockingly high as 0.4 can be detected
xtest = rnorm(l,1,1)
linpred = intercept + xtest*beta
prob = exp(linpred)/(1 + exp(linpred))
runis = runif(l,0,1)
# mimicks the regular distribution of MP
regular <- sample(4, length(d$MP), prob = c(4/399, 101/399, 190/399, (104/399)), replace = T)
# now imagine a real relationship, in which weather puts a floor on mood/productivity (no 1s or 2s)
biased <- sample(4, length(d$MP), prob = c(0, 0, 190/399, 104/399), replace = T)
ytest = ifelse(runis < prob,biased,regular)
# update with our 2 correlated variables
d$MP <- ytest
d$X <- xtest
# and finally run the analysis in question
slmodel <- try(step(multinom(MP ~ ., data=d, maxit=200, trace=FALSE),trace=0))
if("try-error" %in% class(slmodel)) return(c(NA,NA)) else return(c(beta, !is.na(match("X", slmodel$coefnames))))
}
# the easy way?
bs <- boot(data=weather, statistic=found, R=10); bs$t


----

bs

...

boot.ci(bs)
...
plot...
~~~

# Conclusion

An attack on the data turned up nothing in several ways; the only model that seemed to improve on random guessing turned out to look mostly like a fluke.
