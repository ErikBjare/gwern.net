---
description: Mathematical mistake or error-rates limit our understanding of rare risks
tags: philosophy, transhumanism, statistics
created: 20 Jul 2012
status: draft
belief: likely
...

> See also ["Confidence levels inside and outside an argument"](http://lesswrong.com/lw/3be/confidence_levels_inside_and_outside_an_argument/)

Mathematical error has been rarely examined except as a possibility and motivating reason for research into formal methods; [Gaifman 2004](http://www.columbia.edu/~hg17/synthese-paper-as-published.pdf "Reasoning With Limited Resources And Assigning Probabilities to Arithmetical Statements") claims

> An agent might even have beliefs that logically contradict each other. Mersenne believed that 2^67^-1 is a prime number, which was proved false in 1903, cf. Bell (1951). [The factorization, discovered by Cole, is: 193,707,721 × 761,838,257,287.]...Now, there is no shortage of deductive errors and of false mathematical beliefs. Mersenne's is one of the most known in a rich history of mathematical errors, involving very prominent figures (cf. De Millo et al. 1979, 269-270). The explosion in the number of mathematical publications and research reports has been accompanied by a similar explosion in erroneous claims; on the whole, errors are noted by small groups of experts in the area, and many go unheeded. There is nothing philosophically interesting that can be said about such failures.[^Gaifman-references]

[^Gaifman-references]: Citations:

    > - Bell, E.T.: 1951, "The Queen of Mathematics", reprinted in J. R. Newman (ed.), _The World of Mathematics_, Simon and Schuster (1956)
    > - De Milo, R. Lipton, and A. Perlis: 1979, "Social Processes and Proofs of Theorems and Programs", _Communication of the ACM_, Vol. 22, No. 5. Reprinted in T. Tymozcko (ed.), _New Directions in the Philosophy of Mathematics_, Princeton University Press (1998). Page numbers refer to the book

# Untrustworthy proofs

In some respects, there is nothing to be said; in other respects, there is much to be said. ["Probing the Improbable: Methodological Challenges for Risks with Low Probabilities and High Stakes"](http://arxiv.org/abs/0810.5515) discusses a basic issue with [existential threats](!Wikipedia): any useful discussion will be rigorous, hopefully with physics and math proofs; but proofs themselves are empirically unreliable. Given that proofs are the most reliable form of epistemology humans know, this sets a basic upper bound on how much confidence we can put on *any* belief. (There are other rare risks, from mental diseases[^mind] to how to deal with contradictions[^Wittgenstein], but we'll look at mathematical error.)

> "When you have eliminated the impossible, whatever remains is often more improbable than your having made a mistake in one of your impossibility proofs." --[Steven Kaas](http://lesswrong.com/lw/3m/rationalist_fiction/2p6#body_t1_2p6)

# Error distribution

This upper bound on our certainty may force us to disregard certain rare risks because the effect of error on our estimates of existential risks is *asymmetric*: an error will usually reduce the risk, not increase it. The errors are not distributed in any kind of symmetrical around a mean: an existential risk is, by definition, bumping up against the upper bound on possible damage. If we were trying to estimate, say, average human height, and errors were distributed like a bell curve, then we could ignore them. But if we are calculating the risk of a super-asteroid impact which will kill all of humanity, an error which means the super-asteroid will actually kill humanity twice over is irrelevant because it's the same thing (we can't die twice); however, the mirror error - the super-asteroid actually killing half of humanity - matters a great deal!

[^mind]: There are various delusions (eg. [Cotard delusion](!Wikipedia)), [false memory syndrome](!Wikipedia)s, compulsive lying ([pseudologia fantastica](!Wikipedia)), disorders provoking [confabulation](!Wikipedia "Confabulation#Abnormal Psychopathology") such as the general symptom of [anosognosia](!Wikipedia); in a dramatic example of how the mind is what the brain does, some anosognosia can be temporarily cured by squirting cold water in an ear; from ["The Apologist and the Revolutionary"](http://lesswrong.com/lw/20/the_apologist_and_the_revolutionary/):

    > Take the example of the woman discussed in Lishman's _Organic Psychiatry_. After a right-hemisphere stroke, she lost movement in her left arm but continuously denied it. When the doctor asked her to move her arm, and she observed it not moving, she claimed that it wasn't actually her arm, it was her daughter's. Why was her daughter's arm attached to her shoulder? The patient claimed her daughter had been there in the bed with her all week. Why was her wedding ring on her daughter's hand? The patient said her daughter had borrowed it. Where was the patient's arm? The patient "turned her head and searched in a bemused way over her left shoulder"...In any case, a patient who has been denying paralysis for weeks or months will, upon having cold water placed in the ear, admit to paralysis, admit to having been paralyzed the past few weeks or months, and express bewilderment at having ever denied such an obvious fact. And then the effect wears off, and the patient not only denies the paralysis but denies ever having admitted to it.
[^Wittgenstein]: Most/all math results require their system to be consistent; but this is one particular philosophical view. [Ludwig Wittgenstein](!Wikipedia), in _[Remarks on the Foundations of Mathematics](!Wikipedia)_:

    > If a contradiction were now actually found in arithmetic - that would only prove that an arithmetic with *such* a contradiction in it could render very good service; and it would be better for us to modify our concept of the certainty required, than to say it would really not yet have been a proper arithmetic.

    [Saul Kripke](!Wikipedia), [reconstructing a Wittgensteinian skeptical argument](!Wikipedia "Wittgenstein on Rules and Private Language"), points out one way to react to such issues:

    > A *skeptical* solution of a philosophical problem begins... by conceding that the skeptic's negative assertions are unanswerable. Nevertheless our ordinary practice or belief is justified because—contrary appearances notwithstanding—it need not require the justification the sceptic has shown to be untenable. And much of the value of the sceptical argument consists precisely in the fact that he has shown that an ordinary practice, if it is to be defended at all, cannot be defended in a certain way.

How big is this upper bound? Mathematicians have *often* made errors in proofs. But we can divide errors into 2 basic cases corresponding to [type I and type II errors](!Wikipedia):

1. Mistakes where the theorem is still true, but the proof was incorrect (type I)
2. Mistakes where the theorem was *false*, and the proof was also necessarily incorrect (type II)

Before someone comes up with a final answer, a mathematician may have [many levels of intuition](http://mathdl.maa.org/images/upload_library/22/Polya/07468342.di020715.02p0066x.pdf "'What Do I Know? A Study of Mathematical Self-Awareness', Davis 1983") in formulating & working on the problem, but we'll consider the final end-product where the mathematician feels satisfied that he has solved it. Case 1 is perhaps the most common case, with innumerable examples; this is sometimes due to mistakes in the proof that anyone would accept is a mistake, but many of these cases are due to changing standards of proof. For example, when David Hilbert discovered errors in Euclid's proofs which no one noticed before, the theorems were still true, and the gaps more due to Hilbert being a modern mathematician thinking in terms of formal systems (which of course Euclid did not think in). Similarly, early calculus used 'infinitesimals' which were sometimes treated as being 0 and sometimes treated as an indefinitely small non-zero number; this was incoherent and strictly speaking, practically *all* of the calculus results were wrong because they relied on an incoherent concept - but of course the results were some of the greatest mathematical work ever conducted[^Neumann] and when later mathematicians put calculus on a more rigorous footing, they immediately re-derived those results (sometimes with important qualifications). Other cases are more straightforward, with mathematicians publishing multiple proofs/patches or covertly correcting papers[^Nathanson].

[^Colton2]: Colton 2007: "For example, Heawood discovered a flaw in Kempe's 1879 proof of the [four colour theorem](!Wikipedia "Four colour theorem#Early proof attempts"),2 which had been accepted for 11 years." It would ultimately be proved with a computer in 1976 - maybe.
[^Colton1]: ["Computational Discovery in Pure Mathematics"](http://www.doc.ic.ac.uk/~sgc/papers/colton_cdck07.pdf), Simon Colton 2007:

    > A more recent example was the discovery that Andrew Wiles' original proof of Fermat's Last Theorem was flawed (but not, as it turned out, fatally flawed, as Wiles managed to fix the problem (Singh, 1997))...More recently, Larry Wos has been using Otter to find smaller proofs of theorems than the current ones. To this end, he uses Otter to find more succinct methods than those originally proposed. This often results in detecting double negations and removing unnecessary lemmas, some of which were thought to be indispensable. (Wos, 1996) presents a methodology using a strategy known as resonance to search for elegant proofs with Otter. He gives examples from mathematics and logic, and also argues that this work also implications for other fields such as circuit design.
    >
    > (Fleuriot & Paulson, 1998) have studied the geometric proofs in Newton's _Principia_ and investigated ways to prove them automatically with the Isabelle interactive theorem prover (Paulson, 1994). To do this, they formalized the _Principia_ in both Euclidean geometry and non-standard analysis. While working through one of the key results (proposition 11 of book 1, the Kepler problem) they discovered an anomaly in the reasoning. Newton was appealing to a cross-multiplication result which wasn't true for infinitesimals or infinite numbers. Isabelle could therefore not prove the result, but Fleuriot managed to derive an alternative proof of the theorem that the system found acceptable.
[^Neumann]: John von Neumann, ["The Mathematician"](http://www.math.ubc.ca/~fsl/von%20Neumann.pdf) 1947:

    > That Euclid's axiomatization does at some minor points not meet the modern requirements of absolute axiomatic rigour is of lesser importance in this respect...The first formulations of the calculus were not even mathematically rigorous. An inexact, semi-physical formulation was the only one available for over a hundred and fifty years after Newton! And yet, some of the most important advances of analysis took place during this period, against this inexact, mathematically inadequate background! Some of the leading mathematical spirits of the period were clearly not rigorous, like Euler; but others, in the main, were, like Gauss or Jacobi. The development was as confused and ambiguous as can be, and its relation to empiricism was certainly not according to our present (or Euclid's) ideas of abstraction and rigour. Yet no mathematician would want to exclude it from the fold — that period produced mathematics as first-class as ever existed! And even after the reign of rigour was essentially re-established with Cauchy, a very peculiar relapse into semi-physical methods took place with Riemann.
[^Nathanson]: ["Desperately seeking mathematical proof"](http://arxiv.org/pdf/0905.3590) ([arXiv](http://arxiv.org/abs/0905.3590)), Melvyn B. Nathanson 2009:

    > The history of mathematics is full of philosophically and ethically troubling reports about bad proofs of theorems. For example, the [fundamental theorem of algebra](!Wikipedia) states that every polynomial of degree _n_ with complex coefficients has exactly _n_ complex roots. D'Alembert published a proof in 1746, and the theorem became known "D'Alembert's theorem", but the proof was wrong. Gauss published his first proof of the fundamental theorem in 1799, but this, too, had gaps. Gauss's subsequent proofs, in 1816 and 1849, were OK. It seems to have been hard to determine if a proof of the fundamental theorem of algebra was correct. Why?
    >
    > [Poincaré](!Wikipedia "Henri Poincare") was awarded a prize from King Oscar II of Sweden and Norway for a paper on the [three-body problem](!Wikipedia), and his paper was published in _Acta Mathematica_ in 1890. But the published paper was not the prize-winning paper. The paper that won the prize contained serious mistakes, and Poincare and other mathematicians, most importantly, [Mittag-Leffler](!Wikipedia "Gösta Mittag-Leffler"), engaged in a conspiracy to suppress the truth and to replace the erroneous paper with an extensively altered and corrected one.

    (["The Solution of the _n_-body Problem"](http://www.math.uvic.ca/faculty/diacu/diacuNbody.pdf "Diacu 1996") gives a summary of the eventual exact solution to the three-body problem.)

## Type I > Type II?

Case 2 is disturbing, since it is a case in which we wind up with false beliefs and also false beliefs about our beliefs (we no longer know that we don't know). Case 2 could lead to extinction.

The prevalence of case 1 might lead us to be very pessimistic; case 1, case 2, what's the difference? We have demonstrated a large error rate in mathematics (and physics is probably even worse off). Except, errors do not seem to be evenly & randomly distributed between case 1 and case 2. There seem to be far more case 1s than case 2s, as already mentioned in the early calculus example: far more than 50% of the early calculus results were correct when checked more rigorously. [Gian-Carlo Rota](!Wikipedia) gives us an example with Hilbert:

> Once more let me begin with Hilbert. When the Germans were planning to publish Hilbert's collected papers and to present him with a set on the occasion of one of his later birthdays, they realized that they could not publish the papers in their original versions because they were full of errors, some of them quite serious. Thereupon they hired a young unemployed mathematician, Olga Taussky-Todd, to go over Hilbert's papers and correct all mistakes. Olga labored for three years; it turned out that all mistakes could be corrected without any major changes in the statement of the theorems. There was one exception, a paper Hilbert wrote in his old age, which could not be fixed; it was a purported proof of the continuum hypothesis, you will find it in a volume of the _Mathematische Annalen_ of the early thirties. At last, on Hilbert's birthday, a freshly printed set of Hilbert's collected papers was presented to the Geheimrat. Hilbert leafed through them carefully and did not notice anything.^[["Ten Lessons I wish I had been Taught"](http://alumni.media.mit.edu/~cahn/life/gian-carlo-rota-10-lessons.html#mistakes), Gian-Carlo Rota 1996]

So only one of those papers was irreparable, while all the others were correct and fixable? Rota himself experienced this:

> Now let us shift to the other end of the spectrum, and allow me to relate another personal anecdote. In the summer of 1979, while attending a philosophy meeting in Pittsburgh, I was struck with a case of detached retinas. Thanks to Joni's prompt intervention, I managed to be operated on in the nick of time and my eyesight was saved. On the morning after the operation, while I was lying on a hospital bed with my eyes bandaged, Joni dropped in to visit. Since I was to remain in that Pittsburgh hospital for at least a week, we decided to write a paper. Joni fished a manuscript out of my suitcase, and I mentioned to her that the text had a few mistakes which she could help me fix. There followed twenty minutes of silence while she went through the draft. "*Why, it is all wrong!*" she finally remarked in her youthful voice. She was right. Every statement in the manuscript had something wrong. Nevertheless, after laboring for a while, she managed to correct every mistake, and the paper was eventually published.
>
> There are two kinds of mistakes. There are fatal mistakes that destroy a theory; but there are also contingent ones, which are useful in testing the stability of a theory.

A mathematician of my acquaintance referred me to [pg118](http://www.maths.gla.ac.uk/~tl/images/Jech_p118.png) of _The Axiom of Choice_, Jech 2008; he had found the sustained effect of the 5 footnotes humorous:

> 1. The result of Problem 11 contradicts the results announced by Levy [1963b]. Unfortunately, the construction presented there cannot be completed.
> 2. The transfer to ZF was also claimed by Marek [1966] but the outlined method appears to be unsatisfactory and has not been published.
> 3. A contradicting result was announced and later withdrawn by Truss [1970].
> 4. The example in Problem 22 is a counterexample to another condition of Mostowski, who conjectured its sufficiency and singled out this example as a test case.
> 5. The independence result contradicts the claim of Felgner [1969] that the Cofinality Principle implies the Axiom of Choice. An error has been found by Morris (see Felgner's corrections to [1969]).

This look into the proverbial sausage factory should not come as a surprise to anyone taking an Outside View: why wouldn't we expect any area of intellectual endeavour to have error rates within a few orders of magnitude as any other area? How absurd to think that the rate might be ~0%; but it's also a little questionable to be as optimistic as [Anders Sandberg's mathematician friend](http://www.aleph.se/andart/archives/2012/09/flaws_in_the_perfection.html "Flaws in the Perfection"): "he responded that he thought a far smaller number of papers in math were this [1%] flawed."

# Heuristics

Other times, the correct result is known and proven, but many are unaware of the answers[^enduring]. The famous [Millennium Problems](!Wikipedia) - those that have been solved, anyway - have a long history of failed proofs (Fermat surely did not prove [Fermat's Last Theorem](!Wikipedia) and neither did [Lindemann](!Wikipedia "Ferdinand von Lindemann")[^mathworld]). What explains this? The guiding factor that keeps popping up when mathematicians make leaps seems to go under the name of 'elegance' or [mathematical beauty](!Wikipedia), which widely considered important[^Goldstein][^Erdos][^Sinclair1]. This imbalance suggests that mathematicians are quite correct when they say proofs are not the heart of mathematics and that they possess insight into math, a 6th sense for mathematical truth, a nose for aesthetic beauty which correlates with veracity: they disproportionately go after theorems rather than their negations.

Why this is so I do not know.

Outright Platonism like Godel apparently believed in seems unlikely - mathematical expertise resembles a complex skill like chess-playing more than it does a sensory modality like vision. Possibly they have well-developed heuristics and short-cuts and they focus on the subsets of results on which those heuristics work well (the drunk searching under the spotlight), or perhaps they *do* run full rigorous proofs but are doing so subconsciously and merely express themselves ineptly consciously with omissions and erroneous formulations 'left as an exercise for the reader'[^Sinclair2].

[^Sinclair1]: From ["Aesthetics as a Liberating Force in Mathematics Education?"](/docs/2009-sinclair.pdf), by [Nathalie Sinclair](http://www.mth.msu.edu/~nathsinc/) (reprinted in _The Best Writing on Mathematics 2010_, ed. Mircea Pitici); pg208:

    > There is a long tradition in mathematics of describing proofs and theorems in aesthetic terms, often using words such as 'elegance' and 'depth'. Further, mathematicians have also argued that their subject is more akin to an art than it is to a science (see [Hardy, 1967](!Wikipedia "A Mathematician's Apology"); Littlewood, 1986; Sullivan 1925/1956), and, like the arts, ascribe to mathematics aesthetic goals. For example, the mathematician W. Krull ([1930/1987](http://www.springerlink.com/content/3203036jq8v23484/)) writes: "the primary goals of the mathematician are aesthetic, and not epistemological" (p. 49). This statement seems contradictory with the oft-cited concern of mathematics with finding or discovering truths, but it emphasises the fact that the mathematician's interest is in expressing truth, and in doing so in clever, simple, succinct ways.
    >
    > While Krull focuses on mathematical expression, the mathematician H. Poincare ([1908/1966](http://www.webcitation.org/5wo3XAgp5)) concerns himself with the psychology of mathematical invention, but he too underlines the aesthetic dimension of mathematics, not the logical. In Poincare's theory, a large part of a mathematician's work is done at the subconscious level, where an aesthetic sensibility is responsible for alerting the mathematicians to the most fruitful and interesting of ideas. Other mathematicians have spoken of this special sensibility as well and also in terms of the way it guides mathematicians to choose certain problems. This choice is essential in mathematic given that there exists no external reality against which mathematicians can decide which problems or which branches of mathematics are important (see [von Neumann, 1947](http://www.math.ubc.ca/~fsl/von%20Neumann.pdf)): the choice involves human values and preference - and, indeed, these change over time, as exemplified by the dismissal of geometry by some prominent mathematicians in the early 20th century (see [Whiteley, 1999](http://www.math.yorku.ca/Who/Faculty/Whiteley/cmesg.pdf)).
    >
    > - Littlewood, 1986: "The mathematician's art of work"; in B. Bollobas (ed.), _Littlewood's miscellany_, Cambridge University press
    > - Sullivan 1925/1956: "Mathematics as an art"; in J. Newman (ed.), _The world of mathematics_, vol 3 (p 2015-2021)
[^Sinclair2]: From pg 211-212, Sinclair 2009:

    > The survey of mathematicians conducted by [Wells (1990)](/docs/1990-wells.pdf) provides a more empirically-based challenge to the intrinsic view of the mathematical aesthetic. Wells obtained responses from over 80 mathematicians, who were asked to identify the most beautiful theorem from a given set of 24 theorems. (These theorems were chosen because they were 'famous', in the sense that Wells judged them to be well-known by most mathematicians, and of interest to the discipline in general, rather than to a particular subfield.) Wells finds that the mathematicians varied widely in their judgments. More interestingly, in explaining their choices, the mathematicians revealed a wide range of personal responses affecting their aesthetic responses to the theorems. Wells effectively puts to rest the belief that mathematicians have some kind of secret agreement on what counts as beautiful in mathematics....Burton's (2004) work focuses on the practices of mathematicians and their understanding of those practices. Based on extensive interviews with a wide range of mathematicians...She points out that mathematicians range on a continuum from unimportant to crucial in terms of their positionings on the role of the aesthetic, with only 3 of the 43 mathematicians dismissing its importance. For example, one said "Beauty doesn't matter. I have never seen a beautiful mathematical paper in my life" (p. 65). Another mathematician was initially dismissive about mathematical beauty but later, when speaking about the review process, said: "If it was a very elegant way of doing things, I would be inclined to forgive a lot of faults" (p. 65).
    >
    > - Burton, Leone (2004): _Mathematicians as enquirers: Learning about learning mathematics_; Dordrecht: Kluwer Academic Publishers
[^Goldstein]: To take a random example (which could be multiplied indefinitely); from [Gödel and the Nature of Mathematical Truth: A Talk with Rebecca Goldstein](http://www.edge.org/3rd_culture/goldstein05/goldstein05_index.html) (6.8.2005):

    > Einstein told the philosopher of science [Hans Reichenbach](!Wikipedia) that he'd known even before the solar eclipse of 1918 supported his general theory of relativity that the theory must be true because it was so beautiful. And [Hermann Weyl](!Wikipedia), who worked on both relativity theory and quantum mechanics, said "My work always tried to unite the true with the beautiful, but when I had to choose one or the other, I usually chose the beautiful."...Mathematics seems to be the one place where you don't have to choose, where truth and beauty are always united. One of my all-time favorite books is _[A Mathematician's Apology](!Wikipedia)_. [G.H. Hardy](!Wikipedia) tries to demonstrate to a general audience that mathematics is intimately about beauty. He gives as examples two proofs, one showing that the square root of 2 is irrational, the other showing that there's no largest prime number. Simple, easily graspable proofs, that stir the soul with wonder.
[^Erdos]: Nathanson 2009 claims the opposite:

    > Many mathematicians have the opposite opinion; they do not or cannot distinguish the beauty or importance of a theorem from its proof. A theorem that is first published with a long and difficult proof is highly regarded. Someone who, preferably many years later, finds a short proof is "brilliant." But if the short proof had been obtained in the beginning, the theorem might have been disparaged as an "easy result." Erdős was a genius at finding brilliantly simple proofs of deep results, but, until recently, much of his work was ignored by the mathematical establishment.
[^enduring]: An example of this would be ["An Enduring Error"](http://digital.lib.washington.edu/dspace/bitstream/handle/1773/4592/An_enduring_error.pdf?sequence=1), Branko Grünbaum:

    > Mathematical truths are immutable, but mathematicians do make errors, especially when carrying out non-trivial enumerations. Some of the errors are "innocent" -- plain mistakes that get corrected as soon as an independent enumeration is carried out. For example, Daublebsky [14] in 1895 found that there are precisely 228 types of configurations (123), that is, collections of 12 lines and 12 points, each incident with three of the others. In fact, as found by Gropp [19] in 1990, the correct number is 229. Another example is provided by the enumeration of the uniform tilings of the 3-dimensional space by Andreini [1] in 1905; he claimed that there are precisely 25 types. However, as shown [20] in 1994, the correct number is 28. Andreini listed some tilings that should not have been included, and missed several others -- but again, these are simple errors easily corrected....It is surprising how errors of this type escape detection for a long time, even though there is frequent mention of the results. One example is provided by the enumeration of 4-dimensional simple polytopes with 8 facets, by Brückner [7] in 1909. He replaces this enumeration by that of 3-dimensional "diagrams" that he interpreted as Schlegel diagrams of convex 4-polytopes, and claimed that the enumeration of these objects is equivalent to that of the polytopes. However, aside from several "innocent" mistakes in his enumeration, there is a fundamental error: While to all 4-polytopes correspond 3-dimensional diagrams, there is no reason to assume that every diagram arises from a polytope. At the time of Brückner's paper, even the corresponding fact about 3-polyhedra and 2-dimensional diagrams has not yet been established -- this followed only from Steinitz's characterization of complexes that determine convex polyhedra [45], [46]. In fact, in the case considered by Brückner, the assumption is not only unjustified, but actually wrong: One of Brückner's polytopes does not exist, see [25].
    >
    > ...Polyhedra have been studied since antiquity. It is, therefore, rather surprising that even concerning some of the polyhedra known since that time there is a lot of confusion, regarding both terminology and essence. But even more unexpected is the fact that many expositions of this topic commit serious mathematical and logical errors. Moreover, this happened not once or twice, but many times over the centuries, and continues to this day in many printed and electronic publications; the most recent case is in the second issue for 2008 of this journal....With our understandings and exclusions, there are fourteen convex polyhedra that satisfy the local criterion and should be called "Archimedean", but only thirteen that satisfy the global criterion and are appropriately called "uniform" (or "semiregular"). Representatives of the thirteen uniform convex polyhedra are shown in the sources mentioned above, while the fourteenth polyhedron is illustrated in Figure 1. It satisfies the local criterion but not the global one, and therefore is - in our terminology - Archimedean but not uniform. The history of the realization that the local criterion leads to fourteen polyhedra will be discussed in the next section; it is remarkable that this development occurred only in the 20th century. This implies that prior to the twentieth century all enumerations of the polyhedra satisfying the local criterion were mistaken. Unfortunately, many later enumerations make the same error.
[^mathworld]: From _[MathWorld](!Wikipedia)_, ["Fermat's Last Theorem"](http://mathworld.wolfram.com/FermatsLastTheorem.html):

    > "Much additional progress was made over the next 150 years, but no completely general result had been obtained. Buoyed by false confidence after his proof that pi is transcendental, the mathematician Lindemann proceeded to publish several proofs of Fermat's Last Theorem, all of them invalid (Bell 1937, pp. 464-465). A prize of 100000 German marks, known as the Wolfskehl Prize, was also offered for the first valid proof (Ball and Coxeter 1987, p. 72; Barner 1997; Hoffman 1998, pp. 193-194 and 199).
    >
    > A recent false alarm for a general proof was raised by Y. Miyaoka (Cipra 1988) whose proof, however, turned out to be flawed. Other attempted proofs among both professional and amateur mathematicians are discussed by vos Savant (1993), although vos Savant erroneously claims that work on the problem by Wiles (discussed below) is invalid."

We could try to justify the heuristic paradigm by appealing to as-yet poorly understood aspects of the brain, like our visual cortex: argue that what is going on is that mathematicians are subconsciously doing tremendous amounts of computation (like we do tremendous amounts of computation in a thought as ordinary as recognizing a face), which they are unable to bring up explicitly. So after prolonged introspection and some comparatively simple explicit symbol manipulation or thought, they *feel* that a conjecture is true and this is due to a summary of said massive computations. (Perhaps they are checking many instances? Perhaps they are [white-box testing](!Wikipedia) and looking for boundaries? Could there be some sort of *logical probability* where going down possible proof-paths yield probabilistic information about the final target theorem, maybe in some sort of [Monte Carlo tree search](!Wikipedia "Monte Carlo method") of proof-trees?)

Heuristics, however, do not generalize, and fail outside their particular domain. Are we fortunate enough that the domain mathematicians work in is - deliberately or accidentally - just that domain in which their heuristics/intuition succeeds? [Sandberg](http://www.aleph.se/andart/archives/2012/09/flaws_in_the_perfection.html) suggests not:

> Unfortunately I suspect that the connoisseurship of mathematicians for truth might be local to their domain. I have discussed with friends about how "brittle" different mathematical domains are, and our consensus is that there are definitely differences between logic, geometry and calculus. Philosophers also seem to have a good nose for what works or doesn't in their domain, but it doesn't seem to carry over to other domains. Now moving outside to applied domains things get even trickier. There doesn't seem to be the same "nose for truth" in risk assessment, perhaps because it is an interdisciplinary, messy domain. The cognitive abilities that help detect correct decisions are likely local to particular domains, trained through experience and maybe talent (i.e. some conformity between neural pathways and deep properties of the domain). The only thing that remains is general-purpose intelligence, and that has its own limitations.

## Type I vs Type II

So we might forgive case 1 errors entirely: if a community of mathematicians take an 'incorrect' proof about a particular existential risk and ratify it (either by verifying the proof subconsciously or seeing what their heuristics say), it not being written out because it would be tedious too[^Weiner], then we may be more confident in it[^Groupthink] than lumping the two error rates together. Case 2 errors are the problem, and they can sometimes be systematic. Most dramatically, when an entire group of papers with all their results turn out to be wrong since they made a since-disproved assumption:

> In the 1970s and 1980s, mathematicians discovered that framed manifolds with Arf-[Kervaire invariant](!Wikipedia) equal to 1 — oddball manifolds not surgically related to a sphere — do in fact exist in the first five dimensions on the list: 2, 6, 14, 30 and 62.  A clear pattern seemed to be established, and many mathematicians felt confident that this pattern would continue in higher dimensions...Researchers developed what Ravenel calls an entire "cosmology" of conjectures based on the assumption that manifolds with Arf-Kervaire invariant equal to 1 exist in all dimensions of the form $2^n - 2$. Many called the notion that these manifolds might not exist the "Doomsday Hypothesis," as it would wipe out a large body of research. Earlier this year, Victor Snaith of the University of Sheffield in England published a book about this research, warning in the preface, "...this might turn out to be a book about things which do not exist."
>
> Just weeks after Snaith's book appeared, Hopkins announced on April 21 that Snaith's worst fears were justified: that Hopkins, Hill and Ravenel had proved that no manifolds of Arf-Kervaire invariant equal to 1 exist in dimensions 254 and higher. Dimension 126, the only one not covered by their analysis, remains a mystery. The new finding is convincing, even though it overturns many mathematicians' expectations, Hovey said.^[["Mathematicians solve 45-year-old Kervaire invariant puzzle"](http://simonsfoundation.org/mathematics-physical-sciences/featured-articles/-/asset_publisher/bo1E/content/mathematicians-solve-45-year-old-kervaire-invariant-puzzle), Erica Klarreich 2009]

[^Weiner]: The missing steps may be quite difficult to fully prove, though; Nathanson 2009:

    > There is a lovely but probably apocryphal anecdote about [Norbert Weiner](!Wikipedia). Teaching a class at MIT, he wrote something on the blackboard and said it was 'obvious.' One student had the temerity to ask for a proof. Weiner started pacing back and forth, staring at what he had written on the board and saying nothing. Finally, he left the room, walked to his office, closed the door, and worked. After a long absence he returned to the classroom. 'It *is* obvious', he told the class, and continued his lecture.
[^Groupthink]: What conditions count as full scrutiny by the math community may not be too clear; Nathanson 2009 trenchantly mocks math talks:

    > Social pressure often hides mistakes in proofs. In a seminar lecture, for example, when a mathematician is proving a theorem, it is technically possible to interrupt the speaker in order to ask for more explanation of the argument. Sometimes the details will be forthcoming. Other times the response will be that it's "obvious" or "clear" or "follows easily from previous results." Occasionally speakers respond to a question from the audience with a look that conveys the message that the questioner is an idiot. That's why most mathematicians sit quietly through seminars, understanding very little after the introductory remarks, and applauding politely at the end of a mostly wasted hour.

The [parallel postulate](!Wikipedia "Parallel postulate#History") is another fascinating example of mathematical error of the second kind; its history is replete with false proofs even by greats like [Lagrange](!Wikipedia "Joseph Louis Lagrange") (on what strike the modern reader as bizarre grounds)[^lagrange], self-deception, and misunderstandings - [Giovanni Girolamo Saccheri](!Wikipedia) developed a non-Euclidean geometry flawlessly but concluded it was flawed:

> The second possibility turned out to be harder to refute. In fact he was unable to derive a logical contradiction and instead derived many non-intuitive results; for example that triangles have a maximum finite area and that there is an absolute unit of length. He finally concluded that: "the hypothesis of the acute angle is absolutely false; because it is repugnant to the nature of straight lines". Today, his results are theorems of [hyperbolic geometry](!Wikipedia).

[^lagrange]: ["Why Did Lagrange 'Prove' the Parallel Postulate?"](http://mathdl.maa.org/images/upload_library/22/Ford/Grabiner3-18.pdf), Judith V. Grabiner 2009:

    > "It is true that Lagrange never did publish it, so he must have realized there was something wrong. In another version of the story, told by [Jean-Baptiste Biot](!Wikipedia), who claims to have been there (though the minutes do not list his name), everybody there could see that something was wrong, so Lagrange's talk was followed by a moment of complete silence [2, p. 84]. Still, Lagrange kept the manuscript with his papers for posterity to read."

    Why work on it at all?

    > "The historical focus on the fifth postulate came because it felt more like the kind of thing that gets proved. It is not self-evident, it requires a diagram even to explain, so it might have seemed more as though it should be a theorem. In any case, there is a tradition of attempted proofs throughout the Greek and then Islamic and then eighteenth-century mathematical worlds. Lagrange follows many eighteenth-century mathematicians in seeing the lack of a proof of the fifth postulate as a serious defect in [Euclid's _Elements_](!Wikipedia "Euclid's Elements"). But Lagrange's criticism of the postulate in his manuscript is unusual. He said that the assumptions of geometry should be demonstrable "just by the [principle of contradiction](!Wikipedia)"—the same way, he said, that we know the axiom that the whole is greater than the part [32, p. 30R]. The theory of parallels rests on something that is not self-evident, he believed, and he wanted to do something about this."

    What was the strange and alien to the modern mind approach that Lagrange used?

    > Recall that Lagrange said in this manuscript that axioms should follow from the principle of contradiction. But, he added, besides the principle of contradiction, "There is another principle equally self-evident," and that is Leibniz's [principle of sufficient reason](!Wikipedia). That is: nothing is true "unless there is a sufficient reason why it should be so *and not otherwise*" [42, p. 31; italics added]. This, said Lagrange, gives as solid a basis for mathematical proof as does the principle of contradiction [32, p. 30V]. But is it legitimate to use the principle of sufficient reason in mathematics? Lagrange said that we are justified in doing this, because it has already been done. For example, Archimedes [used it](!Wikipedia "/Mechanical advantage#Law of the lever") to establish that equal weights at equal distances from the fulcrum of a lever balance. Lagrange added that we also use it to show that three equal forces acting on the same point along lines separated by a third of the circumference of a circle are in equilibrium [32, pp. 31R-31V]...The modern reader may object that Lagrange's symmetry arguments are, like the uniqueness of parallels, equivalent to Euclid's postulate. But the logical correctness, or lack thereof, of Lagrange's proof is not the point. (In this manuscript, by the way, Lagrange went on to give an analogous proof—also by the principle of sufficient reason—that between two points there is just one straight line, because if there were a second straight line on one side of the first, we could then draw a third straight line on the other side, and so on [32, pp. 34R-34V]. Lagrange, then, clearly liked this sort of argument.)
    >
    > ...Why did philosophers conclude that space had to be infinite, homogeneous, and the same in all directions? Effectively, because of the principle of sufficient reason. For instance, [Giordano Bruno](!Wikipedia) in 1600 argued that the universe must be infinite because there is no reason to stop at any point; the existence of an infinity of worlds is no less reasonable than the existence of a finite number of them. Descartes used similar reasoning in his _Principles of Philosophy_: "We recognize that this world. . . has no limits in its extension. . . . Wherever we imagine such limits, we . . . imagine beyond them some indefinitely extended space" [28, p. 104]. Similar arguments were used by other seventeenth-century authors, including Newton. Descartes identified space and the extension of matter, so geometry was, for him, about real physical space. But geometric space, for Descartes, had to be Euclidean...Descartes, some 50 years before Newton published his first law of motion, was a co-discoverer of what we call linear inertia: that in the absence of external influences a moving body goes in a straight line at a constant speed. Descartes called this the first law of nature, and for him, this law follows from what we now recognize as the principle of sufficient reason. Descartes said, "Nor is there any reason to think that, if [a part of matter] moves. . . and is not impeded by anything, it should ever by itself cease to move with the same force" [30, p. 75]....Leibniz, by contrast, did not believe in absolute space. He not only said that spatial relations were just the relations between bodies, he used the principle of sufficient reason to show this. If there were absolute space, there would have to be a reason to explain why two objects would be related in one way if East is in one direction and West in the opposite direction, and related in another way if East and West were reversed [24, p. 147]. Surely, said Leibniz, the relation between two objects is just one thing! But Leibniz did use arguments about symmetry and sufficient reason—sufficient reason was his principle, after all. Thus, although Descartes and Leibniz did not believe in empty absolute space and Newton did, they all agreed that what I am calling the Euclidean properties of space are essential to physics.
    >
    > ...In his 1748 essay ["Reflections on Space and Time"](http://eulerarchive.maa.org/docs/translations/E149tr.pdf), Euler argued that space must be real; it cannot be just the relations between bodies as the Leibnizians claim [10]. This is because of the principles of mechanics—that is, Newton's first and second laws. These laws are beyond doubt, because of the "marvelous" agreement they have with the observed motions of bodies. The inertia of a single body, Euler said, cannot possibly depend on the behavior of other bodies. The conservation of uniform motion in the same direction makes sense, he said, only if measured with respect to immovable space, not to various other bodies. And space is not in our minds, said Euler; how can physics—real physics—depend on something in our minds?...in his _[Critique of Pure Reason](!Wikipedia)_ of 1781, Kant placed space in the mind nonetheless. We order our perceptions in space, but space itself is in the mind, an intuition of the intellect. Nevertheless, Kant's space turned out to be Euclidean too. Kant argued that we need the intuition of space to prove theorems in geometry. This is because it is in space that we make the constructions necessary to prove theorems. And what theorem did Kant use as an example? The sum of the angles of a triangle is equal to two right angles, a result whose proof requires the truth of the parallel postulate [26, "Of space," p. 423]....Lagrange himself is supposed to have said that spherical trigonometry does not need Euclid's parallel postulate [4, pp. 52-53]. But the surface of a sphere, in the eighteenth-century view, is not non-Euclidean; it exists in 3-dimensional Euclidean space [20, p. 71]. The example of the sphere helps us see that the eighteenth-century discussion of the parallel postulate's relationship to the other postulates is not really about what is logically possible, but about what is true of real space.

    The final step:

    > [Johann Heinrich Lambert](!Wikipedia) was one of the mathematicians who worked on the problem of Postulate 5. Lambert explicitly recognized that he had not been able to prove it, and considered that it might always have to remain a postulate. He even briefly suggested a possible geometry on a sphere with an imaginary radius. But Lambert also observed that the parallel postulate is related to the law of the lever [20, p. 75]. He said that a lever with weightless arms and with equal weights at equal distances is balanced by a force in the opposite direction at the center equal to the sum of the weights, and that all these forces are parallel. So either we are using the parallel postulate, or perhaps, Lambert thought, some day we could use this physical result to prove the parallel postulate....These men did not want to do mechanics, as, say, Newton had done. They wanted to show not only that the world was this way, but that it necessarily had to be. A modern philosophical critic, Helmut Pulte, has said that Lagrange's attempt to "reduce" mechanics to analysis strikes us today as "a misplaced endeavour to mathematize. . . an empirical science, and thus to endow it with infallibility" [39, p. 220]. Lagrange would have responded, "Right! That's just exactly what we are all doing."

We could look upon Type II errors as having a benevolent aspect: they show both that our existing methods are too weak & informal *and* that our intuition/heuristics break down at it - implying that all previous mathematical effort has been *systematically* misled in avoiding that area (as empty), and that there is much low-hanging fruit. (Consider how many scores or hundreds of key theorems were proven by the very first mathematicians to work in the non-Euclidean geometries!)

# Future implications

Should such widely-believed conjectures as [P≠NP](!Wikipedia "P=NP") or the [Riemann hypothesis](!Wikipedia) turn out be false, then because they are assumed by so many existing proofs, a far larger math holocaust would ensue - and our previous estimates of error rates will turn out to have been significant underestimates. But it may be a cloud with a silver lining, if it doesn't come at a time of danger.

# External links

- ["Flaws in the Perfection"](http://www.aleph.se/andart/archives/2012/09/flaws_in_the_perfection.html) -(Anders Sandberg commentary on this essay)
