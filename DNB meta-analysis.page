---
title: Dual N-Back Meta-analysis
description: Does DNB increase IQ? What factors affect the studies?
created: 20 May 2012
tags: DNB, psychology, meta-analysis
status: in progress
belief: likely
...

> I meta-analyze the >19 studies which measure IQ after an [n-back](DNB FAQ) intervention, confirming that there is a [gain](#analysis) of small-to-medium effect size. I also investigate several n-back claims, criticisms, and indicators of bias, finding:
>
> - [active vs passive control groups](#control-groups) criticism: found, and it accounts for half the total effect size (similar to [Zehdner et al 2009](/docs/2009-zehdner.pdf "Memory training effects in old age as markers of plasticity: a meta-analysis") & [Melby-Lervåg & Hulme 2013](http://www.apa.org/pubs/journals/releases/dev-49-2-270.pdf "Is Working Memory Training Effective? A Meta-Analytic Review"))
> - [dose-response relationship](#training-time) of n-back training time & IQ gains claim: not found
> - [payment reducing performance](#paymentextrinsic-motivation) claim: not found
> - [kind of n-back](#training-type) matters: not found
> - [publication bias](#biases) criticism: not found
> - [speeding of IQ tests](#iq-test-time) criticism: not found

Dual n-back is a working memory exercise which stress holding several items in memory and quickly updating them; the study [Jaeggi et al 2008](DNB FAQ#jaeggi-2008) found that training dual n-back increases scores on an IQ test for healthy young adults. If this result were true and influenced underlying intelligence (with its many correlates such as higher income or educational achievement), it would be an unprecedented result of inestimable social value and practical impact, and so is worth investigating in detail. In my [DNB FAQ](), I discuss a list of post-2008 experiments investigating how much and whether practicing dual n-back can increase IQ; they conflict heavily, with [some](DNB FAQ#support) finding large gains and [others](DNB FAQ#criticism) finding gains which are not statistically-significant or no gain at all.

What is one to make of these studies? When one has multiple quantitative studies going in both directions, one resorts to a [meta-analysis](!Wikipedia): we pool the studies with their various sample sizes and effect sizes and get some overall answer - do a bunch of small positive studies outweigh a few big negative ones? Or vice versa? Or any mix thereof? Unfortunately, no one has done one for n-back & IQ already; the existing study, ["Is Working Memory Training Effective? A Meta-Analytic Review"](http://www.apa.org/pubs/journals/releases/dev-49-2-270.pdf) (Melby-Lervåg & Hulme 2013), covers working memory in general, to summarize:

> However, a recent meta-analysis by Melby-Lervåg and Hulme (in press) indicates that even when considering published studies, few appropriately-powered empirical studies have found evidence for transfer from various WM training programs to fluid intelligence. Melby-Lervåg and Hulme reported that WM training showed evidence of transfer to verbal and spatial WM tasks (_d_ = .79 and .52, respectively). When examining the effect of WM training on transfer to nonverbal abilities tests in 22 comparisons across 20 studies, they found an effect of _d_ = .19. Critically, a moderator analysis showed that there was no effect (_d_ = .00) in the 10 comparisons that used a treated control group, and there was a medium effect (_d_ = .38) in the 12 comparisons that used an untreated control group.

I'm not as interested in near WM transfer from n-back training - as the Melby-Lervåg & Hulme 2013 meta-analysis confirms, it surely does - but in the transfer with many more ramifications, transfer to *IQ* as measured by a matrix test. So I decided to do a meta-analysis of my own. The hard part of a meta-analysis is doing a thorough literature search, but the FAQ sections represent a _de facto_ literature search. (I started the DNB FAQ in March 2009 after reading through all available previous discussions, and have kept it up to date with the results of discussions on the [DNB ML](http://groups.google.com/group/brain-training), along with alerts from Google Alerts, Google Scholar, and Pubmed alerts.) The corresponding authors of most of the candidate studies and Chandra Basak have been contacted with the initial list of candidate studies and asked for additional suggestions.

For background on conducting meta-analyses, I am using [chapter 9](http://www.mrc-bsu.cam.ac.uk/cochrane/handbook/chapter_9/9_analysing_data_and_undertaking_meta_analyses.htm) of [part 2](http://www.mrc-bsu.cam.ac.uk/cochrane/handbook/index.htm#chapter_9/9_2_types_of_data_and_effect_measures.htm) of the [Cochrane Collaboration](!Wikipedia)'s [_Cochrane Handbook for Systematic Reviews of Interventions_](http://www.mrc-bsu.cam.ac.uk/cochrane/handbook/front_page.htm). For the actual statistical analysis, I am using the [`metafor` package](http://www.metafor-project.org/) for the R language.

# Data

The candidate studies:

- [Jaeggi       2008](DNB FAQ#jaeggi-2008)
- [Qiu          2009](DNB FAQ#qiu-2009)
- [polar        2009](DNB FAQ#polar-june-2009)
- [Seidler      2010](DNB FAQ#seidler-2010)
- [Stephenson   2010](DNB FAQ#stephenson-2010)
- [Jaeggi       2010](DNB FAQ#jaeggi-2010)
- [Jaeggi       2011](DNB FAQ#jaeggi-2011)
- [Chooi        2011](DNB FAQ#chooi-2011)
- [Schweizer    2011](DNB FAQ#schweizer-2011)
- Preece       2011
- Zhong 2011
- [Jaušovec     2012](DNB FAQ#jaušovec-2012)
- [Kundu et al  2012](DNB FAQ#kundu-et-al-2012)
- [Salminen et al 2012](DNB FAQ#salminen-2012)
- [Redick et al 2012](DNB FAQ#redick-2012)
- Jaeggi 2012?
- Takeuchi et al 2012
- Rudebeck 2012
- [Vartanian 2013](DNB FAQ#vartanian-2013)
- Heinzel et al 2013

## Notes

Going through them, I must note:

- Jaeggi 2008 means & standard deviations estimated from graphs since I couldn't figure out how to extract them from the figures reported in the text; [Jonathan Toomim](https://groups.google.com/group/brain-training/msg/adab8135e7aa2351) counted pixels and gave a better value than I guesstimated, and I used his approach in estimating values for each training group. IQ test time is based on the description in Redick et al 2012:

    > In addition, the 19-session groups were 20 min to complete BOMAT, whereas the 12- and 17-session groups received only 10 min (S. M. Jaeggi, personal communication, May 25, 2011). As shown in Figure 2, the use of the short time limit in the 12- and 17-session studies produced substantially lower scores than the 19-session study.
- polar: control, 2nd scores: 23,27,19,15,12,35,36,34; experiment, 2nd scores: 30,35,33,33,32,30,35,33,35,33,34,30,33
- Jaeggi 2010: used BOMAT scores; should I somehow pool RAPM with BOMAT? Control group split.
- Jaeggi 2011: used SPM (a Raven's); should I somehow pool the TONI?
- Schweizer 2011: used the adjusted final scores as suggested by the authors due to potential pre-existing differences in their control & experimental groups:

    > ...This raises the possibility that the relative gains in Gf in the training versus control groups may be to some extent an artefact of baseline differences. However, the interactive effect of transfer as a function of group remained [statistically-]significant even after more closely matching the training and control groups for pre-training RPM scores (by removing the highest scoring controls) F(1, 30) = 3.66, P = 0.032, gp2 = 0.10. The adjusted means (standard deviations) for the control and training groups were now 27.20 (1.93), 26.63 (2.60) at pre-training (t(43) = 1.29, P.0.05) and 26.50 (4.50), 27.07 (2.16) at post-training, respectively.
- Stephenson data from pg79/95; means are post-scores on Raven's. I am omitting Stephenson scores on WASI, Cattell's Culture Fair Test, & BETA III Matrix Reasoning subset because `metafor` does not support multivariate meta-analyses and including them as separate studies would be statistically illegitimate. The active and passive control groups were split into thirds over each of the 3 n-back training regimens, and each training regimen split in half over the active & passive controls.

    The splitting is worth discussion. Some of these studies have multiple experimental groups, control groups, or both. A criticism of early studies was the use of no-contact control groups - the control groups did nothing except be tested twice, and it was suggested that the experimental group gains might be in part solely because they are doing a task, any task, and the control group should be doing some non-WM task as well. The WM meta-analysis Melby-Lervåg & Hulme 2013 checked for this and found that use of no-contact control groups led to a much larger estimate of effect size than studies which did use an active control. When trying to incorporate such a multi-part experiment, one cannot just copy controls as the [Cochrane Handbook](http://www.mrc-bsu.cam.ac.uk/cochrane/handbook/chapter_16/16_5_4_how_to_include_multiple_groups_from_one_study.htm "16.5.4  How to include multiple groups from one study") points out:

    > One approach that must be avoided is simply to enter several comparisons into the meta-analysis when these have one or more intervention groups in common. This 'double-counts' the participants in the 'shared' intervention group(s), and creates a unit-of-analysis error due to the unaddressed correlation between the estimated intervention effects from multiple comparisons (see Chapter 9, Section 9.3).

    Just dropping one control or experimental group weakens the meta-analysis, and may bias it as well if not done systematically. I have used one of its suggested approaches which accepts some additional error in exchange for greater power in checking this possible active versus no-contact distinction, in which we instead *split* the shared group:

    > A further possibility is to include each pair-wise comparison separately, but with shared intervention groups divided out approximately evenly among the comparisons. For example, if a trial compares 121 patients receiving acupuncture with 124 patients receiving sham acupuncture and 117 patients receiving no acupuncture, then two comparisons (of, say, 61 'acupuncture' against 124 'sham acupuncture', and of 60 'acupuncture' against 117 'no intervention') might be entered into the meta-analysis. For dichotomous outcomes, both the number of events and the total number of patients would be divided up. For continuous outcomes, only the total number of participants would be divided up and the means and standard deviations left unchanged. This method only partially overcomes the unit-of-analysis error (because the resulting comparisons remain correlated) so is not generally recommended. A potential advantage of this approach, however, would be that approximate investigations of heterogeneity across intervention arms are possible (for example, in the case of the example here, the difference between using sham acupuncture and no intervention as a control group).
- Chooi: [the relevant table](/docs/2010-chooi-table.pdf) was provided in private communication; I split each experimental group in half to pair it up with the active and passive control groups which trained the same number of days
- Jaeggi 2011 Psychonomics poster (`Jaeggi4`): as-yet unpublished study; poster with table of scores provided in personal communication, used scores on the RAPM (not Cattell's CFT, BOMAT, or the BOMAT followup); 2 experimental groups (one was single audio n-back, the other dual n-back), one control group split <!-- docs/2011-jaeggi-poster.pdf -->
- [Takeuchi et al 2012](/docs/2012-takeuchi.pdf "Effects of working memory training on functional connectivity and cerebral blood flow during rest"): subjects were trained on 3 WM tasks in addition to DNB for 27 days, 30-60 minutes; RAPM scores used, BOMAT & Tanaka B-type intelligence test scores omitted
- Jaušovec 2012: IQ test time was calculated based on the description

    > Used were 50 test items - 25 easy (Advanced Progressive Matrices Set I - 12 items and the B Set of the Colored Progressive Matrices), and 25 difficult items (Advanced Progressive Matrices Set II, items 12-36). Participants saw a figural matrix with the lower right entry missing. They had to determine which of the four options fitted into the missing space. The tasks were presented on a computer screen (positioned about 80-100 cm in front of the respondent), at fixed 10 or 14 s interstimulus intervals. They were exposed for 6 s (easy) or 10 s (difficult) following a 2-s interval, when a cross was presented. During this time the participants were instructed to press a button on a response pad (1-4) which indicated their answer.

    $\frac{25 \times (6+2) + 25 \times (10+2)}{60} = 8.33$ minutes.
- Zhong 2011: "dual attention channel" task omitted, dual and single n-back scores kept unpooled and controls split across the 2; I thank Emile Kroger for his translations of key parts of the thesis. Unable to get whether IQ test was administered speeded. Zhong 2011 appears to have replicated Jaeggi 2008's training time.
- Jonasson 2011 omitted for lacking any measure of IQ
- Preece 2011 omitted; only the Figure Weights subtest from the WAIS was reported, but RAPM scores were taken and published in the inaccessible Palmer 2011
- Kundu et al 2011 and Kundu 2012 have been split into 2 experiments based on the raw data: the smaller one using the full RAPM 36-matrix 40-minute test, and the larger an 18-matrix 10-minute test
- Redick et al: n-back split over passive control & active control (visual search) RAPM post scores (omitted SPM and Cattell Culture-Fair Test)
- Vartanian 2013: short n-back intervention not adaptive; I did not specify in advance that the n-back interventions had to be adaptive (possibly some of the others were not) and subjects trained for <50 minutes, so the lack of adaptiveness may not have mattered.
- Heinzel et al 2013 mentions conducting a pilot study; I contacted Heinzel and no measures like Raven's were taken in it. The main study used both SPM and also "the Figural Relations subtest of a German intelligence test (LPS)"; as usual, I drop alternatives in favor of the more common test.
- Thompson et al 2013; used RAPM rather than WAIS; treated the "multiple object tracking"/MOT as an active control group since it did not statistically-significantly improve RAPM scores

The following authors had their studies omitted and have been contacted for clarification:

- Seidler, Jaeggi et al 2010 (experimental: _n_=47; control: _n_=45) did not report means or standard deviations

The `active` moderator variable is whether a control group was no-contact or trained on some other task. The review Morrison & Chein 2011[^Chein-no-contact] noted that no-contact control groups limited the validity of such studies, a criticism that was echoed with greater force by Shipstead, Redick, & Engle 2012; the Melby-Lervåg & Hulme 2013 meta-analysis then confirmed that use of no-contact controls inflated the effect size estimates. So I wondered if this held true for the subset of n-back & IQ studies. (Age is an interesting moderator in Melby-Lervåg & Hulme 2013, but in the following DNB & IQ studies there is only 1 study involving children - all the others are adults or young adults.)

Other variables:

1. IQ type:

   0. BOMAT
   1. Raven's Advanced Progressive Matrices (RAPM)
   2. Raven's Standard Progressive Matrices (SPM)
2. record speed of IQ test: minutes allotted (upper bound if more details are given; if no time limits, default to 30 minutes since apparently no one takes longer)
3. n-back type:

   0. dual n-back (audio & visual modalities)
   1. single n-back (visual modality)
   2. single n-back (audio modality)
4. paid: expected value of total payment in dollars, converted if necessary; if a paper does not mention payment or compensation, I assume 0 (likewise subjects receiving course credit or extra credit - so common in psychology studies that there must not be any effect), and if the rewards are of real but small value (eg. "For each correct response, participants earned points that they could cash in for token prizes such as pencils or stickers."), I code as 1.

[^Chein-no-contact]: from pg 54-55:

    > An issue of great concern is that observed test score improvements may be achieved through various influences on the expectations or level of investment of participants, rather than on the intentionally targeted cognitive processes. One form of expectancy bias relates to the placebo effects observed in clinical drug studies. Simply the belief that training should have a positive influence on cognition may produce a measurable improvement on post-training performance. Participants may also be affected by the demand characteristics of the training study. Namely, in anticipation of the goals of the experiment, participants may put forth a greater effort in their performance during the post-training assessment. Finally, apparent training-related improvements may reflect differences in participants' level of cognitive investment during the period of training. Since participants in the experimental group often engage in more mentally taxing activities, they may work harder during post-training assessments to assure the value of their earlier efforts.
    >
    > Even seemingly small differences between control and training groups may yield measurable differences in effort, expectancy, and investment, but these confounds are most problematic in studies that use no control group (Holmes et al., 2010; Mezzacappa & Buckner, 2010), or only a no-contact control group; a cohort of participants that completes the pre and post training assessments but has no contact with the lab in the interval between assessments. Comparison to a no-contact control group is a prevalent practice among studies reporting positive far transfer (Chein & Morrison, 2010; Jaeggi et al., 2008; Olesen et al., 2004; Schmiedek et al., 2010; Vogt et al., 2009). This approach allows experimenters to rule out simple test-retest improvements, but is potentially vulnerable to confounding due to expectancy effects. An alternative approach is to use a "control training" group, which matches the treatment group on time and effort invested, but is not expected to benefit from training (groups receiving control training are sometimes referred to as "active control" groups). For instance, in Persson and Reuter-Lorenz (2008), both trained and control subjects practiced a common set of memory tasks, but difficulty and level of interference were higher in the experimental group's training. Similarly, control train- ing groups completing a non-adaptive form of training (Holmes et al., 2009; Klingberg et al., 2005) or receiving a smaller dose of training (one-third of the training trials as the experimental group, e.g., Klingberg et al., 2002) have been used as comparison groups in assessments of Cogmed variants. One recent study conducted in young children found no differences in performance gains demonstrated by a no-contact control group and a control group that completed a non-adaptive version of training, suggesting that the former approach may be adequate (Thorell et al., 2009). We note, however, that regardless of the control procedures used, not a single study conducted to date has simultaneously controlled motivation, commitment, and difficulty, nor has any study attempted to demonstrate *explicitly* (for instance through subject self-report) that the control subjects experienced a comparable degree of motivation or commitment, or had similar expectancies about the benefits of training

<!-- training:
Jaeggi 2008: see later for extracting numbers for each group
Polar: from spreadsheet, 6 hours
Qiu: 10 days 25 minutes
Stephenson 2010: 20 days x 20 minutes pg39-40
Jaeggi 2010: 4*5*18.5=370
Jaeggi 2011: 15min * 19.09 =
Chooi: 30*8=240; 30*20=600
Jaeggi4: replicating 2008 in part, 20 days/sessions, so 20*25=500??? TODO ask Jaeggi
Kundu: 25 * 40 = 1000
Schweizer: 25 * 18.5=463 (rough midpoint of 18.67, 19, 18)
Jausovec:  "On average each of the participants completed 30 h of training." 30*60=1800
Kundu2: ditto previous
Salminen: 14 days, resulting in a stimulus presentation rate of 3 s...Altogether, 20 runs were completed in each session, and one run consisted of 20 + n trials (e.g., a 2-back task contained 22 trials). ...final mean DNB level was 3.63 from a start of 2
so: ((((((2+3.63)/2 + 20) * 3) / 60) * 20) * 14) = 319
Redick 2012: Subjects in the two training groups completed an additional 20 practice sessions, each of which took between 30-40 min
Takeuchi: 'The WMT program consisted of computerized, in-house developed Borland Cþþ programs comprising four computerized tasks. Subjects undertook approximately 4 weeks (27 days) of training (each day, 20e60 min in most cases).'; DNB was 1 task, so 27 * (((20+60)/2)/4)=270
-->
<!-- Jaeggi 2008: extract by counting pixels! Training gain:

0: 661
1: 554
2: 446
3: 340
4: 232
5: 125

pre-test (experimentals): 9.59(3.465)

Differentials based on fig.3 in Jaeggi 2008:

#8: 634; 571-698 | mean: 661-554 661-634 27/107 = 0.25; stddev: ((634-571) + (698-634))/2=63.5px=0.593
0.25(0.59)
#12: 514; 464-566 | mean=1+(512-446)/107=1+0.61=1.61; stddev: (566-514)/107=0.48
1.61(0.48)
#17: 299; 224-375 | mean=3+(375-299)/107=3+0.71=3.71; stddev: (299-224)/107=0.7
3.7(0.7)
#19: 171; 90-252 | mean=4+(232-171)/107=4.57; stddev: (171-90) / 107=0.76
4.57(0.76)

#8 9.84(0.59)
#12 11.2(0.48)
#17 13.29(0.7)
#19 14.16(0.76)

(25 minutes per day)
-->
<!--
Zhong 2011 active: 0 training: variable IQ: RAPM (MrEmile) speed: unknown
Zhong
283 in first experiment; 16 groups = 17.6 per group; scores in table 2
control
"single task of visual working
memory"
"working memory dual n-back"
"dual-channel attention
task"
1. 25min training per day? replicating jaeggi 2008; so 125,250,375,500 minutes; experiment n=17.6, split control=8.8
2. 74 in second; 4 groups = 18.5 per group; 375,475min; scores in table 7
-->

## Table

The data from the surviving studies:

year study         n.e   mean.e    sd.e   n.c   mean.c  sd.c   active   training   IQ   speed nbt paid
---- ------------- ---   -----     ----   ---   -----   ----   ------   --------   --   ----- --- ----
2008 Jaeggi1.1     16    9.84      0.59   8.75  10.51   3.721  0        200        0    10    0   0
2008 Jaeggi1.2     22    11.2      0.48   8.75  10.51   3.721  0        300        1    10    0   0
2008 Jaeggi1.3     16    13.29     0.7    8.75  10.51   3.721  0        425        1    10    0   0
2008 Jaeggi1.4     15    14.16     0.76   8.75  10.51   3.721  0        475        1    20    0   0
2009 Qiu           9     132.1     3.2    10    130     5.3    0        250        2    25    0   0
2009 polar         13    32.76     1.83   8     25.12   9.37   0        360        0    30    0   0
2010 Jaeggi2.1     21    13.67     3.17   21.5  11.44   2.58   0        370        0    16    1   91
2010 Jaeggi2.2     25    12.28     3.09   21.5  11.44   2.58   0        370        0    16    0   20
2010 Stephenson.1  14    17.54     0.76   9.3   15.50   0.99   1        400        1    10    0   0.44
2010 Stephenson.2  14    17.54     0.76   8.6   14.08   0.65   0        400        1    10    0   0.44
2010 Stephenson.3  14.5  15.34     0.90   9.3   15.50   0.99   1        400        1    10    1   0.44
2010 Stephenson.4  14.5  15.34     0.90   8.6   14.08   0.65   0        400        1    10    1   0.44
2010 Stephenson.5  12.5  15.32     0.83   9.3   15.50   0.99   1        400        1    10    2   0.44
2010 Stephenson.6  12.5  15.32     0.83   8.6   14.08   0.65   0        400        1    10    2   0.44
2011 Jaeggi4.d     25    14.96     2.7    13.5  14.74   2.8    1        500        1    30    0   0
2011 Jaeggi4.s     26    15.23     2.44   13.5  14.74   2.8    1        500        1    30    2   0
2011 Chooi.1.1     4.5   12.7      2      15    13.3    1.91   1        240        1    20    0   0
2011 Chooi.1.2     4.5   12.7      2      22    11.3    2.59   0        240        1    20    0   0
2011 Chooi.2.1     6.5   12.1      2.81   11    13.4    2.7    1        600        1    20    0   0
2011 Chooi.2.2     6.5   12.1      2.81   23    11.9    2.64   0        600        1    20    0   0
2011 Jaeggi3       32    16.94     4.75   30    16.2    5.1    1        287        2    10    1   1
2011 Kundu1        3     31        1.73   3     30.3    4.51   1        1000       1    40    0   0
2011 Schweizer     29    27.07     2.16   16    26.5    4.5    1        463        2    30    0   0
2011 Zhong.1.05d   17.6  21.38     1.71   8.8   21.85   2.6    0        125        1          0   0
2011 Zhong.1.05s   17.6  22.83     2.5    8.8   21.85   2.6    0        125        1          0   0
2011 Zhong.1.10d   17.6  22.21     2.3    8.8   21      1.94   0        250        1          0   0
2011 Zhong.1.10s   17.6  23.12     1.83   8.8   21      1.94   0        250        1          0   0
2011 Zhong.1.20d   17.6  23.06     1.48   8.8   23.38   1.56   0        500        1          0   0
2011 Zhong.1.20s   17.6  23.06     3.15   8.8   23.38   1.56   0        500        1          0   0
2011 Zhong.2.15s   18.5  6.89      0.99   18.5  5.15    2.01   0        375        1          0   0
2011 Zhong.2.19s   18.5  6.72      1.07   18.5  5.35    1.62   0        475        1          0   0
2011 Zhong.1.15d   17.6  24.12     1.83   8.8   23.78   1.48   0        375        1          0   0
2011 Zhong.1.15s   17.6  25.11     1.45   8.8   23.78   1.48   0        375        1          0   0
2012 Jaušovec      14    32.43     5.65   15    29.2    6.34   1        1800       1    8.3   0   0
2012 Kundu2        11    10.81     2.32   12    9.5     2.02   1        1000       1    10    0   0
2012 Redick.1      12    6.25      3.08   20    6       3      0        700        1    10    0   204.3
2012 Redick.2      12    6.25      3.08   29    6.24    3.34   1        700        1    10    0   204.3
2012 Rudebeck      27    9.52      2.03   28    7.75    2.53   0        400        0    10    0   0
2012 Salminen      13    13.7      2.2    9     10.9    4.3    0        319        1    20    0   55
2012 Takeuchi      41    31.9      0.4    20    31.2    0.9    0        270        1    30    0   0
2013 Vartanian     17    11.18     2.53   17    10.41   2.24   1        60         1    10    1   0
2013 Heinzel.1     15    24.53     2.9    15    23.07   2.34   0        540        2    7.5   1   129
2013 Heinzel.2     15    17        3.89   15    15.87   3.13   0        540        2    7.5   1   129
2013 Thompson.1    10    13.2      0.67   19    12.7    0.62   0        800        1    25    0   287
2013 Thompson.2    10    13.2      0.67   19    13.3    0.5    1        800        1    25    0   287

# Analysis

The result of the meta-analysis:

~~~{.R}
Random-Effects Model (k = 45; tau^2 estimator: REML)

tau^2 (estimated amount of total heterogeneity): 0.2062 (SE = 0.0789)
tau (square root of estimated tau^2 value):      0.4541
I^2 (total heterogeneity / total variability):   57.10%
H^2 (total variability / sampling variability):  2.33

Test for Heterogeneity:
Q(df = 44) = 106.3486, p-val < .0001

Model Results:

estimate       se     zval     pval    ci.lb    ci.ub
  0.5119   0.0915   5.5959   <.0001   0.3326   0.6913
~~~

To depict the random-effects model in a more graphic form, we use the "[forest plot](!Wikipedia)":

![`forest(res1, slab = paste(dnb$study, dnb$year, sep = ", "))`](/images/dnb/forest.png)

The overall effect is reasonably strong. But there seems to be substantial differences between studies: this heterogeneity may be what is showing up as a high τ^2^ and [_i_^2^](http://eprints.lincoln.ac.uk/1932/1/MetaAnalysisPaper.pdf "'Measuring inconsistency in meta-analyses', Higgins et al 2003"); and indeed, if we look at the computed SMDs, we see one sample with _d_=2.59 (!) and some instances of _d_<0. The high heterogeneity means that the fixed-effects model is inappropriate, as clearly the studies are not all measuring the same effect, so we use a random-effects.

The confidence interval excludes zero, so one might conclude that n-back does increase IQ scores. From a Bayesian standpoint, it's worth pointing out that this is not nearly as conclusive as it seems: our prior that any particular intervention would increase the underlying genuine fluid intelligence is extremely small, as scores or hundreds of attempts to increase IQ over the past century have all eventually turned out to be failures, with extremely few exceptions (eg. pre-natal iodine or iron), so very strong evidence is necessary to conclude that a particular attempt is one of those extremely rare exceptions. This skeptical attitude is relevant to our examination of moderators.

## Moderators
### Control groups

A major criticism of n-back studies is that the effect is being manufactured by the methodological problem of some studies using a no-contact or passive control group rather than an active control group. (Passive controls know they received no intervention and that the researchers don't expect them to do better on the post-test, which may reduce their efforts & lower their scores.) Each study has been coded appropriately, and we can ask whether it matters:

~~~{.R}
Mixed-Effects Model (k = 45; tau^2 estimator: REML)

tau^2 (estimated amount of residual heterogeneity):     0.1416 (SE = 0.0653)
tau (square root of estimated tau^2 value):             0.3763
I^2 (residual heterogeneity / unaccounted variability): 47.62%
H^2 (unaccounted variability / sampling variability):   1.91

Test for Residual Heterogeneity:
QE(df = 43) = 91.7976, p-val < .0001

Test of Moderators (coefficient(s) 1,2):
QM(df = 2) = 44.4450, p-val < .0001

Model Results:

                 estimate      se    zval    pval    ci.lb   ci.ub
factor(active)0    0.6653  0.1018  6.5326  <.0001   0.4657  0.8649
factor(active)1    0.1898  0.1427  1.3303  0.1834  -0.0898  0.4694
~~~

The active/control variable confirms the criticism: lack of active control groups *is* responsible for a large chunk of the overall effect, with the confidence intervals overlap only partially. The effect with passive control groups is a dramatic _d_=0.7 while with active control groups, the IQ gains shrink to _d_=0.2 (and the 95% CI does not exclude _d_=0). Indeed, the difference is almost statistically-significant since the CIs only just barely overlap.

We can see the difference by splitting a forest plot on passive vs active:

![The visibly different groups of passive then active studies, plotted on the same axis](/images/dnb/forest-activevspassive.png)

### Training time

Jaeggi et al 2008 observed a dose-response to training, where those who trained the longest apparently improved the most. Ever since, this has been cited as a factor in what studies will observe gains or as an explanation why some studies did not see improvements - perhaps they just didn't do enough training. `metafor` is able to look at the number of minutes subjects in each study trained for to see if there's any obvious linear relationship:

~~~{.R}
Mixed-Effects Model (k = 45; tau^2 estimator: REML)

tau^2 (estimated amount of residual heterogeneity):     0.2178 (SE = 0.0825)
tau (square root of estimated tau^2 value):             0.4667
I^2 (residual heterogeneity / unaccounted variability): 58.40%
H^2 (unaccounted variability / sampling variability):   2.40

Test for Residual Heterogeneity:
QE(df = 43) = 105.9564, p-val < .0001

Test of Moderators (coefficient(s) 2):
QM(df = 1) = 0.2098, p-val = 0.6469

Model Results:

         estimate      se     zval    pval    ci.lb   ci.ub
intrcpt    0.5818  0.1766   3.2951  0.0010   0.2358  0.9279
mods      -0.0001  0.0003  -0.4581  0.6469  -0.0008  0.0005
~~~

The estimate of the relationship is that there is none at all: the estimated coefficient has a large _p_-value, and further, that coefficient is negative. This may seem initially implausible but if we graph the time spent training per study with the final (unweighted) effect size, we see why:

![`plot(dnb$training, res1$yi)`](/images/dnb/effectsizevstrainingtime.png)

### IQ test time

Similarly, Moody 2009 identified the 10 minute test-time or "speeding" of the RAPM as a concern in whether far transfer actually happened; after collecting the allotted test time for the studies, we can likewise look for whether there is an inverse relationship (the more time given to subjects on the IQ test, the smaller their IQ gains):

~~~{.R}
Mixed-Effects Model (k = 35; tau^2 estimator: REML)

tau^2 (estimated amount of residual heterogeneity):     0.2887 (SE = 0.1127)
tau (square root of estimated tau^2 value):             0.5373
I^2 (residual heterogeneity / unaccounted variability): 65.15%
H^2 (unaccounted variability / sampling variability):   2.87

Test for Residual Heterogeneity:
QE(df = 33) = 90.2648, p-val < .0001

Test of Moderators (coefficient(s) 2):
QM(df = 1) = 0.3870, p-val = 0.5339

Model Results:

         estimate      se     zval    pval    ci.lb   ci.ub
intrcpt    0.6763  0.2561   2.6407  0.0083   0.1744  1.1783
mods      -0.0086  0.0138  -0.6221  0.5339  -0.0355  0.0184
~~~

A tiny slope which is extremely non-statistically-significant; graphing the (unweighted) studies suggests as much:

![`plot(dnb$speed, res1$yi)`](/images/dnb/iqspeedversuseffect.png)

### Training type

One question of interest both for issues of validity and for effective training is whether the existing studies show larger effects for a particular kind of n-back training: dual (visual & audio; labeled 0) or single (visual; labeled 1) or single (audio; labeled 2)? If visual single n-back turns in the largest effects, that is troubling since it's also the one most resembling a matrix IQ test. Checking against the 3 kinds of n-back training:

~~~{.R}
Mixed-Effects Model (k = 45; tau^2 estimator: REML)

tau^2 (estimated amount of residual heterogeneity):     0.2325 (SE = 0.0870)
tau (square root of estimated tau^2 value):             0.4821
I^2 (residual heterogeneity / unaccounted variability): 59.74%
H^2 (unaccounted variability / sampling variability):   2.48

Test for Residual Heterogeneity:
QE(df = 42) = 105.7191, p-val < .0001

Test of Moderators (coefficient(s) 1,2,3):
QM(df = 3) = 29.6189, p-val < .0001

Model Results:

              estimate      se    zval    pval    ci.lb   ci.ub
factor(nbt)0    0.5334  0.1085  4.9145  <.0001   0.3207  0.7461
factor(nbt)1    0.4556  0.2281  1.9972  0.0458   0.0085  0.9026
factor(nbt)2    0.4492  0.3694  1.2159  0.2240  -0.2749  1.1732
~~~

There are not enough studies using the other kinds of n-back to say anything conclusive.

### Payment/extrinsic motivation

In a 2013 talk, ["'Brain Training: Current Challenges and Potential Resolutions', with Susanne Jaeggi, PhD"](http://vimeo.com/65064957), Jaeggi suggests

> Extrinsic reward can undermine people's intrinsic motivation. If extrinsic reward is crucial, then its influence should be visible in our data.

I investigated payment as a moderator. Payment seems to actually be quite rare in n-back studies (in part because it's so common to just recruit students with course credit or extra credit), and so the result is that as a moderator payment is currently a small and non-statistically-significant negative effect, whether you regress on the total payment amount or treat it as a boolean variable. More interestingly, it seems that the negative sign is being driven by payment being associated with higher-quality studies using active control groups, because when you look at the interaction, payment in a study with an active control group actually flips sign to being positive again (correlating with a bigger effect size).

More specifically, if we check payment as a binary variable, we get a decrease in effect size which is not statistically-significant:

~~~{.R}
tau^2 (estimated amount of residual heterogeneity):     0.2073 (SE = 0.0803)
tau (square root of estimated tau^2 value):             0.4553
I^2 (residual heterogeneity / unaccounted variability): 57.03%
H^2 (unaccounted variability / sampling variability):   2.33

Test for Residual Heterogeneity:
QE(df = 43) = 103.4400, p-val < .0001

Test of Moderators (coefficient(s) 2):
QM(df = 1) = 1.1418, p-val = 0.2853

Model Results:

                      estimate      se     zval    pval    ci.lb   ci.ub
intrcpt                 0.5689  0.1060   5.3691  <.0001   0.3612  0.7766
as.logical(paid)TRUE   -0.2254  0.2110  -1.0686  0.2853  -0.6389  0.1881
~~~

If we instead regress against the total payment size (perhaps larger payments discourage one more?), the effect of each additional dollar is very small and 0 is far from excluded as the coefficient:

~~~{.R}
Model Results:

         estimate      se     zval    pval    ci.lb   ci.ub
intrcpt    0.5558  0.1003   5.5394  <.0001   0.3591  0.7524
paid      -0.0013  0.0012  -1.0714  0.2840  -0.0036  0.0011
~~~

Finally, as I've mentioned before, the difference in effect size between active and passive control groups is quite striking, and I noticed that eg. the Redick et al 2012 experiment paid subjects a lot of money to put up with all its tests and ensure subject retention & Thompson et al 2013 paid a lot to put up with the fMRI machine and long training sessions, so what happens if we look for an interaction?

~~~{.R}
Model Results:

                             estimate      se     zval    pval    ci.lb    ci.ub
intrcpt                        0.7276  0.1212   6.0035  <.0001   0.4901   0.9652
active                        -0.4791  0.2070  -2.3144  0.0206  -0.8848  -0.0734
as.logical(paid)TRUE          -0.2303  0.2362  -0.9750  0.3296  -0.6933   0.2327
active:as.logical(paid)TRUE   -0.0072  0.4136  -0.0175  0.9860  -0.8179   0.8034
~~~

Active control groups cut the observed effect of n-back by more than half, as usual, and passive+payment shrinks the effect size by ~34%, but active+payment cuts to zero effect.

Hence, I infer that the previous negative correlation is probably because all the passive experiments are also not paying (no need to pay controls if they're not doing anything besides showing up twice for testing, really), but the true impact of payment is, if anything, probably opposite of Jaeggi's suggestion.

<!--
interaction between speeding & test:

rma(measure="SMD", m1i = mean.e, m2i = mean.c, sd1i = sd.e, sd2i = sd.c, n1i = n.e, n2i = n.c, data = dnb, mods= ~ dnb$speed * dnb$IQ)

all moderators, currently:
rma(measure="SMD", m1i = mean.e, m2i = mean.c, sd1i = sd.e, sd2i = sd.c, n1i = n.e, n2i = n.c, data = dnb, mods = ~(factor(dnb$active) + dnb$training + dnb$speed + factor(dnb$IQ)))
-->
<!-- preliminary multivariate data

Stephenson 2010 scores on Cattell's Culture Fair Test, BETA III Matrix Reasoning, & the Wechsler
Abbreviated Scale of Intelligence (WASI) Matrix Reasoning subset
"Stephenson.1" 2010   41   16.085    0.83   28    15.50   0.99   1        400        RAPM    10
"Stephenson.2" 2010   41   16.085    0.83   26    14.08   0.65   0        400        RAPM   10
pg110 mean gains
cattell
 dual       28.75 0.95
 visual     27.59 0.89
 auditory   26.60 0.92
 stm        26.50 0.90
 control    26.27 0.90
wasi
 dual       22.04 0.44
 visual     20.79 0.43
 auditory   21.64 0.45
 stm        21.39 0.47
 control    21.12 0.42
beta-iii
 dual       22.12 0.38
 visual     21.00 0.36
 auditory   20.64 0.40
 stm        20.96 0.45
 control    19.42 0.48

Takeuchi et al 2012 BOMAT & Tanaka B-type intelligence test scores omitted
"Takeuchi"     2012   41   31.9      0.4    20    31.2    0.9    0        270        RAPM    30
Bomat 9.26 0.32 / 9.72 0.54
Tanaka 123.9 1.6 / 120.3 2.8

Redick et al: omitted SPM and Cattell Culture-Fair Test
"Redick.1"     2012   12   6.25      3.08   20    6       3      0        700        RAPM    10
"Redick.2"     2012   12   6.25      3.08   29    6.24    3.34   1        700        1    10
Cattell control post 11.45 2.65
        visual post 11.24 2.25
        experimental post 11.38 2.45
SPM control: 16.85 2.35
    visual: 16.45 2.47
    experimental: 16.09 2.61

CFT, BOMAT, exclude BOMAT followup
"Jaeggi4"      2011   51   15.097    2.56   27    14.74   2.8    1        500        1    30
dual
 bomat n=25 18.48 3.90
 catell n=25 20.56 3.25
single
 bomat n=26 18.96 2.84
 cattell n=26 20.15 2.51
control
 bomat n=27 17.63 3.78
 catell 19.63 3.12
-->

## Biases

N-back has been presented in some popular & academic medias in an entirely uncritical & positive light: ignoring the overwhelming failure of intelligence interventions in the past, not citing the failures to replicate, and giving short schrift to the criticisms which have been made. (Examples include the [_NYT_](http://www.nytimes.com/2008/04/29/health/research/29brai.html), [_WSJ_](http://online.wsj.com/article/SB10001424052702304432304576371462612272884.html), [_Scientific American_](https://docs.google.com/file/d/0B-bpmBygrg8LUTJvazhzOFYwQUk/edit), & [Nisbett et al 2012](http://scottbarrykaufman.com/wp-content/uploads/2012/01/Nisbett-et-al.-2012.pdf "Intelligence: New Findings and Theoretical Developments").) One researcher told me that a reviewer savaged their work, asserting that n-back works and thus their null result meant only that they did something wrong. So it's worth investigating, to the extent we can, whether there is a [publication bias](!Wikipedia) towards publishing only positive results.

20+ groups (some from quite small studies) is considered medium for a meta-analysis but it does permit us to generate [funnel plots](!Wikipedia "Funnel plot") , or check for possible publication bias via the [trim-and-fill method](/docs/2000-duval.pdf "'Trim and fill: A simple funnel-plot-based method of testing and adjusting for publication bias in meta-analysis', Duval & Tweedie 2000").

### Funnel plot

~~~{.R}
Regression Test for Funnel Plot Asymmetry

model:     mixed-effects meta-regression model
predictor: standard error

test for funnel plot asymmetry: z = 2.7702, p = 0.0056
~~~

The asymmetry has reached statistical-significance, so let's visualize it:

![`funnel(res1)`](/images/dnb/funnel.png)

This looks reasonably good, although we see that studies are crowding the edges of the funnel. We know that the studies with active control groups show twice the effect-size of the passive control groups, is this related? If we plot the residual left after correcting for active vs passive, the funnel plot improves a lot (Stephenson remains an outlier):

![Mixed-effects plot of standard error versus effect size after moderator correction.](/images/dnb/funnel-moderators.png)

### Trim-and-fill

The trim-and-fill estimate:

~~~{.R}
Estimated number of missing studies on the left side: 0

Random-Effects Model (k = 45; tau^2 estimator: REML)

tau^2 (estimated amount of total heterogeneity): 0.2062 (SE = 0.0789)
tau (square root of estimated tau^2 value):      0.4541
I^2 (total heterogeneity / total variability):   57.10%
H^2 (total variability / sampling variability):  2.33

Test for Heterogeneity:
Q(df = 44) = 106.3486, p-val < .0001

Model Results:

estimate       se     zval     pval    ci.lb    ci.ub
  0.5119   0.0915   5.5959   <.0001   0.3326   0.6913
~~~

Graphing it:

![`funnel(tf)`](/images/dnb/funnel-trimfill.png)

Overall, the results suggest that this particular (comprehensive) collection of DNB studies does not suffer from serious publication bias after taking in account the active/passive moderator.

<!--
TODO:

1. followup on Seidler/Jaeggi emails
2. do some form of [sensitivity analysis](!Wikipedia) for study selection - remove or include the most questionable studies? Polar, perhaps
3. ping Kenny Hicks for meta-analysis review once Stephenson and Seidler are in
-->

## Source

Run as `R --slave --file=dnb.r`:

~~~{.R}
set.seed(7777) # for reproducible numbers
# TODO: factor out common parts of `png` (& make less square), and `rma` calls
library(XML)
dnb <- readHTMLTable(colClasses = c("integer", "character", rep("numeric", 11)),
                     "http://www.gwern.net/DNB%20meta-analysis")[[1]]
# install.packages("metafor") # if not installed
library(metafor)

cat("Basic random-effects meta-analysis of all studies:\n")
res1 <- rma(measure="SMD", m1i = mean.e, m2i = mean.c, sd1i = sd.e, sd2i = sd.c, n1i = n.e, n2i = n.c,
            data = dnb); res1

png(file="~/wiki/images/dnb/forest.png", width = 580, height = 580)
forest(res1, slab = paste(dnb$study, dnb$year, sep = ", "))
invisible(dev.off())

cat("Random-effects with passive/active control groups moderator:\n")
rma(measure="SMD", m1i = mean.e, m2i = mean.c, sd1i = sd.e, sd2i = sd.c, n1i = n.e, n2i = n.c, data = dnb,
    mods = ~ factor(active) - 1)

# this is ridiculously ugly & fragile, I regret ever wanting to show the split.
png(file="~/wiki/images/dnb/forest-activevspassive.png", width = 580, height = 780)
totalHeight <- nrow(dnb) + 8
activeTop <- nrow(dnb[dnb$active==1,])+2
forest(rma(measure="SMD", m1i = mean.e, m2i = mean.c, sd1i = sd.e, sd2i = sd.c, n1i = n.e, n2i = n.c,
       data = dnb), slab = paste(dnb$study, dnb$year, sep = ", "), order=order(dnb$active, decreasing=T),
       ylim=c(0, totalHeight), rows=c(3:activeTop, (4+activeTop):(totalHeight-3)), mlab="overall")
res2 <- rma(measure="SMD", m1i = mean.e, m2i = mean.c, sd1i = sd.e, sd2i = sd.c, n1i = n.e, n2i = n.c,
            subset=(active==0),data = dnb)
addpoly(res2, row=19, cex=.75, atransf=exp, mlab="passive")
res3 <- rma(measure="SMD", m1i = mean.e, m2i = mean.c, sd1i = sd.e, sd2i = sd.c, n1i = n.e, n2i = n.c,
            subset=(active==1),data = dnb)
addpoly(res3, row=1, cex=.75, atransf=exp, mlab="active")
invisible(dev.off())

cat("Random-effects, regressing against training time:\n")
rma(measure="SMD", m1i = mean.e, m2i = mean.c, sd1i = sd.e, sd2i = sd.c, n1i = n.e, n2i = n.c, data = dnb,
    mods = training)

png(file="~/wiki/images/dnb/effectsizevstrainingtime.png", width = 580, height = 580)
plot(dnb$training, res1$yi)
invisible(dev.off())

cat("Random-effects, regressing against administered speed of IQ tests:\n")
rma(measure="SMD", m1i = mean.e, m2i = mean.c, sd1i = sd.e, sd2i = sd.c, n1i = n.e, n2i = n.c,
    data = dnb, mods=speed)

png(file="~/wiki/images/dnb/iqspeedversuseffect.png", width = 580, height = 580)
plot(dnb$speed, res1$yi)
invisible(dev.off())

cat("Random-effects, regressing against kind of n-back training:\n")
rma(measure="SMD", m1i = mean.e, m2i = mean.c, sd1i = sd.e, sd2i = sd.c, n1i = n.e, n2i = n.c,
                   data = dnb, mods=~factor(nbt)-1)

cat("*, regressing against payment amount:\n")
rma(measure="SMD", m1i = mean.e, m2i = mean.c, sd1i = sd.e, sd2i = sd.c, n1i = n.e, n2i = n.c, data = dnb,
    mods = ~ paid)
cat("*, payment as a binary moderator:\n")
rma(measure="SMD", m1i = mean.e, m2i = mean.c, sd1i = sd.e, sd2i = sd.c, n1i = n.e, n2i = n.c, data = dnb,
    mods = ~ as.logical(paid))
cat("*, checking for interaction with higher experiment quality:\n")
rma(measure="SMD", m1i = mean.e, m2i = mean.c, sd1i = sd.e, sd2i = sd.c, n1i = n.e, n2i = n.c, data = dnb,
    mods = ~ active * as.logical(paid))

cat("Publication bias checks using funnel plots:\n")
regtest(res1, model = "rma", predictor = "sei", ni = NULL)

png(file="~/wiki/images/dnb/funnel.png", width = 580, height = 580)
funnel(res1)
invisible(dev.off())

# If we plot the residual left after correcting for active vs passive, the funnel plot improves
png(file="~/wiki/images/dnb/funnel-moderators.png", width = 580, height = 580)
res2 <- rma(measure="SMD", m1i = mean.e, m2i = mean.c, sd1i = sd.e, sd2i = sd.c, n1i = n.e, n2i = n.c,
           data = dnb, mods = ~ factor(active)-1 )
funnel(res2)
invisible(dev.off())

cat("Little publication bias, but let's see trim-and-fill's suggestions anyway:\n")
tf <- trimfill(res1); tf

png(file="~/wiki/images/dnb/funnel-trimfill.png", width = 580, height = 580)
funnel(tf)
invisible(dev.off())

# optimize the generated graphs by cropping whitespace & losslessly compressing them
system(paste('cd ~/wiki/images/dnb/ &&',
             'for f in *.png; do convert "$f" -crop',
             '`nice convert "$f" -virtual-pixel edge -blur 0x5 -fuzz 10% -trim -format',
             '\'%wx%h%O\' info:` +repage "$f"; done'))
system("optipng -o9 -fix ~/wiki/images/dnb/*.png", ignore.stdout = TRUE)
~~~
