---
title: When Does The Mail Come?
description: Bayesian & max-likelihood analysis of local mail delivery times
tags: statistics
created: 21 June 2015
status: in progress
belief: possible
...

> Mail is delivered by the USPS mailman at a regular but not observed time; what is observed is whether the mail has been delivered at a time, yielding somewhat-unusual "interval-censored data".
> I describe the problem of estimating when the mailman delivers, write a simulation of the data-generating process, and demonstrate analysis of interval-censored data in R using maximum-likelihood (the survival library), MMC (JAGS), and likelihood-free Bayesian inference (ABC using the simulation). This allows estimation of when to check the mailbox with a certain probability it will be there such as 95%.
> Finally, I compare those estimates from the interval-censored data with estimates from a (smaller) set of exact delivery-times provided by USPS tracking, using a multilevel model to deal with heterogeneity apparently due to a change in USPS routes/postmen.

Consider a question of burning importance: what time does the mailman come, bearing gifts?

# Interval-censored data

No one wants to sit around all morning to spot the *exact* time the mailman comes. At least, I don't.

We could more easily measure by going out in the morning at a random time to see if the mail has come yet, and then (somehow) estimate.

Given a set of data like "2015-06-20 11:00AM: mail has not come yet; 2015-06-21 11:59AM: mail had come", how can we estimate?
This is not a normal setup where we estimate a mean but our data is interestingly messed up: censored or truncated or an interval somehow.

[Survival analysis](!Wikipedia) seems like the appropriate paradigm.
This is not a simple survival analysis with "right-censoring" where each individual is followed up to a censoring time and the exact time of 'failure' is observed.
(This would be right-censoring if instead we had gone out to the mailbox early in the morning and sat there waiting for the mailman to record when she came, occasionally getting bored around 10AM or 11AM and wandering off without seeing when the mail comes.)
This isn't "left-censoring" either (for left-censoring, we'd go out to the mailbox late in the morning when the mail might already be there, and if it isn't, then wait until it does come).
I don't think this is left or right truncation either, since each day data is collected and there's no sampling biases at play.
What this is is interval censoring: when we go out to the mailbox at 11AM and discover the mail is there, we learn that the mail was delivered today sometime in the interval midnight-10:59AM, or if the mail isn't there, we learn it will be delivered later today sometime during the interval 11:01AM-midnight (hopefully closer to the first end than the second).
Interval censoring comes up in biostatistics for situations like periodic checkups for cancer, which does resemble our mail situation.

## ML

The R [`survival` library](http://cran.r-project.org/web/packages/survival/index.html) supports the usual right/left-censoring but also the interval-censoring.
It supports two encodings of intervals, `interval` and `interval2`[^survival-interval-encoding]; I use the former, which format works well with both the `survival` library and also other tools like JAGS.
Times are written as minutes since midnight, so they can be handled as positive numbers rather than date-times (ie midnight=0, 11AM=660, noon=720, midnight=1440, etc), and the upper and lower bounds on intervals are 0 and 1440 (so if I check the mail at 660 and it's there, then the interval is 0-660, and if it's not, 660-1440).

[^survival-interval-encoding]: From the documentation:

    > `Surv(time, time2, event, type="interval")`
    >
    > 1. `time`: For interval data, the first argument is the starting time for the interval.
    > 2. `time2`: ending time of the interval for interval censored or counting process data only. Intervals are assumed to be open on the left and closed on the right, `(start, end]`. For counting process data, event indicates whether an event occurred at the end of the interval.
    > 3. `event`: The status indicator, normally 0=alive, 1=dead....For interval censored data, the status indicator is 0=right censored, 1=event at time, 2=left censored, 3=interval censored.
    >
    > Interval censored data can be represented in two ways. For the first use `type = "interval"` and the codes shown above. In that usage the value of the `time2` argument is ignored unless `event=3`. The second approach is to think of each observation as a time interval with $({-\infty}, t)$ for left censored, $(t, \infty)$ for right censored, $(t,t)$ for exact and $(t_1, t_2)$ for an interval. This is the approach used for `type = "interval2"`. Infinite values can be represented either by actual infinity (`Inf`) or `NA`. The second form has proven to be the more useful one.
    >
    > ...a subject's data for the pair of columns in the dataset `(time1, time2)` is $(t_e, t_e)$ if the event time $t_e$ is known exactly; $(t_l, \text{NA})$ if right censored (where $t_l$ is the censoring time); and $(t_l, t_u)$ if interval censored (where $t_l$ is the lower and $t_u$ is the upper bound of the interval).

~~~{.R}
set.seed(2015-06-21)
# simulate a scenario in which the mailman tends to come around 11AM (660) and I tend to check around then,
# & generate interval data for each time, bounded by end-of-day/midnight below & above, collecting ~1 month:
simulateMailbox <- function(n, time) {
    deliveryTime <- round(rnorm(n, mean = time, sd = 30))
    checkTime <- round(rnorm(n, mean = time, sd = 60))
    simulates <- mapply(function (ck, dy) { if(ck>dy) { return(c(0,ck)) } else { return(c(ck,1440)) }},
                         checkTime, deliveryTime)
    return(data.frame(Time1=simulates[1,], Time2=simulates[2,])) }
mailSim <- simulateMailbox(30, 660); mailSim
##        Time1 Time2
## 1    569  1440
## 2      0   664
## 3    592  1440
## 4    581  1440
## 5    596  1440
## 6      0   703
## 7      0   705
## 8    667  1440
## ...
library(ggplot2)
png(file="~/wiki/images/maildelivery-simulated.png", width = 800, height = 500)
ggplot(mailSim) + geom_segment(aes(x=Time1, xend=Time2, y=1:nrow(mailSim), yend=1:nrow(mailSim))) +
    geom_vline(xintercept=660, color="blue") + ylab("Day") + xlab("Time")
invisible(dev.off())
~~~

Inferring the mean time of delivery might sound difficult with such extremely crude data of intervals 700 minutes wide or worse, but plotting the little simulated dataset and marking the true mean time of 660, we see it's not *that* bad - the mean time is probably whatever line passes through the most intervals:

![The simulated overlapping-intervals data, with the true mean time drawn in blue](/images/maildelivery-simulated.png)

And also with our simulated dataset, we can see if the standard R survival library and a interval-censored model written in JAGS can recover the 660:

~~~{.R}
library(survival)
surv <- Surv(mailSim$Time1, mailSim$Time2, type="interval2")
s <- survfit(surv ~ 1, data=mailSim); summary(s)
##  time   n.risk   n.event survival   std.err lower 95% CI upper 95% CI
## 633.0 30.00000  7.503147 0.749895 0.0790680     0.609889     0.922041
## 651.0 22.49685 12.188431 0.343614 0.0867071     0.209546     0.563458
## 663.5 10.30842  0.611086 0.323245 0.0853927     0.192604     0.542496
## 685.0  9.69734  9.697336 0.000000       NaN           NA           NA
plot(s)
## https://i.imgur.com/nzOHQT0.png
sr <- survreg(surv ~ 1, dist="gaussian", data=mailSim); summary(sr)
##                 Value Std. Error        z           p
## (Intercept) 656.21819   7.324758 89.58906 0.00000e+00
## Log(scale)    2.93664   0.444159  6.61169 3.79957e-11
##
## Scale= 18.8524
##
## Gaussian distribution
## Loglik(model)= -7.1   Loglik(intercept only)= -7.1
## Number of Newton-Raphson Iterations: 10
## n= 30
~~~

## MCMC

More Bayesianly, we can write an interval-censoring model in JAGS, which gives us the opportunity to use an informative prior about the mean time the mailman comes.

They work normal 9-5 hours as far as I know, so we can rule out anything outside 540-1020.
From past experience, I expect the mail to show up not before 10AM (600) and not after 1PM (780), with those extremes being rare and sometime around 11AM (660) being much more common; so not a uniform distribution over 600-780 but a normal one centered on 660 and then somewhat arbitrarily saying that 600-700 represent 3 SDs out from the mean of delivery times to get SD=~30 minutes so in all, `dnorm(660, pow(30, -2))`.
The SD itself seems to me like it could range anywhere from a few minutes to an hour, but much beyond that is impossible (if the SD was over an hour, then every so often the mailman would have to come at 8AM! and if it was smaller than 10 minutes, then I would never have noticed much variation in the first place).

~~~{.R}
library(R2jags)
model1 <- "model { for (i in 1:n){
           y[i] ~ dinterval(t[i], dt[i,])
           t[i] ~ dnorm(mu,tau)
           }

           mu ~ dnorm(660, pow(30, -2))
           sd ~ dunif(10, 60)
           tau <- pow(1/sd, 2)

           y.new ~ dnorm(mu, tau)
           }"
# y=1 == Event=3 for `Surv`: event is hidden inside interval, not observed/left-/right-censored
data <- list("dt"=mailSim, "n"=nrow(mailSim), "y"=rep(1, nrow(mailSim)))
inits <- function() { list(mu=rnorm(1),sd=30,t=as.vector(apply(mailSim,1,mean))) }
params <- c("mu", "sd", "y.new")
j1 <- jags(data,inits, params, textConnection(model1)); j1
##          mu.vect sd.vect    2.5%     25%     50%     75%   97.5%  Rhat n.eff
## mu       657.855   9.362 640.022 651.823 657.567 663.866 676.540 1.003  1100
## sd        28.226  10.859  12.033  20.051  26.412  34.750  54.141 1.068    35
## y.new    656.872  32.354 592.156 638.315 656.290 675.344 721.606 1.005  3000
## deviance   0.000   0.000   0.000   0.000   0.000   0.000   0.000 1.000     1
~~~

Both approaches' point-value mean time of 656/657 (10:54AM) come close to the true simulation value of 660 (11AM) and the prediction interval of 592-721 also sounds right, validating the models.
The estimated standard deviation isn't as accurate with a wide credible interval, reflecting that it's a harder parameter to estimate and the estimate is still vague with only _n_=30.

~~~{.R}
mailSim <- simulateMailbox(180, 660); mailSim

# http://cran.r-project.org/web/packages/animation/index.html
library(animation)
saveGIF(
    for(n in 1:nrow(mailSim)){
        data <- list("dt"=mailSim[1:n,], "n"=nrow(mailSim[1:n,]), "y"=rep(1, nrow(mailSim[1:n,])))
        inits <- function() { list(mu=rnorm(1),sd=30,t=as.vector(apply(mailSim[1:n,],1,mean))) }
        params <- c("mu","sd", "y.new")
        j1 <- jags(data, inits, params, textConnection(model1), progress.bar="none")

        lowerMean <- j1$BUGSoutput$summary[c(2),][3]
        medianMean  <- j1$BUGSoutput$mean$mu
        upperMean <- j1$BUGSoutput$summary[c(2),][7]

        lowerPredictive <- j1$BUGSoutput$summary[c(4),][3]
        upperPredictive <- j1$BUGSoutput$summary[c(4),][7]

        confintTrue <- round(qnorm(c(0.025, 0.975), mean=660, sd=30))
        lowerPredictiveTrue <- confintTrue[1]
        upperPredictiveTrue <- confintTrue[2]

        # need an environment call for `ggplot` inside a function: http://stackoverflow.com/a/29595312/329866
        p <- ggplot(mailSim[1:n,]) +
              coord_cartesian(xlim = c(7*60, 13*60)) +
              ylab("Day") + xlab("Time") +
              geom_segment(aes(x=mailSim[1:n,]$Time1, xend=mailSim[1:n,]$Time2, y=1:n, yend=1:n)) +
              geom_vline(xintercept=medianMean, color="blue") +
              geom_vline(xintercept=lowerMean, color="green") +
              geom_vline(xintercept=upperMean, color="green") +
              geom_vline(xintercept=lowerPredictive, color="red") +
              geom_vline(xintercept=upperPredictive, color="red") +
              geom_vline(xintercept=lowerPredictiveTrue, color="red4") +
              geom_vline(xintercept=upperPredictiveTrue, color="red4")
        print(p)
        },
    interval = 0.7, ani.width = 800, ani.height=800,
    movie.name = "/home/gwern/wiki/images/mail-simulated-inferencesamplebysample.gif")
~~~

With a simulation and JAGS set up, we could also see how the posterior estimates of the mean, 95% CI of the mean, and predictive interval change as an additional datapoint is added:

![Simulated data: posterior estimates evolving sample by sample](/images/mail-simulated-inferencesamplebysample.gif)

Probably the most striking aspect of watching these summaries updated datum by datum, to me, is how the estimate of the mean homes in almost immediately on close to the true value (this isn't due solely to the informative prior either, as with a completely uninformative `dunif(0,1440)` will zoom in within 4 or 5 datapoints as well, after some violent thrashing around).
What happens is that the intervals initially look uninformative if the first two or three all turn out to be delivered/non-delivered and so the mean delivery time could still be anywhere from ~11:AM-midnight or vice-versa, but then as soon as even one needle falls the other way, then the mean suddenly snaps into tight focus and gets better from there.
While I understand this abrupt transition in hindsight (only a tiny subset of values around the overlapping tips of the needles can yield the observed flip-flops of delivery/non-delivery, while a mean time far from the tips would yield a completely consistent dataset of all deliveries/non-deliveries), I didn't expect this, simply reasoning that "one interval-censored datum seems very uninformative, so it must take many data to yield any sort of decent result for the mean & standard deviation and hence the predictions".
But, even as the mean becomes precisely estimated, the predictive interval - which is what, in the end, we really care about - remains obdurate and broad, because we assume the delivery time is generated by a normal distribution and so the predicted delivery times are the product of not just the mean but the standard deviation as well, and the standard deviation is hard to estimate (a 95% credible interval of 12-51!).
Also in hindsight this seems obvious as well, since the flip-flopping needles may be sensitive to the mean, but not to the spread of delivery-times; the data would not look much different than it does if the mailman could deliver anywhere from 8AM to 3PM - on early days she delivers a few hours before I check the mailbox around 11AM and on late days she delivers a few hours after.

TODO: seems that the predictive interval hits diminishing returns after n=50 or so

These considerations also raise questions about statistical power/optimal experiment design: what are the best times to sample from interval-censored data in order to estimate as precisely as possible with a limited budget of samples?
I searched for material on interval-censored data but didn't find anything directly addressing my question.
The flip-flops suggest that to estimate the mean, one should sample only at the current estimated mean, which maximizes the probability that there will be a net 50-50 split of delivery/non-delivery; but where should one sample for the SD as well?

~~~{.R}
set.seed(2015-06-24)
library(lubridate)
clockS <- function(t){hour(t)*60 + minute(t) + second(t)/60}

mail <- data.frame(Time=clockS(as.POSIXct(c("2015-06-20 11:00", "2015-06-21 11:06", "2015-06-23 11:03",
                                 "2015-06-24 11:05", "2015-06-25 11:00", "2015-06-26 10:56",
                                 "2015-06-27 10:45", "2015-06-29 10:31", "2015-06-30 10:39", "2015-07-01 10:27",
                                 "2015-07-02 10:47", "2015-07-03 10:27", "2015-07-04 10:54", "2015-07-05 10:55",
                                 "2015-07-06 11:21", "2015-07-07 10:01", "2015-07-08 10:20", "2015-07-09 10:50"),
                                "EDT")),
    Delivered=c(FALSE, FALSE, TRUE, TRUE, TRUE, TRUE, TRUE, FALSE, TRUE, FALSE, TRUE, FALSE, FALSE, FALSE, TRUE, FALSE, FALSE, FALSE))

mail <- data.frame(Time1 = ifelse(mail$Delivered, 0, mail$Time),
                           Time2 = ifelse(mail$Delivered,  mail$Time, 1440))

library(R2jags)
model1 <- "model { for (i in 1:n){
           y[i] ~ dinterval(t[i], dt[i,])
           t[i] ~ dnorm(mu,tau)
           }

           mu ~ dnorm(660, pow(30, -2))
           sd ~ dunif(10, 60)
           tau <- pow(1/sd, 2)

           y.new ~ dnorm(mu, tau)
           }"

data <- list("dt"=mail, "n"=nrow(mail), "y"=rep(1, nrow(mail)))
inits <- function() { list(mu=rnorm(1),sd=30,t=as.vector(apply(mail,1,mean))) }
params <- c("mu","sd", "y.new")
j1 <- jags(data, inits, params, textConnection(model1), n.iter=100000); j1
##          mu.vect sd.vect    2.5%     25%     50%     75%   97.5%  Rhat n.eff
## mu       643.770  16.440 609.812 633.338 644.257 654.347 674.943 1.001  3000
## sd        41.072  12.335  15.849  31.725  42.366  51.450  59.173 1.001  3000
## y.new    643.003  45.034 550.865 615.279 643.776 671.716 730.792 1.001  3000
## deviance   0.000   0.000   0.000   0.000   0.000   0.000   0.000 1.000     1

lowerMean <- j1$BUGSoutput$summary[c(2),][3]
medianMean  <- j1$BUGSoutput$mean$mu
upperMean <- j1$BUGSoutput$summary[c(2),][7]

lowerPredictive <- j1$BUGSoutput$summary[c(4),][3]
# medianMean ~= predictive mean in this case, so don't bother
upperPredictive <- j1$BUGSoutput$summary[c(4),][7]

reformat <- function (time) {
   paste0(as.character(round((time %/% 60))), ":",  as.character(round((time %% 60)))) }
sapply(c(lowerMean, medianMean, upperMean), reformat)
sapply(c(lowerPredictive, upperPredictive), reformat)

library(ggplot2)

png(file="~/wiki/images/maildelivery-real.png", width = 900, height = 600)
ggplot(mail) + geom_segment(aes(x=Time1, xend=Time2, y=1:nrow(mail), yend=1:nrow(mail))) +
    ylab("Day") + xlab("Time") +
    geom_vline(xintercept=medianMean, color="blue") +
    geom_vline(xintercept=lowerMean, color="green") +
    geom_vline(xintercept=upperMean, color="green") +
    geom_vline(xintercept=lowerPredictive, color="red") +
    geom_vline(xintercept=upperPredictive, color="red")
invisible(dev.off())
~~~

![Overlapping-intervals data, with the estimated mean time in blue, 95% CI around the mean time in green, and 95% predictive intervals as to when the delivery is made](/images/maildelivery-real.png)

Posterior predictive checks (PPC) are a technique recommended by Gelman as a test of how appropriate the model is (as the model or likelihood often is far more important in forcing particular results than the priors one might've used), where one generates possible observations from the model and inferred posteriors, and sees how often the simulated data matches the real data; if that is rarely or never, then the model may be bad and one should fix it or possibly throw in additional models for Bayesian model comparison (which will hopefully then pick out the right model).
For continuous data or more than trivially small data, we have the same issue as in ABC (which is similar to PPCs): the simulates will never exactly match the data, so we must define some sort of summary or distance measure which lets us say that a particular simulated dataset is similar enough to the real data.
In the two summaries I tried for ABC, counting 'flip-flops' worked better, so I'll reuse that here.

~~~{.R}
posteriorTimes <- j1$BUGSoutput$sims.list[["y.new"]]

simsPPC <- replicate(10000, {
    n <- nrow(mail)
    deliveryTime <- sample(posteriorTimes, size=n, replace=TRUE)
    checkTime <- round(rnorm(n, mean = 660, sd = 30))
    newSamples <- mapply(function (ck, dy) { if(ck>dy) { return(c(0,ck)) } else { return(c(ck,1440)) }},
                         checkTime, deliveryTime)
    newData <- data.frame(Time1=newSamples[1,], Time2=newSamples[2,])

    return(sum(newData$Time1 == 0))
    }
)

qplot(simsPPC) + geom_vline(xintercept=sum(mail$Time1 == 0), color="blue")
~~~

The true data has a summary near the middle of the distribution of the summaries of the posterior predictions, suggesting that the model fit to the problem is not terrible at the moment.

### Optimal mail checking time

With a posterior distribution of delivery times (`y.new`) we can try to find an optimal time to check, assuming we can figure out a [loss function](!Wikipedia) giving a cost of checking at each possible time.
The loss here can be considered in units of time; if I didn't care about the delay between the package being delivered and me getting my hands on it, I would simply check at 2PM and have done with it.
The loss function itself is not obvious; squared loss, absolute loss and 0-1 loss don't correspond to my process of checking mail.

#### Defining a loss function

Expecting a package wears on my mind and in some cases, I'd like to start using the contents ASAP; I see a delay of 10 minutes as being twice as bad as 5 minutes and not 4 times as bad, so this part is an absolute loss. (If it arrives at 11AM and I check at 11:30AM, then the loss due to the delay is 30.)
Besides the delay, there's also the cost of walking all the way down to the mailbox and back, which due to the long driveway is around 10 minutes, but it's mostly unshaded and hot out and an interruption, so the loss is much greater than that; introspecting, I would be willing to wait at least 60 minutes to save one roundtrip, so I will define the cost of a mail check at 60 'minutes'.
So suppose I check at 10:30AM, the mail comes at 11AM, and I check again at 11:30AM and find it; then the cost is 60+60+(11:30AM-11:00AM) = 150, while if I had checked at 11:20AM then the cost is better and smaller at just 60+(11:20AM-11:00AM) = 70.

OK, so if the package is there, that's easy, but what if I walk out at 11:20AM and it's *not* there?
In a proper [Bayesian search theory](!Wikipedia) application, if I check once, I would then update my posterior and run a loss function again to decide when to check next, but this is impracticable for daily usage and in reality, what I would do is if the first check didn't turn up the package, I would then give up in frustration & disgust and not come back until ~1PM (780) when the package would definitely have arrived. Then I'd incur a loss of two walks and possibly a long wait until 1PM.

All these considerations give me a weird-looking but nevertheless realistic loss function: if the package is delivered at _d_ and we check at a particular time _t_, then if _t_>_d_ and the package had arrived we incur a total loss of $60+(t-d)$; otherwise, we check back a second and final time at 1PM, incurring a total loss of $60+60+(780-d)$

#### Finding the optimum

Having figured that out, we run the loss function on each sample from the posterior, averaging over all weighted possible delivery times, and find what time of day minimizes the loss:

~~~{.R}
posteriorTimes <- j1$BUGSoutput$sims.list[["y.new"]]
lossFunction <- function(t, walk_cost, predictions, lastResortT) {
    mean(sapply(predictions,
                function(delivered) { if (delivered<t) { return(walk_cost   + (t           - delivered)); } else
                                                       { return(2*walk_cost + (lastResortT - t) ); }}))
    }
losses <- sapply(c(0:1440), function (tm) { lossFunction(tm, 60, posteriorTimes, 780);})
which.min(losses)
# [1] 696
## 11:36AM
qplot(0:1440, losses)
~~~

We can compare the cost of check times. If I check at 11:28AM rather than the current estimated optimal time of 11:38AM, then the loss each time is -2.1:

~~~{.R}
losses[698]
# [1] 125.1687367
losses[688]
# [1] 127.2646144
losses[698] - losses[688]
# [1] -2.095877679
~~~

With average losses per check time, an estimate of number of check times per year, and a discount rate, we can derive a [net present value](!Wikipedia); I usually use the approximation $\frac{\text{gain}}{ln(1 + \text{discount rate})}$ and a discount rate of 5% annually, but in this case assuming an indefinite horizon is wrong - the delivery time will change and the problem will reset as soon as the next mailman begins this route, which has already happened once in the 4 years thus far, so I instead assume that any estimate is worthless after 3 years (the mailman will have changed, the route or USPS will have changed, I may not even be living in the same place in 3 years).
So if I check for a package 30 times a year (which is roughly as much as I did in 2014 & am on track to do in 2015), and I do so suboptimally each time and incur a loss of -2, then the total loss is equivalent to

~~~{.R}
netPresentValue <- function(p) { (p / (1 + 0.05)^1) + (p / (1 + 0.05)^2) + (p / (1 + 0.05)^3)  }
netPresentValue(losses[698]*30 - losses[688]*30)
## [1] -48.95181657
~~~

This is similar to the "[expected value of perfect information](!Wikipedia)": if I estimated the optimal time as t=688 and it was actually t=698, I should be willing to pay up to 49 'minutes' to learn the truth and instead start checking the mail at t=698.

Perfect information is not available at any cost, however.^[I could always resort to additional technology like motion-activated cameras or radio bugs on the mailbox which send alerts when it's been opened if I *really* wanted, but for the purpose of this exercise, let's ignore those possibilities.]
All I can do is go out to the mail box _n_ times, and as we saw in the simulation, diminishing returns always happens and at some point the predictive intervals stop changing noticeably.

So the question is what is the "[expected value of sample information](!Wikipedia)" (EVSI): what is the value of collecting one more additional datapoint, which can help pin down the optimal time a little more precisely and reduce the loss from a suboptimal time?
EVSI can be defined as "expected value of best decision with some additional sample information" minus "expected value of best current decision".
More specifically, if many times we draw a sample from the posterior (which represents our best estimate of the world), turn it into a new interval-censored datapoint, create an updated posterior distribution incorporating that datapoint, run the loss function again & calculate a new optimal check time, and compute the improvement of the new check time's NPV over the old check time's NPV to get the EVSI, what is the mean of the EVSIs?

~~~{.R}
sampleValues <- NULL
sampleValuesProfit <- NULL
for (i in seq(from=1, to=30)) {

evsis <- replicate(100, {
    n <- i
    deliveryTime <- sample(posteriorTimes, size=n, replace=TRUE)
    checkTime <- round(rnorm(n, mean = 660, sd = 30))
    newSamples <- mapply(function (ck, dy) { if(ck>dy) { return(c(0,ck)) } else { return(c(ck,1440)) }},
                             checkTime, deliveryTime)
    newSamples <- data.frame(Time1=newSamples[1,], Time2=newSamples[2,])
    newData <- rbind(mail, newSamples)

    dataEVSI <- list("dt"=newData, "n"=nrow(newData), "y"=rep(1, nrow(newData)))
    initsEVSI <- function() { list(mu=rnorm(1),sd=15,t=as.vector(apply(newData,1,mean))) }
    paramsEVSI <- c("y.new")
    jEVSI <- jags(dataEVSI, initsEVSI, paramsEVSI, textConnection(model1), n.iter=100, progress.bar="none")
    # jEVSI

    posteriorTimesEVSI <- jEVSI$BUGSoutput$sims.list[["y.new"]]
    lossesEVSI <- sapply(c(0:1440), function (tm) { lossFunction(tm, 60, posteriorTimesEVSI, 780);})
    # which.min(lossesEVSI)

    newOptimum <- netPresentValue(min(lossesEVSI) * 30)
    oldOptimum <- netPresentValue(min(losses)     * 30)
    EVSI <- newOptimum - oldOptimum
    return(EVSI)
    }
    )

    print(i)

    print(mean(evsis))
    sampleValues[i] <- mean(evsis)

    print(mean(evsis - (i*60)))
    sampleValuesProfit[i] <- mean(evsis - (i*60))
}
sampleValues
##  [1]  621.9099624  676.3511205  711.6662485  828.0513308  771.5025080  758.5178942  883.5654301
##  [8]  878.3814885  792.5198993  935.5268629  924.4320994  961.1936763  895.4424611 1024.0913322
## [15] 1091.0730761 1007.0386573 1043.8139893 1107.7887113 1039.2301437 1070.4069060  979.9023277
## [22] 1071.9631383 1070.6226339 1067.4642958 1132.5503132 1048.9637609 1001.1076490 1239.8961317
## [29] 1079.4585210 1083.7775260
sampleValuesProfit
##  [1]  561.90996235  556.35112053  531.66624852  588.05133078  471.50250798  398.51789419
##  [7]  463.56543005  398.38148850  252.51989929  335.52686294  264.43209935  241.19367629
## [13]  115.44246111  184.09133217  191.07307609   47.03865732   23.81398930   27.78871132
## [19] -100.76985626 -129.59309403 -280.09767225 -248.03686170 -309.37736610 -372.53570425
## [25] -367.44968683 -511.03623905 -618.89235097 -440.10386832 -660.54147903 -716.22247398
~~~

## ABC

Because JAGS provides an interval-censored distribution in the form of `dinterval()` with a [likelihood function](!Wikipedia), we can use MCMC for inverse inference (reasoning from data to the underlying process)
But if it didn't, I wouldn't know how to write one down for it and then the MCMC wouldn't work; but I was able to write a little simulation of how the underlying process of delivery-and-checking works, which, given a set of parameters, spits out simulated results generated by the process, which is probability or forward inference (reasoning from a version of an underlying process to see what it creates).
This is a common situation: you can write a good simulation simply by describing how you think something works, but you can't write a likelihood function.

[ABC](!Wikipedia "Approximate Bayesian computation") (exemplified in the fun example ["Tiny Data, Approximate Bayesian Computation and the Socks of Karl Broman"](http://www.sumsar.net/blog/2014/10/tiny-data-and-the-socks-of-karl-broman/); see also Wilkinson's [intro](https://darrenjw.wordpress.com/2013/03/31/introduction-to-approximate-bayesian-computation-abc/ "Introduction to Approximate Bayesian Computation (ABC)") & [summary statistics](https://darrenjw.wordpress.com/2013/09/01/summary-stats-for-abc/ "Summary stats for ABC") posts) is a remarkably simple and powerful idea which lets us take a forward simulation and use it to run backwards inference.
(Reminds me a little of [Solomonoff induction](!Wikipedia "Solomonoff's theory of inductive inference").)

The simplest ABC goes like this:
You sample possible parameters from your prior, feed the set of parameters into your simulation, and if the result is identical to your data, you save that set of parameters.
At the end, you're left with a bunch of sets and that's your posterior distribution which you can look at the histograms of and calculate 95% densities etc.

So for the mail data, ABC goes like this:

~~~{.R}
simulateMailbox <- function(n, dTime, dSD) {
    deliveryTime <- round(rnorm(n, mean = dTime, sd = dSD))
    checkTime <- round(rnorm(n, mean = dTime, sd = dSD))
    simulates <- mapply(function (ck, dy) { if(ck>dy) { return(c(0,ck)) } else { return(c(ck,1440)) }},
                         checkTime, deliveryTime)
    return(data.frame(Time1=simulates[1,], Time2=simulates[2,])) }

# if both dataframes are sorted, comparison is easier
mailSorted <- mail[order(mail$Time1),]

mail_sim <- replicate(100000, {
    # mu ~ dnorm(660, 30)
    mu <- rnorm(n=1, mean=660, sd=30)
    # sd ~ dunif(10, 60)
    sd <- runif(n=1, min=10, max=60)

    newData <- simulateMailbox(nrow(mailSorted), mu, sd)
    newDataSorted <- newData[order(newData$Time1),]

    if (all(newDataSorted == mailSorted)) { return(c(Mu=mu, SD=sd)) }
   }
  )
results <- Filter(function(x) {!is.null(x)}, mail_sim)
results <- data.frame(t(sapply(results,c)))
summary(results)
~~~

TODO: parallelize, print progress

The first thing to note is efficiency: I can get reasonable number of samples in reasonable amount of time for _n_=1-3, but at 4 datapoints, it becomes slow.
There's so many possible datasets when 4 checks are simulated that almost all get rejected because they are not identical to the real dataset and it takes millions of samples and hours to run.
And this problem only gets worse for _n_=5 and bigger.

To run ABC more efficiently, you relax the requirement that the simulated data == real data and instead accept the pair of parameters if the simulated data is 'close enough' in some sense to the real data, close in terms of some summary statistic (hopefully sufficient) like the mean.
I don't know what are the sufficient statistics for a set of interval-censored data, but I figure that if the means of the pairs of times are similar in both datasets, then they are probably close enough for ABC to work, so I can use that as a rejection tolerance; implementing that and playing around, it seems I can make the difference in means as tight as <2 while still running fast.

~~~{.R}
mail_abc <- function(samples) {
    results <- list()
    n <- 0
    while (n<samples) {

      # Priors:
      ## mu ~ dnorm(660, 30)
      mu <- rnorm(n=1, mean=660, sd=30)
      ## sd ~ dunif(10, 60)
      sd <- runif(n=1, min=10, max=60)

      # generate new data set based on a pair of possible parameters:
      newData <- simulateMailbox(nrow(mail), mu, sd)

      # see if some summaries of the new data were within tolerance ϵ<2 of the real data:
      if (abs(mean(newData$Time1) - mean(mail$Time1)) < 2 &&
          abs(mean(newData$Time2) - mean(mail$Time2)) < 2)
        { results <- list(c(Mu=mu, SD=sd), results); n <- n+1; }
    }
   return(results)
  }
sims <- mail_abc(1000)
results <- matrix(unlist(sims), ncol=2, byrow=TRUE)
summary(results)
##       V1                 V2
## Min.   :621.0812   Min.   :10.00167
## 1st Qu.:644.7149   1st Qu.:15.71872
## Median :648.0593   Median :20.57838
## Mean   :652.3028   Mean   :23.42996
## 3rd Qu.:651.2694   3rd Qu.:28.52015
## Max.   :739.4133   Max.   :59.89326
~~~

The mean value here is somewhat acceptable but the SD is considerably off from the JAGS estimate (which I assume to be correct).
Post hoc, this makes some sense since my summary statistic *is* just means; it might make more sense to check the SD of each column as well (at the cost of more runtime).

Another way to summarize the dataset occurs to me while looking at the graphs: the most striking visual feature of the interval-censored data is how the 'needles' overlap slightly and it is this slight overlap which determines where the mean is; the most informative set of data would be balanced exactly between needles that fall to the left and needles that fall to the right, leaving as little room as possible for the mean to 'escape' out into the wider intervals and be uncertain.
(Imagine a set of data where all the needles fall to the left, because I only checked the mail at 2PM; I would then be extremely certain that the mail is not delivered after 2PM but I would have little more idea than when I started about when the mail is actually delivered in the morning and my posterior would repeat the prior.)
So I could use the count of left or right intervals (it doesn't matter if I use `sum(Time1 == 0)` or `sum(Time2 == 1440)` since they are mutually exclusive) as the summary statistic.

~~~{.R}
mail_abc <- function(samples) {
    results <- list()
    n <- 0
    while (n<samples) {

      # Priors:
      ## mu ~ dnorm(660, 30)
      mu <- rnorm(n=1, mean=660, sd=30)
      ## sd ~ dunif(10, 60)
      sd <- runif(n=1, min=10, max=60)

      # generate new data set based on a pair of possible parameters:
      newData <- simulateMailbox(nrow(mail), mu, sd)

      # see if a summary of the new data matches the old:
      if (sum(mail$Time1 == 0) == sum(newData$Time1 == 0))
        { results <- list(c(Mu=mu, SD=sd), results); n <- n+1; }
    }
   return(results)
  }
sims <- mail_abc(10000)
results <- matrix(unlist(sims), ncol=2, byrow=TRUE)
summary(results)
##       V1                 V2
## Min.   :550.4086   Min.   :10.00066
## 1st Qu.:640.0134   1st Qu.:22.33501
## Median :660.2284   Median :34.76427
## Mean   :660.2783   Mean   :34.83866
## 3rd Qu.:680.5165   3rd Qu.:47.45385
## Max.   :760.6862   Max.   :59.99828
~~~

This summary, simple as it is, does better in replicating the JAGS estimates.

# Exact delivery-time data

Halfway through compiling my notes, I realized that I *did* in fact have several exact times for deliveries: the USPS tracking emails for packages, while useless on the day of delivery for knowing when to check (since the alerts are only sent around 3-4PM), do include the exact time of delivery that day.
And then while recording interval data, I did sometimes spot the mailman on her rounds; to keep things simple, I still recorded it as an interval.

Exact data makes estimating a mean & SD trivial:

~~~{.R}
set.seed(2015-06-25)
library(lubridate)
clockS <- function(t){hour(t)*60 + minute(t) + second(t)/60}

mailExact <- data.frame(Date=as.POSIXct(c("2010-04-29 11:33AM", "2010-05-12 11:31AM", "2014-08-20 12:14PM",
                                          "2014-09-29 11:15AM", "2014-12-15 12:02PM", "2015-03-09 11:19AM",
                                          "2015-06-10 10:34AM", "2015-06-20 11:02AM", "2015-06-23 10:58AM",
                                          "2015-06-24 10:53AM", "2015-06-25 10:55AM", "2015-06-30 10:36AM",
                                          "2015-07-02 10:45AM", "2015-07-06 11:19AM"),
                        "EDT"))
mailExact$TimeDelivered <- clockS(mailExact$Date)
##                   Date TimeDelivered Group
## 1  2010-04-29 11:33:00           693 FALSE
## 2  2010-05-12 11:31:00           691 FALSE
## 3  2014-08-20 12:14:00           734 FALSE
## 4  2014-09-29 11:15:00           675 FALSE
## 5  2014-12-15 12:02:00           722 FALSE
## 6  2015-03-09 11:19:00           679  TRUE
## 7  2015-06-10 10:34:00           634  TRUE
## 8  2015-06-20 11:02:00           662  TRUE
## 9  2015-06-23 10:58:00           658  TRUE
## 10 2015-06-24 10:53:00           653  TRUE
## 11 2015-06-25 10:55:00           655  TRUE
mean(mailExact$TimeDelivered)
## [1] 677.8181818
sd(mailExact$TimeDelivered)
## [1] 30.36714732
library(ggplot2); qplot(Date, TimeDelivered, data=mailExact)
## https://i.imgur.com/oJlGp6t.png
~~~

Plotted over time, there's a troubling amount of heterogeneity: despite the sparsity of data (apparently I did not usually bother to set up USPS tracking alerts 2011-2014, to my loss) it's hard not to see two separate clusters there.

## ML

Besides the visual evidence, a _t_-test agrees with there being a difference between 2010-2014 and 2015

~~~{.R}
mailExact$Group <- year(mailExact$Date) > 2014
t.test(TimeDelivered ~ Group, data=mailExact)
##
##  Welch Two Sample t-test
##
## data:  TimeDelivered by Group
## t = 3.7348635, df = 6.3085711, p-value = 0.008834452
## alternative hypothesis: true difference in means is not equal to 0
## 95% confidence interval:
##  16.27540251 76.05793082
## sample estimates:
## mean in group FALSE  mean in group TRUE
##         703.0000000         656.8333333
mean(mailExact[mailExact$Group,]$TimeDelivered)
## [1] 656.8333333
sd(mailExact[mailExact$Group,]$TimeDelivered)
## [1] 14.55220487
~~~

Why might there be two clusters?
Well, now that I think about it, I recall that my mailman used to be an older gentleman with white hair (I remember him vividly because in mid-August 2013 a package of fish oil was damaged in transit, and he showed up to explain it to me and offer suggestions on returns; Amazon didn't insist on a leaky fish oil bottle being shipped back and simply sent me a new one).
But now my mailman is a younger middle-aged woman. That seems like a good reason for a shift in delivery times (perhaps she drives faster).

## MCMC

Estimating the two distributions separately in a simple multilevel/hierarchical model:

~~~{.R}
library(R2jags)
model2 <- "model{
  # I expect all deliveries ~11AM/660:
  grand.mean ~ dnorm(660, pow(30, -2))

  # different mailman/groups will deliver at different offsets, but not by more than 2 hours or so:
  delta.between.group ~ dunif(0, 100)

  # similarly, both mailman times and delivery times are reasonably precise within 2 hours or so:
  tau.between.group <- pow(sigma.between.group, -2)
  sigma.between.group ~ dunif(0, 100)

  for(j in 1:K){
   # let's say the group-level differences are also normally-distributed:
   group.delta[j] ~ dnorm(delta.between.group, tau.between.group)
   # and each group also has its own standard-deviation, potentially different from the others':
   group.within.sigma[j] ~ dunif(0, 100)
   group.within.tau[j] <- pow(group.within.sigma[j], -2)

   # save the net combo for convenience & interpretability:
   group.mean[j] <- grand.mean + group.delta[j]
  }

  for (i in 1:N) {
   # each individual observation is from the grand-mean + group-offset, then normally distributed:
   Y[i] ~ dnorm(grand.mean + group.delta[Group[i]], group.within.tau[Group[i]])
  }

  # prediction interval for the second group, the 2015 data, which is the one I care about:
  y.new2 ~ dnorm(group.mean[2], group.within.tau[2])
  }"
data <- list(N=nrow(mailExact), Y=mailExact$TimeDelivered, K=max(mailExact$Group+1),
             Group=(mailExact$Group+1))
params <- c("grand.mean","delta.between.group", "sigma.between.group", "group.delta", "group.mean",
            "group.within.sigma", "y.new2")
k1 <- jags(data=data, parameters.to.save=params, inits=NULL, model.file=textConnection(model2)); k1
##                       mu.vect sd.vect    2.5%     25%     50%     75%   97.5%  Rhat n.eff
## delta.between.group    39.435  25.398   2.120  18.421  35.277  57.075  92.897 1.004   510
## grand.mean            650.551  25.354 601.337 633.208 650.248 667.426 701.950 1.006   340
## group.delta[1]         48.751  28.402  -8.001  29.126  48.571  67.956 104.437 1.005   460
## group.delta[2]          8.542  26.788 -45.488  -9.510   9.095  26.338  59.247 1.004   580
## group.mean[1]         699.302  16.314 663.215 690.396 700.385 708.990 730.555 1.001  3000
## group.mean[2]         659.092  10.377 640.786 653.343 658.322 663.680 683.464 1.002  1400
## group.within.sigma[1]  35.048  15.767  16.221  23.941  30.830  41.618  77.953 1.003  1200
## group.within.sigma[2]  21.018  10.808   9.943  14.337  18.138  23.957  53.014 1.001  3000
## sigma.between.group    48.012  24.095  10.639  28.634  44.631  65.673  95.790 1.004  1000
## y.new2                659.272  25.976 609.740 645.561 658.339 672.493 715.009 1.001  3000
## deviance               99.322   4.246  93.844  96.092  98.403 101.657 109.751 1.001  3000

posteriorTimes <- k1$BUGSoutput$sims.list[["y.new2"]]
lossFunction <- function(t, walk_cost, predictions, lastResortT) {
    mean(sapply(predictions,
                function(delivered) { if (delivered<t) { return(walk_cost   + (t           - delivered)); } else
                                                       { return(2*walk_cost + (lastResortT - t) ); }}))
    }
losses <- sapply(c(0:1440), function (tm) { lossFunction(tm, 60, posteriorTimes, 780);})
which.min(losses)
# [1] 691
## 11:31AM
~~~

# Conclusions

Lessons learned:

1. my original prior estimate of when the mailman comes was accurate, but I seem to have underestimated the standard deviation; despite that, providing informative priors meant that the predictions from the JAGS model were sensible from the start (mostly from the small values for the SD, as the mean time was easily estimated from the intervals) and it made good use of the data.
2. the variability of mail delivery times is high enough that the prediction intervals are inherently wide; after relatively few datapoints, whether interval or exact, diminishing returns has set in.
3. ABC really is simple to implement and getting computational tractability is the problem (besides making it tedious to use, long runtimes also interfere with writing a correct implementation in the first place)
4. while powerful, JAGS models can be confusing to write; it's easy to lose track of what the structure of your model is and write the wrong thing, and the use of precision rather than standard-deviation adds boilerplate and makes it even easier to get lost in a welter of variables & distributions, in addition to a fair amount of boilerplate in running the JAGS code at all - leading to errors where you are not certain whether you have a conceptual problem or your boilerplate is out of date
5. the combination of `ggplot2` and [`animation`](http://cran.r-project.org/web/packages/animation/index.html) bade fair to make animations as easy as devising a ggplot2 image, but due to some ugly interactions between them (`ggplot2` interacts with the R toplevel scope/environment in a way which breaks when it's called inside a function, and `animation` has some subtle bug relating to deciding how long to delay frames in an animation which I couldn't figure out), I lost hours to getting it to work at all.
