---
title: When Does The Mail Come?
description: Bayesian decision-theoretic analysis of local mail delivery times
tags: statistics, decision theory
created: 21 June 2015
status: in progress
belief: possible
...

> Mail is delivered by the USPS mailman at a regular but not observed time; what is observed is whether the mail has been delivered at a time, yielding somewhat-unusual "interval-censored data".
> I describe the problem of estimating when the mailman delivers, write a simulation of the data-generating process, and demonstrate analysis of interval-censored data in R using maximum-likelihood (survival analysis with Gaussian regression using `survival` library), MCMC (Bayesian model in JAGS), and likelihood-free Bayesian inference (custom ABC, using the simulation). This allows estimation of the distribution of mail delivery times.
> I compare those estimates from the interval-censored data with estimates from a (smaller) set of exact delivery-times provided by USPS tracking & personal observation, using a multilevel model to deal with heterogeneity apparently due to a change in USPS routes/postmen.
> Finally, I define a loss function on mail checks, enabling a choice of optimal time to check the mailbox and estimates of the value-of-information of another datapoint, defining when to stop data collection.

Consider a question of burning importance: what time does the mailman come, bearing gifts and when should we check the mail?

# Background

No one wants to sit around all morning to spot the *exact* time the mailman comes. At least, I don't.

We could more easily measure by going out in the morning at a random time to see if the mail has come yet, and then (somehow) estimate.

Given a set of data like "2015-06-20 11:00AM: mail has not come yet; 2015-06-21 11:59AM: mail had come", how can we estimate?
This is not a normal setup where we estimate a mean but our data is interestingly messed up: censored or truncated or an interval somehow.

[Survival analysis](!Wikipedia) seems like the appropriate paradigm.
This is not a simple survival analysis with "right-censoring" where each individual is followed up to a censoring time and the exact time of 'failure' is observed.
(This would be right-censoring if instead we had gone out to the mailbox early in the morning and sat there waiting for the mailman to record when she came, occasionally getting bored around 10AM or 11AM and wandering off without seeing when the mail comes.)
This isn't "left-censoring" either (for left-censoring, we'd go out to the mailbox late in the morning when the mail might already be there, and if it isn't, then wait until it does come).
I don't think this is left or right truncation either, since each day data is collected and there's no sampling biases at play.
What this is, is interval censoring: when we go out to the mailbox at 11AM and discover the mail is there, we learn that the mail was delivered today sometime in the interval midnight-10:59AM, or if the mail isn't there, we learn it will be delivered later today sometime during the interval 11:01AM-midnight (hopefully closer to the first end than the second).
Interval censoring comes up in biostatistics for situations like periodic checkups for cancer, which does resemble our mail situation.

# Inference
## Interval-censored data
### ML

The R [`survival` library](http://cran.r-project.org/web/packages/survival/index.html) supports the usual right/left-censoring but also the interval-censoring.
It supports two encodings of intervals, `interval` and `interval2`[^survival-interval-encoding]; I use the former, which format works well with both the `survival` library and also other tools like JAGS.
Times are written as minutes since midnight, so they can be handled as positive numbers rather than date-times (ie midnight=0, 11AM=660, noon=720, midnight=1440, etc), and the upper and lower bounds on intervals are 0 and 1440 (so if I check the mail at 660 and it's there, then the interval is 0-660, and if it's not, 660-1440).

[^survival-interval-encoding]: From the documentation:

    > `Surv(time, time2, event, type="interval")`
    >
    > 1. `time`: For interval data, the first argument is the starting time for the interval.
    > 2. `time2`: ending time of the interval for interval censored or counting process data only. Intervals are assumed to be open on the left and closed on the right, `(start, end]`. For counting process data, event indicates whether an event occurred at the end of the interval.
    > 3. `event`: The status indicator, normally 0=alive, 1=dead....For interval censored data, the status indicator is 0=right censored, 1=event at time, 2=left censored, 3=interval censored.
    >
    > Interval censored data can be represented in two ways. For the first use `type = "interval"` and the codes shown above. In that usage the value of the `time2` argument is ignored unless `event=3`. The second approach is to think of each observation as a time interval with $({-\infty}, t)$ for left censored, $(t, \infty)$ for right censored, $(t,t)$ for exact and $(t_1, t_2)$ for an interval. This is the approach used for `type = "interval2"`. Infinite values can be represented either by actual infinity (`Inf`) or `NA`. The second form has proven to be the more useful one.
    >
    > ...a subject's data for the pair of columns in the dataset `(time1, time2)` is $(t_e, t_e)$ if the event time $t_e$ is known exactly; $(t_l, \text{NA})$ if right censored (where $t_l$ is the censoring time); and $(t_l, t_u)$ if interval censored (where $t_l$ is the lower and $t_u$ is the upper bound of the interval).

~~~{.R}
set.seed(2015-06-21)
# simulate a scenario in which the mailman tends to come around 11AM (660) and I tend to check around then,
# & generate interval data for each time, bounded by end-of-day/midnight below & above, collecting ~1 month:
simulateMailbox <- function(n, time) {
    deliveryTime <- round(rnorm(n, mean = time, sd = 30))
    checkTime <- round(rnorm(n, mean = time, sd = 20))
    simulates <- mapply(function (ck, dy) { if(ck>dy) { return(c(0,ck)) } else { return(c(ck,1440)) }},
                         checkTime, deliveryTime)
    return(data.frame(Time1=simulates[1,], Time2=simulates[2,])) }
mailSim <- simulateMailbox(30, 650); mailSim
##        Time1 Time2
## 1    569  1440
## 2      0   664
## 3    592  1440
## 4    581  1440
## 5    596  1440
## 6      0   703
## 7      0   705
## 8    667  1440
## ...
library(ggplot2)
png(file="~/wiki/images/maildelivery-simulated.png", width = 800, height = 500)
ggplot(mailSim) + geom_segment(aes(x=Time1, xend=Time2, y=1:nrow(mailSim), yend=1:nrow(mailSim))) +
    geom_vline(xintercept=650, color="blue") + ylab("Day") + xlab("Time")
invisible(dev.off())
~~~

Inferring the mean time of delivery might sound difficult with such extremely crude data of intervals 700 minutes wide or worse, but plotting the little simulated dataset and marking the true mean time of 650, we see it's not *that* bad - the mean time is probably whatever line passes through the most intervals:

![The simulated overlapping-intervals data, with the true mean time drawn in blue](/images/maildelivery-simulated.png)

And also with our simulated dataset, we can see if the standard R survival library and a interval-censored model written in JAGS can recover the 650:

~~~{.R}
library(survival)
surv <- Surv(mailSim$Time1, mailSim$Time2, type="interval2")
s <- survfit(surv ~ 1, data=mailSim); summary(s)
##  time   n.risk   n.event survival   std.err lower 95% CI upper 95% CI
## 633.0 30.00000  7.503147 0.749895 0.0790680     0.609889     0.922041
## 651.0 22.49685 12.188431 0.343614 0.0867071     0.209546     0.563458
## 663.5 10.30842  0.611086 0.323245 0.0853927     0.192604     0.542496
## 685.0  9.69734  9.697336 0.000000       NaN           NA           NA
plot(s)
## https://i.imgur.com/nzOHQT0.png
sr <- survreg(surv ~ 1, dist="gaussian", data=mailSim); summary(sr)
##                 Value Std. Error        z           p
## (Intercept) 656.21819   7.324758 89.58906 0.00000e+00
## Log(scale)    2.93664   0.444159  6.61169 3.79957e-11
##
## Scale= 18.8524
##
## Gaussian distribution
## Loglik(model)= -7.1   Loglik(intercept only)= -7.1
## Number of Newton-Raphson Iterations: 10
## n= 30
~~~

### MCMC

More Bayesianly, we can write an interval-censoring model in JAGS, which gives us the opportunity to use an informative prior about the mean time the mailman comes.

They work normal 9-5 hours as far as I know, so we can rule out anything outside 540-1020.
From past experience, I expect the mail to show up not before 10AM (600) and not after 1PM (780), with those extremes being rare and sometime around 11AM (650) being much more common; so not a uniform distribution over 600-780 but a normal one centered on 650 and then somewhat arbitrarily saying that 600-700 represent 3 SDs out from the mean of delivery times to get SD=~30 minutes so in all, `dnorm(650, pow(30, -2))`.
The SD itself seems to me like it could range anywhere from a few minutes to an hour, but much beyond that is impossible (if the SD was over an hour, then every so often the mailman would have to come at 8AM! and if it was smaller than 10 minutes, then I would never have noticed much variation in the first place).

~~~{.R}
library(R2jags)
model1 <- function () {
    for (i in 1:n){
           y[i] ~ dinterval(t[i], dt[i,])
           t[i] ~ dnorm(mu,tau)
           }

           mu ~ dnorm(650, pow(30, -2))
           sd ~ dunif(10, 60)
           tau <- pow(1/sd, 2)

           y.new ~ dnorm(mu, tau)
    }
# y=1 == Event=3 for `Surv`: event is hidden inside interval, not observed/left-/right-censored
data <- list("dt"=mailSim, "n"=nrow(mailSim), "y"=rep(1, nrow(mailSim)))
inits <- function() { list(mu=rnorm(1),sd=30,t=as.vector(apply(mailSim,1,mean))) }
params <- c("mu", "sd", "y.new")
j1 <- jags(data,inits, params, model1); j1
##          mu.vect sd.vect    2.5%     25%     50%     75%   97.5%  Rhat n.eff
## mu       657.855   9.362 640.022 651.823 657.567 663.866 676.540 1.003  1100
## sd        28.226  10.859  12.033  20.051  26.412  34.750  54.141 1.068    35
## y.new    656.872  32.354 592.156 638.315 656.290 675.344 721.606 1.005  3000
## deviance   0.000   0.000   0.000   0.000   0.000   0.000   0.000 1.000     1
~~~

Both approaches' point-value mean time of 656/657 (10:54AM) come close to the true simulation value of 650 (11AM) and the prediction interval of 592-721 also sounds right, validating the models.
The estimated standard deviation isn't as accurate with a wide credible interval, reflecting that it's a harder parameter to estimate and the estimate is still vague with only _n_=30.

~~~{.R}
mailSim <- simulateMailbox(180, 650); mailSim

# http://cran.r-project.org/web/packages/animation/index.html
library(animation)
saveGIF(
    for(n in 1:nrow(mailSim)){
        data <- list("dt"=mailSim[1:n,], "n"=nrow(mailSim[1:n,]), "y"=rep(1, nrow(mailSim[1:n,])))
        inits <- function() { list(mu=rnorm(1),sd=30,t=as.vector(apply(mailSim[1:n,],1,mean))) }
        params <- c("mu","sd", "y.new")
        j1 <- jags(data, inits, params, model1, progress.bar="none")

        lowerMean <- j1$BUGSoutput$summary[c(2),][3]
        medianMean  <- j1$BUGSoutput$mean$mu
        upperMean <- j1$BUGSoutput$summary[c(2),][7]

        lowerPredictive <- j1$BUGSoutput$summary[c(4),][3]
        upperPredictive <- j1$BUGSoutput$summary[c(4),][7]

        confintTrue <- round(qnorm(c(0.025, 0.975), mean=650, sd=30))
        lowerPredictiveTrue <- confintTrue[1]
        upperPredictiveTrue <- confintTrue[2]

        # need an environment call for `ggplot` inside a function: http://stackoverflow.com/a/29595312/329866
        p <- ggplot(mailSim[1:n,]) +
              coord_cartesian(xlim = c(7*60, 13*60)) +
              ylab("Day") + xlab("Time") +
              geom_segment(aes(x=mailSim[1:n,]$Time1, xend=mailSim[1:n,]$Time2, y=1:n, yend=1:n)) +
              geom_vline(xintercept=medianMean, color="blue") +
              geom_vline(xintercept=lowerMean, color="green") +
              geom_vline(xintercept=upperMean, color="green") +
              geom_vline(xintercept=lowerPredictive, color="red") +
              geom_vline(xintercept=upperPredictive, color="red") +
              geom_vline(xintercept=lowerPredictiveTrue, color="red4") +
              geom_vline(xintercept=upperPredictiveTrue, color="red4")
        print(p)
        },
    interval = 0.7, ani.width = 800, ani.height=800,
    movie.name = "/home/gwern/wiki/images/mail-simulated-inferencesamplebysample.gif")
~~~

With a simulation and JAGS set up, we could also see how the posterior estimates of the mean, 95% CI of the mean, and predictive interval change as an additional datapoint is added:

![Simulated data: posterior estimates evolving sample by sample](/images/mail-simulated-inferencesamplebysample.gif)

Probably the most striking aspect of watching these summaries updated datum by datum, to me, is how the estimate of the mean homes in almost immediately on close to the true value (this isn't due solely to the informative prior either, as with a completely uninformative `dunif(0,1440)` will zoom in within 4 or 5 datapoints as well, after some violent thrashing around).
What happens is that the intervals initially look uninformative if the first two or three all turn out to be delivered/non-delivered and so the mean delivery time could still be anywhere from ~11:AM-midnight or vice-versa, but then as soon as even one needle falls the other way, then the mean suddenly snaps into tight focus and gets better from there.
While I understand this abrupt transition in hindsight (only a tiny subset of values around the overlapping tips of the needles can yield the observed flip-flops of delivery/non-delivery, while a mean time far from the tips would yield a completely consistent dataset of all deliveries/non-deliveries), I didn't expect this, simply reasoning that "one interval-censored datum seems very uninformative, so it must take many data to yield any sort of decent result for the mean & standard deviation and hence the predictions".
But, even as the mean becomes precisely estimated, the predictive interval - which is what, in the end, we really care about - remains obdurate and broad, because we assume the delivery time is generated by a normal distribution and so the predicted delivery times are the product of not just the mean but the standard deviation as well, and the standard deviation is hard to estimate (a 95% credible interval of 12-51!).
Also in hindsight this seems obvious as well, since the flip-flopping needles may be sensitive to the mean, but not to the spread of delivery-times; the data would not look much different than it does if the mailman could deliver anywhere from 8AM to 3PM - on early days she delivers a few hours before I check the mailbox around 11AM and on late days she delivers a few hours after.

These considerations also raise questions about statistical power/optimal experiment design: what are the best times to sample from interval-censored data in order to estimate as precisely as possible with a limited budget of samples?
I searched for material on interval-censored data but didn't find anything directly addressing my question.
The flip-flops suggest that to estimate the mean, one should sample only at the current estimated mean, which maximizes the probability that there will be a net 50-50 split of delivery/non-delivery; but where should one sample for the SD as well?

~~~{.R}
set.seed(2015-06-24)
library(lubridate)
fromClock <- function(ts){ (hour(ts)*60) + minute(ts) + (second(ts)/60)}
toClock <- function(t) {
   h <- floor(t/60)
   m <- floor(t - h*60)
   sprintf("%0.2d:%0.2d", h, m)
}

mailInterval <- data.frame(Date=as.POSIXct(c("2015-06-20 11:00AM", "2015-06-21 11:06AM", "2015-06-23 11:03AM",
                                 "2015-06-24 11:05AM", "2015-06-25 11:00AM", "2015-06-26 10:56AM",
                                 "2015-06-27 10:45AM", "2015-06-29 10:31AM", "2015-06-30 10:39AM", "2015-07-01 10:27AM",
                                 "2015-07-02 10:47AM", "2015-07-03 10:27AM", "2015-07-04 10:54AM", "2015-07-05 10:55AM",
                                 "2015-07-06 11:21AM", "2015-07-07 10:01AM", "2015-07-08 10:20AM", "2015-07-09 10:50AM",
                                 "2015-07-10 11:10AM", "2015-07-11 11:12AM", "2015-07-13 11:05AM", "2015-07-14 11:14AM",
                                 "2015-07-15 11:40AM", "2015-07-16 11:24AM", "2015-07-17 11:03AM", "2015-07-18 10:46AM",
                                 "2015-07-20 11:05AM", "2015-07-21 10:56AM", "2015-07-22 11:00AM", "2015-07-23 11:17AM",
                                 "2015-07-24 11:15AM", "2015-07-27 11:11AM", "2015-07-28 10:44AM", "2015-07-29 11:18AM",
                                 "2015-07-30 11:08AM", "2015-07-31 10:44AM", "2015-08-01 11:25AM", "2015-08-03 10:45AM",
                                 "2015-08-04 10:45AM", "2015-08-05 10:44AM", "2015-08-06 10:33AM", "2015-08-07 10:55AM",
                                 "2015-08-10 11:09AM", "2015-08-11 11:16AM", "2015-08-12 11:14AM", "2015-08-13 11:10AM",
                                 "2015-08-14 11:02AM", "2015-08-15 11:04AM", "2015-08-18 11:15AM", "2015-08-20 11:20AM",
                                 "2015-08-22 11:46AM", "2015-08-23 11:04AM", "2015-08-24 10:56AM", "2015-08-25 10:26AM",
                                 "2015-08-26 11:57AM", "2015-08-27 10:15AM", "2015-08-31 12:35AM", "2015-09-01 11:44AM",
                                 "2015-09-02 10:54AM", "2015-09-03 10:29AM", "2015-09-04 10:08AM", "2015-09-05 10:45AM",
                                 "2015-09-07 10:58AM", "2015-09-08 10:25AM", "2015-09-09 10:29AM", "2015-09-10 10:48AM",
                                 "2015-09-16 11:22AM", "2015-09-17 11:06AM", "2015-09-18 11:22AM", "2015-09-19 11:12AM"),
                                "EDT"),
                   Delivered=c(FALSE, FALSE, TRUE, TRUE, TRUE, TRUE, TRUE, FALSE, TRUE, FALSE, TRUE, FALSE, FALSE,
                               FALSE, TRUE, FALSE, FALSE, FALSE, TRUE, TRUE, FALSE, TRUE, TRUE, TRUE, TRUE, FALSE,
                               FALSE, FALSE, TRUE, TRUE, TRUE, FALSE, TRUE, TRUE, FALSE, TRUE, TRUE, FALSE, TRUE,
                               FALSE, FALSE, FALSE, FALSE, TRUE, TRUE, TRUE, TRUE, TRUE, FALSE, TRUE, TRUE, FALSE,
                               FALSE, FALSE, TRUE, FALSE, TRUE, TRUE, FALSE, FALSE, FALSE, FALSE, FALSE, FALSE,
                               FALSE, FALSE, TRUE, TRUE, TRUE, FALSE))
mailInterval$Time <- fromClock(mailInterval$Date)
mail <- with(mailInterval, data.frame(Time1 = ifelse(Delivered, 0, Time),
                           Time2 = ifelse(Delivered,  Time, 1440)))

library(R2jags)
model1 <- function() { for (i in 1:n){
           y[i] ~ dinterval(t[i], dt[i,])
           t[i] ~ dnorm(mu,tau)
           }

           mu ~ dnorm(650, pow(30, -2))
           sd ~ dunif(10, 60)
           tau <- pow(1/sd, 2)

           y.new ~ dnorm(mu, tau)
           }
data <- list("dt"=mail, "n"=nrow(mail), "y"=rep(1, nrow(mail)))
inits <- function() { list(mu=rnorm(1),sd=30,t=as.vector(apply(mail,1,mean))) }
params <- c("mu","sd", "y.new")
j1 <- jags(data, inits, params, model1, n.iter=100000); j1
#          mu.vect sd.vect    2.5%     25%     50%     75%   97.5%  Rhat n.eff
# mu       652.726   7.024 637.219 648.659 653.207 657.233 665.507 1.001  3000
# sd        34.456  11.279  17.134  25.214  32.751  42.679  57.474 1.001  3000
# y.new    652.073  36.256 575.454 630.579 652.712 673.006 724.427 1.001  3000
# deviance   0.000   0.000   0.000   0.000   0.000   0.000   0.000 1.000     1

lowerMean <- j1$BUGSoutput$summary[c(2),][3]
medianMean  <- j1$BUGSoutput$mean$mu
upperMean <- j1$BUGSoutput$summary[c(2),][7]

lowerPredictive <- j1$BUGSoutput$summary[c(4),][3]
# medianMean ~= predictive mean in this case, so don't bother
upperPredictive <- j1$BUGSoutput$summary[c(4),][7]

sapply(c(lowerMean, medianMean, upperMean), toClock)
sapply(c(lowerPredictive, upperPredictive), toClock)

library(ggplot2)

png(file="~/wiki/images/maildelivery-real.png", width = 900, height = 600)
timeLimits <- c(8*60, 13*60)
ggplot(mail) + geom_segment(aes(x=Time1, xend=Time2, y=1:nrow(mail), yend=1:nrow(mail))) +
    ylab("Day") + xlab("Time") +
    scale_x_continuous(breaks=timeLimits, labels=sapply(timeLimits, toClock)) +
    coord_cartesian(xlim = timeLimits) +
    geom_vline(xintercept=medianMean, color="blue") +
    geom_vline(xintercept=lowerMean, color="green") +
    geom_vline(xintercept=upperMean, color="green") +
    geom_vline(xintercept=lowerPredictive, color="red") +
    geom_vline(xintercept=upperPredictive, color="red")
invisible(dev.off())
~~~

![Overlapping-intervals data, with the estimated mean time in blue, 95% CI around the mean time in green, and 95% predictive intervals as to when the delivery is made](/images/maildelivery-real.png)

Posterior predictive checks (PPC) are a technique recommended by Gelman as a test of how appropriate the model is (as the model or likelihood often is far more important in forcing particular results than the priors one might've used), where one generates possible observations from the model and inferred posteriors, and sees how often the simulated data matches the real data; if that is rarely or never, then the model may be bad and one should fix it or possibly throw in additional models for Bayesian model comparison (which will hopefully then pick out the right model).
For continuous data or more than trivially small data, we have the same issue as in ABC (which is similar to PPCs): the simulates will never exactly match the data, so we must define some sort of summary or distance measure which lets us say that a particular simulated dataset is similar enough to the real data.
In the two summaries I tried for ABC, counting 'flip-flops' worked better, so I'll reuse that here.

~~~{.R}
posteriorTimes <- j1$BUGSoutput$sims.list[["y.new"]]

simsPPC <- replicate(10000, {
    n <- nrow(mail)
    deliveryTime <- sample(posteriorTimes, size=n, replace=TRUE)
    checkTime <- round(rnorm(n, mean = 650, sd = 20))
    newSamples <- mapply(function (ck, dy) { if(ck>dy) { return(c(0,ck)) } else { return(c(ck,1440)) }},
                         checkTime, deliveryTime)
    newData <- data.frame(Time1=newSamples[1,], Time2=newSamples[2,])

    return(sum(newData$Time1 == 0))
    }
)

qplot(simsPPC) + geom_vline(xintercept=sum(mail$Time1 == 0), color="blue")
~~~

The true data has a summary near the middle of the distribution of the summaries of the posterior predictions, suggesting that the model fit to the problem is not terrible at the moment.

### ABC

Because JAGS provides an interval-censored distribution in the form of `dinterval()` with a [likelihood function](!Wikipedia), we can use MCMC for inverse inference (reasoning from data to the underlying process)
But if it didn't, I wouldn't know how to write one down for it and then the MCMC wouldn't work; but I was able to write a little simulation of how the underlying process of delivery-and-checking works, which, given a set of parameters, spits out simulated results generated by the process, which is probability or forward inference (reasoning from a version of an underlying process to see what it creates).
This is a common situation: you can write a good simulation simply by describing how you think something works, but you can't write a likelihood function.

[ABC](!Wikipedia "Approximate Bayesian computation") (exemplified in the fun example ["Tiny Data, Approximate Bayesian Computation and the Socks of Karl Broman"](http://www.sumsar.net/blog/2014/10/tiny-data-and-the-socks-of-karl-broman/); see also Wilkinson's [intro](https://darrenjw.wordpress.com/2013/03/31/introduction-to-approximate-bayesian-computation-abc/ "Introduction to Approximate Bayesian Computation (ABC)") & [summary statistics](https://darrenjw.wordpress.com/2013/09/01/summary-stats-for-abc/ "Summary stats for ABC") posts) is a remarkably simple and powerful idea which lets us take a forward simulation and use it to run backwards inference.
(Reminds me a little of [Solomonoff induction](!Wikipedia "Solomonoff's theory of inductive inference").)

The simplest ABC goes like this:
You sample possible parameters from your prior, feed the set of parameters into your simulation, and if the result is identical to your data, you save that set of parameters.
At the end, you're left with a bunch of sets and that's your posterior distribution which you can look at the histograms of and calculate 95% densities etc.

So for the mail data, ABC goes like this:

~~~{.R}
simulateMailbox <- function(n, dTime, dSD) {
    deliveryTime <- round(rnorm(n, mean = dTime, sd = dSD))
    checkTime <- round(rnorm(n, mean = dTime, sd = dSD))
    simulates <- mapply(function (ck, dy) { if(ck>dy) { return(c(0,ck)) } else { return(c(ck,1440)) }},
                         checkTime, deliveryTime)
    return(data.frame(Time1=simulates[1,], Time2=simulates[2,])) }

# if both dataframes are sorted, comparison is easier
mailSorted <- mail[order(mail$Time1),]

mail_sim <- replicate(100000, {
    # mu ~ dnorm(650, 20)
    mu <- rnorm(n=1, mean=650, sd=20)
    # sd ~ dunif(10, 60)
    sd <- runif(n=1, min=10, max=60)

    newData <- simulateMailbox(nrow(mailSorted), mu, sd)
    newDataSorted <- newData[order(newData$Time1),]

    if (all(newDataSorted == mailSorted)) { return(c(Mu=mu, SD=sd)) }
   }
  )
results <- Filter(function(x) {!is.null(x)}, mail_sim)
results <- data.frame(t(sapply(results,c)))
summary(results)
~~~

TODO: parallelize, print progress

The first thing to note is efficiency: I can get a reasonable number of samples in reasonable amount of time for _n_=1-3, but at 4 datapoints, it becomes slow.
There's so many possible datasets when 4 checks are simulated that almost all get rejected because they are not identical to the real dataset and it takes millions of samples and hours to run.
And this problem only gets worse for _n_=5 and bigger.

To run ABC more efficiently, you relax the requirement that the simulated data == real data and instead accept the pair of parameters if the simulated data is 'close enough' in some sense to the real data, close in terms of some summary statistic (hopefully sufficient) like the mean.
I don't know what are the sufficient statistics for a set of interval-censored data, but I figure that if the means of the pairs of times are similar in both datasets, then they are probably close enough for ABC to work, so I can use that as a rejection tolerance; implementing that and playing around, it seems I can make the difference in means as tight as <2 while still running fast.

~~~{.R}
mail_abc <- function(samples) {
    results <- list()
    n <- 0
    while (n<samples) {

      # Priors:
      ## mu ~ dnorm(650, 20)
      mu <- rnorm(n=1, mean=650, sd=20)
      ## sd ~ dunif(10, 60)
      sd <- runif(n=1, min=10, max=60)

      # generate new data set based on a pair of possible parameters:
      newData <- simulateMailbox(nrow(mail), mu, sd)

      # see if some summaries of the new data were within tolerance ϵ<2 of the real data:
      if (abs(mean(newData$Time1) - mean(mail$Time1)) < 2 &&
          abs(mean(newData$Time2) - mean(mail$Time2)) < 2)
        { results <- list(c(Mu=mu, SD=sd), results); n <- n+1; }
    }
   return(results)
  }
sims <- mail_abc(1000)
results <- matrix(unlist(sims), ncol=2, byrow=TRUE)
summary(results)
##        V1                 V2
##  Min.   :613.7729   Min.   :10.02594
##  1st Qu.:651.9860   1st Qu.:13.78808
##  Median :655.2138   Median :16.68179
##  Mean   :653.1099   Mean   :18.11844
##  3rd Qu.:657.7102   3rd Qu.:20.87232
##  Max.   :698.5301   Max.   :57.65834
~~~

The mean value here is somewhat acceptable but the SD is considerably off from the JAGS estimate (which I assume to be correct).
Post hoc, this makes some sense since my summary statistic *is* just means; it might make more sense to check the SD of each column as well (at the cost of more runtime).

Another way to summarize the dataset occurs to me while looking at the graphs: the most striking visual feature of the interval-censored data is how the 'needles' overlap slightly and it is this slight overlap which determines where the mean is; the most informative set of data would be balanced exactly between needles that fall to the left and needles that fall to the right, leaving as little room as possible for the mean to 'escape' out into the wider intervals and be uncertain.
(Imagine a set of data where all the needles fall to the left, because I only checked the mail at 2PM; I would then be extremely certain that the mail is not delivered after 2PM but I would have little more idea than when I started about when the mail is actually delivered in the morning and my posterior would repeat the prior.)
So I could use the count of left or right intervals (it doesn't matter if I use `sum(Time1 == 0)` or `sum(Time2 == 1440)` since they are mutually exclusive) as the summary statistic.

~~~{.R}
mail_abc <- function(samples) {
    results <- list()
    n <- 0
    while (n<samples) {

      # Priors:
      ## mu ~ dnorm(650, 20)
      mu <- rnorm(n=1, mean=650, sd=20)
      ## sd ~ dunif(10, 60)
      sd <- runif(n=1, min=10, max=60)

      # generate new data set based on a pair of possible parameters:
      newData <- simulateMailbox(nrow(mail), mu, sd)

      # see if a summary of the new data matches the old:
      if (sum(mail$Time1 == 0) == sum(newData$Time1 == 0))
        { results <- list(c(Mu=mu, SD=sd), results); n <- n+1; }
    }
   return(results)
  }
sims <- mail_abc(10000)
results <- matrix(unlist(sims), ncol=2, byrow=TRUE)
summary(results)
##        V1                 V2
##  Min.   :571.3127   Min.   :10.00442
##  1st Qu.:636.2064   1st Qu.:22.76214
##  Median :649.8232   Median :35.41042
##  Mean   :649.7943   Mean   :35.20876
##  3rd Qu.:663.2624   3rd Qu.:47.68031
##  Max.   :725.4359   Max.   :59.99888
~~~

This summary, simple as it is, does better in replicating the JAGS estimates.

## Exact delivery-time data

Halfway through compiling my notes, I realized that I *did* in fact have several exact times for deliveries: the USPS tracking emails for packages, while useless on the day of delivery for knowing when to check (since the alerts are only sent around 3-4PM), do include the exact time of delivery that day.
And then while recording interval data, I did sometimes spot the mailman on her rounds; to keep things simple, I still recorded it as an interval.

Exact data makes estimating a mean & SD trivial:

~~~{.R}
set.seed(2015-06-25)
library(lubridate)

mailExact <- data.frame(Date=as.POSIXct(c("2010-04-29 11:33AM", "2010-05-12 11:31AM", "2014-08-20 12:14PM",
                                          "2014-09-29 11:15AM", "2014-12-15 12:02PM", "2015-03-09 11:19AM",
                                          "2015-06-10 10:34AM", "2015-06-20 11:02AM", "2015-06-23 10:58AM",
                                          "2015-06-24 10:53AM", "2015-06-25 10:55AM", "2015-06-30 10:36AM",
                                          "2015-07-02 10:45AM", "2015-07-06 11:19AM", "2015-07-10 10:54AM",
                                          "2015-07-11 11:09AM", "2015-07-15 10:29AM", "2015-07-16 11:02AM",
                                          "2015-07-17 10:46AM", "2015-07-27 11:12AM", "2015-08-15 10:56AM",
                                          "2015-08-17 11:40AM", "2015-08-18 11:19AM", "2015-08-27 10:43AM",
                                          "2015-09-04 10:56AM", "2015-09-18 11:15AM"),
                        "EDT"))
mailExact$TimeDelivered <- fromClock(mailExact$Date)
mean(mailExact$TimeDelivered)
## [1] 667.4
sd(mailExact$TimeDelivered)
## [1] 27.34073498
library(ggplot2); qplot(Date, TimeDelivered, data=mailExact)
## https://i.imgur.com/oJlGp6t.png
~~~

Plotted over time, there's a troubling amount of heterogeneity: despite the sparsity of data (apparently I did not usually bother to set up USPS tracking alerts 2011-2014, to my loss) it's hard not to see two separate clusters there.

### ML

Besides the visual evidence, a _t_-test agrees with there being a difference between 2010-2014 and 2015

~~~{.R}
mailExact$Group <- year(mailExact$Date) > 2014
t.test(TimeDelivered ~ Group, data=mailExact)
##
##  Welch Two Sample t-test
##
## data:  TimeDelivered by Group
## t = 4.105, df = 5.152, p-value = 0.008741
## alternative hypothesis: true difference in means is not equal to 0
## 95 percent confidence interval:
##  18.00369556 76.92963777
## sample estimates:
## mean in group FALSE  mean in group TRUE
##         703.0000000         655.5333333
sd(mailExact[mailExact$Group,]$TimeDelivered)
## [1] 15.59
~~~

Why might there be two clusters?
Well, now that I think about it, I recall that my mailman used to be an older gentleman with white hair (I remember him vividly because in mid-August 2013 a package of fish oil was damaged in transit, and he showed up to explain it to me and offer suggestions on returns; Amazon didn't insist on a leaky fish oil bottle being shipped back and simply sent me a new one).
But now my mailman is a younger middle-aged woman. That seems like a good reason for a shift in delivery times (perhaps she drives faster).

### MCMC

Estimating the two distributions separately in a simple multilevel/hierarchical model:

~~~{.R}
library(R2jags)
model2 <- function() {
  # I expect all deliveries ~11AM/650:
  grand.mean ~ dnorm(650, pow(20, -2))

  # different mailman/groups will deliver at different offsets, but not by more than 2 hours or so:
  delta.between.group ~ dunif(0, 100)
  # similarly, both mailman times and delivery times are reasonably precise within 2 hours or so:
  tau.between.group <- pow(sigma.between.group, -2)
  sigma.between.group ~ dunif(0, 100)

  for(j in 1:K){
   # let's say the group-level differences are also normally-distributed:
   group.delta[j] ~ dnorm(delta.between.group, tau.between.group)
   # and each group also has its own standard-deviation, potentially different from the others':
   group.within.sigma[j] ~ dunif(0, 100)
   group.within.tau[j] <- pow(group.within.sigma[j], -2)

   # save the net combo for convenience & interpretability:
   group.mean[j] <- grand.mean + group.delta[j]
  }

  for (i in 1:N) {
   # each individual observation is from the grand-mean + group-offset, then normally distributed:
   Y[i] ~ dnorm(grand.mean + group.delta[Group[i]], group.within.tau[Group[i]])
  }

  # prediction interval for the second group, the 2015 data, which is the one I care about:
  y.new2 ~ dnorm(group.mean[2], group.within.tau[2])
  }
data <- list(N=nrow(mailExact), Y=mailExact$TimeDelivered, K=max(mailExact$Group+1),
             Group=(mailExact$Group+1))
params <- c("grand.mean","delta.between.group", "sigma.between.group", "group.delta", "group.mean",
            "group.within.sigma", "y.new2")
k1 <- jags(data=data, parameters.to.save=params, inits=NULL, model.file=model2); k1
##                       mu.vect sd.vect    2.5%     25%     50%     75%   97.5%  Rhat n.eff
## delta.between.group    40.507  25.039   2.354  20.631  37.412  58.775  92.712 1.002  1700
## grand.mean            645.629  18.825 606.884 633.280 645.989 658.322 682.017 1.001  3000
## group.delta[1]         52.764  24.302   5.309  36.717  52.991  68.608 100.232 1.001  3000
## group.delta[2]         10.475  19.271 -27.404  -2.572  10.308  22.807  49.381 1.001  3000
## group.mean[1]         698.393  17.665 657.704 689.190 699.706 708.878 731.648 1.009  3000
## group.mean[2]         656.104   4.535 647.273 653.114 656.101 659.051 665.312 1.001  3000
## group.within.sigma[1]  36.124  16.999  15.897  24.210  31.250  43.269  83.633 1.018   180
## group.within.sigma[2]  17.293   3.735  11.718  14.679  16.696  19.129  26.794 1.004  3000
## sigma.between.group    46.836  24.157   7.854  27.498  43.892  64.723  95.735 1.011   570
## y.new2                655.946  17.791 621.060 644.478 655.762 667.243 692.420 1.000  3000
## deviance              174.435   3.629 169.615 171.603 173.642 176.616 182.813 1.016   170

posteriorTimes <- k1$BUGSoutput$sims.list[["y.new2"]]
lossFunction <- function(t, walk_cost, predictions, lastResortT) {
    mean(sapply(predictions,
                function(delivered) { if (delivered<t) { return(walk_cost   + (t           - delivered)); } else
                                                       { return(2*walk_cost + (lastResortT - t) ); }}))
    }
losses <- sapply(c(0:1440), function (tm) { lossFunction(tm, 60, posteriorTimes, 780);})
which.min(losses)
# [1] 683
## 11:23AM
~~~

## Combined data

Combining the interval and exact data is easy: exact data are interval data with very narrow intervals (accurate roughly to within half a minute), where the beginning and end match.
If exact delivery time is available for a day, then the interval data for that day is omitted:

~~~{.R}
mail2 <- rbind(
 with(mailExact,
      data.frame(Date=Date,
                 Time1 = TimeDelivered,
                 Time2= TimeDelivered + 0.5)),
 with(mailInterval[!(as.Date(mailInterval$Date) %in% as.Date(mailExact$Date)),],
     data.frame(Date=Date,
                Time1 = ifelse(Delivered, 0, Time),
                Time2 = ifelse(Delivered,  Time, 1440))))

mail2$Group <- year(mail2$Date) > 2014
~~~

The multilevel model is the same as before with the exception that the likelihood needs to be tweaked to put `dinterval` on top, and the input data needs to be munged appropriately to match its expectations of it being a two-column dataframe of the form `(Time1,Time2)`:

~~{.R}
model3 <- function() {
  grand.mean          ~ dnorm(650, pow(20, -2))
  delta.between.group ~ dunif(0, 100)
  tau.between.group   <- pow(sigma.between.group, -2)
  sigma.between.group ~ dunif(0, 100)
  y.new2015 ~ dnorm(group.mean[2], group.within.tau[2])

  for(j in 1:K){
   group.delta[j] ~ dnorm(delta.between.group, tau.between.group)
   group.within.sigma[j] ~ dunif(0, 100)
   group.within.tau[j] <- pow(group.within.sigma[j], -2)
   group.mean[j] <- grand.mean + group.delta[j]
  }

  for (i in 1:N) {
   y[i] ~ dinterval(t[i], dt[i,])
   t[i] ~ dnorm(grand.mean + group.delta[Group[i]], group.within.tau[Group[i]])
   }
 }

data2 <- list(N=nrow(mail2), K=max(mail2$Group+1),
             Group=(mail2$Group+1), "dt"=subset(mail2, select=c("Time1", "Time2")),
             "y"=rep(1, nrow(mail2)))
params2 <- c("grand.mean","delta.between.group", "sigma.between.group", "group.delta", "group.mean",
            "group.within.sigma", "y.new2015")
inits2 <- function() { list(grand.mean=650, t=as.vector(apply(data2[["dt"]],1,mean))) }
k2 <- jags(data=data2, parameters.to.save=params2, inits=inits2, model.file=model3, n.iter=100000); k2
#                       mu.vect sd.vect    2.5%     25%     50%     75%   97.5%  Rhat n.eff
# delta.between.group    38.867  24.819   2.408  18.240  35.275  56.292  91.096 1.001  3000
# grand.mean            646.898  18.435 609.935 634.606 646.924 658.985 683.461 1.001  3000
# group.delta[1]         51.506  23.614   4.394  35.750  51.684  66.929  97.821 1.002  1300
# group.delta[2]         10.912  18.761 -24.987  -1.254  10.837  23.141  48.655 1.001  3000
# group.mean[1]         698.403  16.954 659.574 689.534 699.472 708.661 729.519 1.002  1500
# group.mean[2]         657.810   3.404 651.219 655.659 657.814 659.903 664.849 1.001  3000
# group.within.sigma[1]  36.127  16.760  15.777  24.132  31.607  43.781  82.065 1.002  1900
# group.within.sigma[2]  18.564   3.417  13.320  16.116  18.128  20.418  26.570 1.002  1600
# sigma.between.group    45.854  24.149   7.752  26.652  42.258  63.388  95.747 1.001  3000
# y.new2015             658.124  18.541 621.740 646.109 658.232 669.937 695.879 1.003   960
# deviance                0.000   0.000   0.000   0.000   0.000   0.000   0.000 1.000     1
~~~

# Decision theory

One oft-mentioned advantage of Bayesian approaches, and subjective Bayesian probability in particular, is that it plugs immediately into economic or decision theory approaches to making choices under uncertainty, so we don't stop at merely providing an estimate of some parameter but can continue onwards to reach a decision.
(See for example [_Probability and Statistics for Business Decisions: an Introduction to Managerial Economics Under Uncertainty_, Schlaifer 1959](/docs/statistics/1959-schlaifer-probabilitystatisticsbusinessdecisions.djvu) and the more recent [_Introduction to Statistical Decision Theory_, Pratt et al 1995](/docs/statistics/1995-pratt-introductionstatisticaldecisiontheory.epub); an applied example of going from Bayesian multilevel models to optimal decisions is ["Bayesian prediction of mean indoor radon concentrations for Minnesota counties"](http://stat.columbia.edu/~gelman/research/published/price.pdf "Price et al 1996").)

Since the motivation for estimating mail-delivery time is implicitly a decision-theoretic problem ("at what time *should* I decide to check the mail?"), there's no reason to settle for just a mean or credible interval for the delivery time.

## Optimal mail checking time

With a posterior distribution of delivery times (`y.new`) we can try to find an optimal time to check, assuming we can figure out a [loss function](!Wikipedia) giving a cost of checking at each possible time.
The loss here can be considered in units of time; if I didn't care about the delay between the package being delivered and me getting my hands on it, I would simply check at 2PM and have done with it.

The usual stopping point in an analysis like this would involve the mean, least-squares, and [mean squared error](!Wikipedia).
There's a [lot of good things to say about that](http://www.benkuhn.net/squared "Why squared error?") as a default
But as an algorithm for deciding when check the mail, squaring errors and averaging them seems like a strange way to evaluate mistakes: is it really *equally* bad to check the mail box 5 minutes before the mail and 5 minutes afterwards? is checking 10 minutes after really *more* than twice as bad as checking 5 minutes after?
I would say "no" in both cases: it's much better to check 5 minutes after rather than 5 minutes before, since then the package will be there and I won't have to make another hike out to the mailbox; and checking 10 minutes seems only twice as bad to me as checking 5 minutes after.
So the loss function itself is not obvious; squared loss, absolute loss and 0-1 loss don't correspond to my process of checking mail.
I need to define my own loss function, expressing the real costs and gains of this situation.

### Defining a loss function

Expecting a package wears on my mind and in some cases, I'd like to start using the contents ASAP; I see a delay of 10 minutes as being twice as bad as 5 minutes and not 4 times as bad, so this part is an absolute loss. (If it arrives at 11AM and I check at 11:30AM, then the loss due to the delay is 30.)
Besides the delay, there's also the cost of walking all the way down to the mailbox and back, which due to the long driveway is around 10 minutes, but it's mostly unshaded and hot out and an interruption, so the loss is much greater than that; introspecting, I would be willing to wait at least 60 minutes to save one roundtrip, so I will define the cost of a mail check at 60 'minutes'.
So suppose I check at 10:30AM, the mail comes at 11AM, and I check again at 11:30AM and find it; then the cost is 60+60+(11:30AM-11:00AM) = 150, while if I had checked at 11:20AM then the cost is better and smaller at just 60+(11:20AM-11:00AM) = 70.

OK, so if the package is there, that's easy, but what if I walk out at 11:20AM and it's *not* there?
In a proper [Bayesian search theory](!Wikipedia) application, if I check once, I would then update my posterior and run a loss function again to decide when to check next, but this is impracticable for daily usage and in reality, what I would do is if the first check didn't turn up the package, I would then give up in frustration & disgust and not come back until ~1PM (780) when the package would definitely have arrived. Then I'd incur a loss of two walks and possibly a long wait until 1PM.

All these considerations give me a weird-looking but nevertheless realistic loss function: if the package is delivered at _d_ and we check at a particular time _t_, then if _t_>_d_ and the package had arrived we incur a total loss of $60+(t-d)$; otherwise, we check back a second and final time at 1PM, incurring a total loss of $60+60+(780-d)$

<!-- $ -->

### Finding the optimum

Having figured that out, we run the loss function on each sample from the posterior, averaging over all weighted possible delivery times, and find what time of day minimizes the loss:

~~~{.R}
posteriorTimes <- k2$BUGSoutput$sims.list[["y.new2015"]]
lossFunction <- function(t, walk_cost, predictions, lastResortT) {
    mean(sapply(predictions,
                function(delivered) { if (delivered<t) { return(walk_cost   + (t           - delivered)); } else
                                                       { return(2*walk_cost + (lastResortT - t) ); }}))
    }
losses <- sapply(c(0:1440), function (tm) { lossFunction(tm, 60, posteriorTimes, 780);})
toClock(which.min(losses))
# [1] 686
## 11:26AM
qplot(0:1440, losses)
~~~

### Total costs

We can compare the cost of check times. If I check 10 minutes later than the current estimated optimal time, then the loss each time is:

~~~{.R}
minIndex <- which.min(losses)
losses[minIndex]
# [1] 98.18117677
losses[minIndex+10]
# [1] 101.1307662
losses[minIndex] - losses[minIndex+10]
# [1] -2.949589399
~~~

With average losses per check time, an estimate of number of check times per year, and a discount rate, we can derive a [net present value](!Wikipedia); I usually use the approximation $\frac{\text{gain}}{ln(1 + \text{discount rate})}$ and a discount rate of 5% annually, but in this case assuming an indefinite horizon is wrong - the delivery time will change and the problem will reset as soon as the next mailman begins this route, which has already happened once in the 4 years thus far, so I instead assume that any estimate is worthless after 3 years (the mailman will have changed, the route or USPS will have changed, I may not even be living in the same place in 3 years).
So if I check for a package 30 times a year (which is roughly as much as I did in 2014 & am on track to do in 2015), and I do so suboptimally each time and incur a loss of -2, then the total loss is equivalent to

~~~{.R}
netPresentValue <- function(p) { ((p / (1 + 0.05)^1) + (p / (1 + 0.05)^2) + (p / (1 + 0.05)^3)) * 30  }
netPresentValue(-2)
## [1] -163.3948818
~~~

## Optimal data-sampling
### Reinforcement learning

Given a loss function, we might be able to optimize our data-sampling by balancing exploration and exploitation using a [reinforcement learning](!Wikipedia) approach to the [multi-armed bandit problem](!Wikipedia) such as [Thompson sampling](!Wikipedia)/probability-sampling/probability-matching - where different check-times are 'arms', the payoff is given by the loss, and we draw a check time from the current posterior, check at that time, and then with the new data update the posterior for the next day.

I have [implemented Thompson sampling for this problem](#thompson-sampling).
While probability sampling is conceptually simple, intuitive, has no hyperparameters to tweak, and optimal in a number of ways, implementing it would have required more discipline than I cared for; so I didn't use it.

### Expected information gains

Ignoring indefinite online learning, it is possible to optimize data sampling by estimating what parameter value is predicted to yield the most information, in the sense of entropy of distributions, giving Expected Information Gains (or "expected gain in Shannon information", or "expected Kullback–Leibler discrepancy").

## Optimal sample-size: Value of Information

If we are not optimizing the kind of data collected, we can optimize how *much* data we collect by periodically calculating how much additional data could improve our estimates and how much this improvement is worth on average, especially compared to how much that additional data would cost.

Expected values of various kinds of information show up frequently in Bayesian decision theory, and are valuable for linking questions of how much & what data to collect to real-world outcomes (not just money but lives and welfare; [an Alzheimer's example](http://eprints.whiterose.ac.uk/946/1/claxtonk1.pdf "'Bayesian value-of-information analysis: an application to a policy model of Alzheimer's disease', Caxton et al 2001")).

### EVPI

The first kind is "[expected value of perfect information](!Wikipedia)" (EVPI): if I estimated the optimal time as t=688 and it was actually t=698, then I could be suffering a penalty of 163 minutes and I should be willing to pay somewhere up to that (based on how certain I am it's t=688) to learn the truth and instead start checking the mail at t=698.
The value of perfect information is simply the difference between the current estimate and whatever you hypothesize the true time is.

The *expected* value or EVPI is the cost of the current estimate of the optimum versus a possible alternative time, where the alternatives are weighted by probabilities (if the mail really comes at 9AM then that implies a big loss if you foolishly check at 11:30AM but the probability of such a big loss is almost zero); the probabilities come, as usual, from the posterior distribution of times which is approximated as a big batch of samples:

~~~{.R}
mean(sapply(round(posteriorTimes), function(time) { netPresentValue((losses[time] - min(losses))) } ))
# [1] 5815.991296
~~~

Perfect information is not available at any cost, however.^[I could always resort to additional technology like motion-activated cameras or radio bugs on the mailbox which send alerts when it's been opened if I *really* wanted, but for the purpose of this exercise, let's ignore those possibilities.]
All I can do is go out to the mail box _n_ times, and as we saw in the simulation, diminishing returns always happens and at some point the predictive intervals stop changing noticeably.
So EVPI represents an upper bound on how much any lesser amount of information could be worth, but doesn't tell us how much we are willing to pay for imperfect information.

### EVSI

The next kind is what is the "[expected value of sample information](!Wikipedia)" (EVSI): what is the value of collecting one more additional datapoint, which can help pin down the optimal time a little more precisely and reduce the risk of loss from picking a bad time?
EVSI can be defined as "expected value of best decision with some additional sample information" minus "expected value of best current decision".
More specifically, if many times we created a new datapoint, create an updated posterior distribution incorporating that new datapoint, run the loss function again & calculate a new optimal check time, and compute the improvement of the new check time's NPV over improved estimate of the old check time's NPV to estimate a possible EVSI, what is the mean of the EVSIs?

This tells us the benefit of that datapoint, and then we can subtract the cost of collecting one more datapoint; if it's still positive, we want to collect more data, but if it's negative (the gain from the estimate improved by one more datapoint) is less than a datapoint would cost to collect, then we have hit diminishing returns and it may be time to stop collecting data.

We could do this for one datapoint to decide whether to stop. But we could also go back and look at how the EVSI & profit shrank during the original collection of the mail data.

~~~{.R}
data <- mail # original interval-only data
sampleValues <- data.frame(N=NULL, newOptimum=NULL, newOptimumLoss=NULL, sampleValue=NULL, sampleValueProfit=NULL)
for (n in seq(from=0, to=(nrow(data)+10))) {

    evsis <- replicate(200, {
            # if n is more than we collected, bootstrap hypothetical new data; otherwise, just take that prefix
            # and pretend we are doing a sequential trial where we have only collected the first n observations thus far
            if (n > nrow(data)) { newData <- rbind(data, data[sample(1:nrow(data), n - nrow(data) , replace=TRUE),]) }
                              else { newData <- data[1:n,] }

            dataEVSI <- list(n=nrow(newData), "dt"=subset(newData, select=c("Time1", "Time2")),
                         "y"=rep(1, nrow(newData)))
            paramsEVSI <- c("mu","sd", "y.new")
            initsEVSI <- function() { list(mu=rnorm(1),sd=30,t=as.vector(apply(dataEVSI[["dt"]],1,mean))) }
            kEVSI <- jags(data=dataEVSI, parameters.to.save=paramsEVSI, inits=initsEVSI, model.file=model1,
                          n.iter=3000, progress.bar="none")

            posteriorTimesEVSI <- kEVSI$BUGSoutput$sims.list[["y.new"]]
            lossesEVSI <- sapply(c(0:1440), function (tm) { lossFunction(tm, 60, posteriorTimesEVSI, 780);})

            if (n==0) { oldOptimum <- min(lossesEVSI)  }
                 else {oldOptimum <- lossesEVSI[sampleValues[sampleValues$N==0,]$newOptimum] } # compare to the estimate based on priors
            newOptimum <- min(lossesEVSI)
            sampleValue <- netPresentValue(newOptimum - oldOptimum)
            sampleValueProfit <- sampleValue + (n*60)

            return(list(N=n, newOptimum=which.min(lossesEVSI), newOptimumLoss=newOptimum,
                        sampleValue=sampleValue, sampleValueProfit=sampleValueProfit))
            }
        )

    sampleValues <- rbind(sampleValues, data.frame(N=n, newOptimum=mean(unlist(evsis[2,])), newOptimumLoss=mean(unlist(evsis[3,])),
                                                   sampleValue=mean(unlist(evsis[4,])), sampleValueProfit=mean(unlist(evsis[5,]))))
    }
sampleValues
#     N newOptimum newOptimumLoss    sampleValue sampleValueProfit
# 1   0    712.600    126.6888958    0.00000000      0.0000000000
# 2   1    712.575    126.7579219  -11.93854006     48.0614599416
# 3   2    719.980    122.3724436 -121.31501738     -1.3150173784
# 4   3    708.150    126.9706051  -32.03379518    147.9662048193
# 5   4    701.790    127.9095775 -143.35714066     96.6428593445
# 6   5    696.695    128.8701531 -299.02233338      0.9776666227
# 7   6    692.440    129.8191319 -466.05452337   -106.0545233747
# 8   7    688.695    132.0807898 -633.09206632   -213.0920663197
# 9   8    690.765    129.5727029 -542.78513810    -62.7851380951
# 10  9    687.660    132.0364916 -681.32564291   -141.3256429125
# 11 10    689.210    130.2163274 -597.48233962      2.5176603795
# 12 11    686.395    130.5338326 -758.15486682    -98.1548668161
# 13 12    687.455    128.6273025 -695.28046012     24.7195398753
# 14 13    691.525    129.2092477 -496.05481084    283.9451891562
# 15 14    695.410    129.2232888 -347.92312703    492.0768729651
# 16 15    693.405    128.1591433 -433.22087565    466.7791243520
# 17 16    693.650    127.0530983 -436.23463412    523.7653658752
# 18 17    693.735    125.5236645 -444.81276457    575.1872354295
# 19 18    695.725    125.3049938 -355.24354368    724.7564563236
# 20 19    693.155    123.7840402 -465.57031668    674.4296833227
# 21 20    691.340    122.2890257 -573.81903972    626.1809602784
# 22 21    695.460    124.4854690 -373.66455168    886.3354483178
# 23 22    693.835    123.3286284 -453.72808090    866.2719190994
# 24 23    692.890    122.1550798 -511.31873755    868.6812624468
# 25 24    691.585    120.6627142 -594.94760901    845.0523909901
# 26 25    689.930    119.8993341 -688.68789781    811.3121021938
# 27 26    690.845    118.8742591 -644.53601191    915.4639880893
# 28 27    693.915    120.5363205 -480.27587034   1139.7241296593
# 29 28    695.455    120.5722357 -410.57354380   1269.4264562033
# 30 29    693.720    119.8216438 -491.01196669   1248.9880333079
# 31 30    692.560    118.5479051 -566.54877290   1233.4512271013
# 32 31    691.265    117.4006005 -642.81873824   1217.1812617638
# 33 32    694.800    120.2531077 -443.02993834   1476.9700616613
# 34 33    694.500    122.6974736 -444.15880135   1535.8411986539
# 35 34    693.200    121.6848098 -506.60101893   1533.3989810750
# 36 35    695.640    123.1766613 -377.44505442   1722.5549455769
# 37 36    695.055    125.0534433 -389.10082564   1770.8991743593
# 38 37    694.210    124.1294044 -439.58662308   1780.4133769227
# 39 38    694.610    123.0556714 -424.08194842   1855.9180515813
# 40 39    694.310    124.7484991 -436.35444597   1903.6455540347
# 41 40    694.505    123.4282826 -429.85235414   1970.1476458589
# 42 41    694.280    122.2800503 -437.52664626   2022.4733537369
# 43 42    695.660    121.9807264 -395.84245846   2124.1575415363
# 44 43    697.795    123.4574969 -298.08161577   2281.9183842337
# 45 44    696.745    122.5201770 -345.76139793   2294.2386020661
# 46 45    695.845    121.6540080 -389.60085022   2310.3991497771
# 47 46    695.090    120.9569570 -436.42100092   2323.5789990833
# 48 47    693.990    120.5449101 -483.41266155   2336.5873384516
# 49 48    694.305    120.7051254 -487.09162861   2392.9083713904
# 50 49    693.840    120.1719436 -499.45106193   2440.5489380720
# 51 50    693.940    119.9749612 -504.81034896   2495.1896510352
# 52 51    693.960    119.9420246 -500.28329793   2559.7167020736
# 53 52    693.595    119.7075089 -512.73745021   2607.2625497905
# 54 53    693.875    119.4981456 -515.01771496   2664.9822850449
# 55 54    693.505    119.1947709 -524.05689719   2715.9431028079
# 56 55    693.900    119.5021316 -517.24622527   2782.7537747290
# 57 56    693.715    119.1529502 -536.63625754   2823.3637424644
# 58 57    693.530    118.7091686 -531.78528869   2888.2147113139
plot(sampleValues$N, round(sampleValues$newOptimum))
# https://i.imgur.com/PrkMCsr.png
~~~

This would suggest that diminishing returns were reached early on (possibly the informative priors had done more work than I appreciated).

The graph shows the estimated optimal mail-check time (700=11:40AM etc) as each datapoint was collected.
You can see that with the priors I set, they were biased towards too-late mail times but as more data comes in, the new optimal check-time drifts steadily downwards until the bias of the prior has been neutralized and now it begins to follow a random walk around 695, with the final estimated mailchecktime being ~693/11:33AM
And at around n=48, diminishing returns has set in so hard that the decision actually stops changing entirely, it's all small fluctuations around 693.
When you include the cost of gathering data, the analysis says you should collect up to _n_=13 and then stop (after that, the loss does not decrease but begins to increase because each datapoint inflates costs by +60, and we want to minimize loss); at _n_=13, one decides to check at 687/11:27AM, which is extremely close to the 693 which was estimated with 4x more data (!).

So this is interesting: by using informative priors and then taking a decision-theoretic approach, in this case I can make high-quality decisions on surprisingly little data.


<!--
# Bayesian Model Averaging

Possibilities:

nested sampling
reversible jump MCMC
product-space method
BIC/DIC/WAIC deviance criteria
non-Bayesian: cross-validation

"Bayesian model choice via Markov chain Monte Carlo methods" Carlin & Chib 1995 http://stats.ma.ic.ac.uk/~das01/MyWeb/SCBI/Papers/CarlinChib.pdf
"A tutorial on Bayes factor estimation with the product space method", Lodewyckx et al 2011 http://ejwagenmakers.com/2011/LodewyckxEtAl2011.pdf
"A guide to Bayesian model selection for ecologists" http://www.esajournals.org/doi/full/10.1890/14-0661.1
Hoeting et al 1999. "Bayesian model averaging: a tutorial" https://projecteuclid.org/download/pdf_1/euclid.ss/1009212519

> ...It is no surprise that when this model fails, it is the [likelihood](!Wikipedia "Likelihood function") rather than the [prior](!Wikipedia "Prior probability") that is causing the problem. In the binomial model under consideration here, the prior comes into the posterior distribution only once and the likelihood comes in _n_ times. It is perhaps merely an accident of history that skeptics and subjectivists alike strain on the gnat of the prior distribution while swallowing the camel that is the likelihood.^[["'Not Only Defended But Also Applied': The Perceived Absurdity of Bayesian Inference"](http://www.stat.columbia.edu/~gelman/research/published/feller8.pdf), Gelman & Robert 2013]

In setting up an analysis, one must [make a variety of choices](http://www.stat.columbia.edu/~gelman/research/unpublished/p_hacking.pdf "'The garden of forking paths: Why multiple comparisons can be a problem, even when there is no `fishing expedition` or `p-hacking` and the research hypothesis was posited ahead of time', Gelman & Loken 2013") in what models to use, which influences the result; this neglect of "model uncertainty" can lead to overconfident inferences or miss critical details.
In Bayesian analyses, typically the prior comes in for the most scrutiny as to how it determines the result; but the prior part of the model often has much less influence than the structural or functional form of the analysis, where it is assumed that the outcome is drawn from a normal distribution or another such common choice, although it's not.
This choice itself is often arbitrary and can determine the result almost regardless of what the data says. For example, the normal distribution has thin tails and so the probability of something being far out on a tail will be extremely small and combined with other considerations like measurement error, can result in a model refusing to ever accept that a value really is 'extreme' without equally extreme amounts of data - a refusal that would not happen if any of a number of other perfectly possible distributions had been chosen instead like lognormal distributions or Cauchy distributions.

For example, in [one analysis of charities](http://lesswrong.com/lw/745/why_we_cant_take_expected_value_estimates/ "Why We Can't Take Expected Value Estimates Literally (Even When They're Unbiased)"), the writer uses normal distributions and thus winds up arguing that with error in our assessments of charities' values, our estimate must be shrunken far towards the mean (which is entirely true); but [this is circular](http://lesswrong.com/lw/gzq/bayesian_adjustment_does_not_defeat_existential/), since by assuming normal distributions he has also assumed away the existence of large differences in value between charities (even though there *are* large differences - consider [knitting sweaters for penguins](http://www.giantflightlessbirds.com/2011/10/the-great-penguin-sweater-fiasco/ "The Great Penguin Sweater Fiasco") vs [stopping children from being infected by parasites](!Wikipedia "Schistosomiasis Control Initiative") or [killed by malaria](!Wikipedia "Against Malaria Foundation"); are they really within a few standard deviations of each other?) and it's clear that whatever the right distribution for modeling charity is, it must allow these differences.

In the case of mail deliveries, there's clearly model uncertainty.
While I used [normal distributions](!Wikipedia), I could as easily have used [_t_-distributions](!Wikipedia "Student's t-distribution") (fatter-tailed normals), [log-normals](!Wikipedia "Log-normal distribution"), [exponentials](!Wikipedia "Exponential distribution"), [uniforms](!Wikipedia "Uniform distribution (discrete)"), [betas](!Wikipedia "Beta distribution"), [overdispersed](!Wikipedia "Overdispersion") versions of some of the previous, or picked some even more exotic [univariate distribution](http://www.math.wm.edu/~leemis/chart/UDR/UDR.html "Univariate Distribution Relationships") and easily come up with a just-so story for some of them.
(It's a negative-binomial because we're modeling minutes until "failure" ie delivery; no, it's a log-normal because each stop slows down the mailman and delays multiply; no, it's a normal distribution because each stop adds time and the sum of many small random deviates is itself normally distributed; no, it's a _t_-distribution because sometimes the mail arrives much earlier or later and it's a more robust distribution anyway; no, it's even wider than that and a uniform distribution because the mailman starts and ends their shift at particular times but otherwise doesn't care about speed; no, it's a beta distribution over time because that's more flexible and can include the others as special-cases...)

Model uncertainty can be handled several ways ([Hooten & Hobbs 2015]( http://www.esajournals.org/doi/full/10.1890/14-0661.1 "A guide to Bayesian model selection for ecologists")), including:

- Penalty or regularization or sparsity priors (only handles selection of variables/predictors; gains largely eliminated in this case by informative priors)
- Diagnostics like posterior predictive checks, [crossvalidation](!Wikipedia), and [AIC](!Wikipedia "Akaike information criterion")/[BIC](!Wikipedia "Bayesian information criterion")/[DIC](!Wikipedia "Deviance information criterion")/[WAIC](http://arxiv.org/pdf/1208.6338 "'A Widely Applicable Bayesian Information Criterion', Watanabe 2012") ([overview](http://www.stat.columbia.edu/~gelman/research/published/waic_understand3.pdf  "'Understanding predictive information criteria for Bayesian models', Gelman et al 2013")) can be used to check for lack of fit; but like tests in software development, they can only show the presence of bugs & not their absence
- Maximally flexible models can be used like nonparametric Bayesian models, to minimize the possibility of model misspecification ([Dunson 2013](/docs/statistics/2013-dunson.pdf "Nonparametric Bayes"))
- [Ensemble approaches](!Wikipedia "Ensemble learning") (often targeted at selection of variables/predictors but should be applicable to choosing between different distributions as well)

   - Bayesian model choice: models can be pitted against each other (eg using Bayes factors) implemented using [nested sampling](!Wikipedia) or [reversible jump MCMC](!Wikipedia)/product-space method, and the one with highest posterior probability is selected for any further analysis while all others are ignored
   - Bayesian model averaging: posterior probabilities of models are calculated, but all models are retained and their posteriors are combined weighted by the probability that model is the true one (but with enough data, the posterior probability of the model closest to the truth will converge on 1 and BMA becomes a model selection procedure)
   - Bayesian model combination: posterior probabilities of combinations/ensembles of models are calculated, yielding a richer set of 'models' which often outperform all of the original models; such as by learning an optimal weighting in a linear combination ([Monteith et al  2011](http://axon.cs.byu.edu/papers/Kristine.ijcnn2011.pdf "Turning Bayesian Model Averaging Into Bayesian Model Combination"))

Of the non-ensemble approaches: regularization is already done; some of the diagnostics will work but others won't (JAGS won't calculate DIC for interval-censored data); nonparametric approaches would be too challenging for me to implement in JAGS and I'm not sure there is enough data to allow the nonparametric models to work well.
Of the ensemble approaches: Bayesian model combination is difficult to implement




http://lesswrong.com/lw/hzu/model_combination_and_adjustment/ popularization

Crossvalidation
leave one out crossvalidation; zero-one loss

    library(R2jags)
    modelN <- "model { for (i in 1:n){
                y[i] ~ dinterval(t[i], dt[i,])
                t[i] ~ dnorm(mu,tau)
               }
               mu ~ dnorm(650, pow(30, -2))
               sd ~ dunif(10, 60)
               tau <- pow(1/sd, 2)

               y.new ~ dnorm(mu, tau)
               }"
    modelT <- "model { for (i in 1:n){
                y[i] ~ dinterval(t[i], dt[i,])
                t[i] ~ dt(mu,tau,nu)
               }

               nu ~ dexp(1/30)

               mu ~ dnorm(650, pow(30, -2))
               sd ~ dunif(10, 60)
               tau <- pow(1/sd, 2)

               y.new ~ dnorm(mu, tau)
               }"

I want to compare them. (You might say use DIC, but R2jags doesn't provide that for interval-censored data). Since this is small data, leave-one-out crossvalidation seems like a good idea. But what is the rror?

Mean-squared-error is out due to the intervaling, as are variants like absolute error. Thinking about it some, it seems to me that each datapoint is really made of two things: the check-time, and delivery status. The check-time has nothing to do with the model quality and the model isn't trying to predict it; what I want the model to predict is whether the mail is delivered or not if I were to check at particular times. The check-time is the predictor variable and the delivery-status is the response variable.

So I could ask the model's posterior predictive distribution of delivery-times (`y.new`) whether or not the mail would or would not be delivered at a particular time, and compare it against the held-out datapoint's actual delivery status, 1 if the model correctly predicts delivered/not-delivered and 0 if it predicts the opposite of what the data said. (So a zero-one loss.)

Here's a try at implementing leave-one-out crossvalidation for those two JAGS models on the interval-censored mail data with a zero-one loss defined that way based on delivery status:

    loocvs <- function(dt, model) {
        results <- NULL

        for (i in 1:nrow(dt)) {
         # set up relevant data for this particular fold in the crossvalidation
         ith       <- dt[i,]
         newData   <- dt[-i,] # drop the _i_th datapoint
         checkTime <- if (ith$Time1==0) { ith$Time2 } else { ith$Time1 }
         there     <- if (ith$Time1==0) { TRUE } else { FALSE }

         # set up and run the model and extract predicted delivery times
         data   <- list("dt"=newData, "n"=nrow(newData), "y"=rep(1, nrow(newData)))
         inits  <- function() { list(mu=rnorm(1),sd=30,t=as.vector(apply(newData,1,mean))) }
         params <- c("y.new")
         cv1 <- jags(data, inits, params, textConnection(model), n.iter=10000, progress.bar="none")
         posteriorTimes <- cv1$BUGSoutput$sims.list[["y.new"]]

         # score predictions against heldout data point
         results[i] <- mean(sapply(posteriorTimes, function(t) { if (t<checkTime && there) { 1 } else { if (t>checkTime && !there) { 1 } else { 0 } }}))
        }
       return(results)
    }

    loocvsN <- loocvs(mail, modelN)
    loocvsT <- loocvs(mail, modelT)

    mean(loocvsN)
    # 0.5838765432
    mean(loocvsT)
    # 0.5837777778
-->

# Conclusions

Lessons learned:

1. my original prior estimate of when the mailman comes was accurate, but I seem to have underestimated the standard deviation; despite that, providing informative priors meant that the predictions from the JAGS model were sensible from the start (mostly from the small values for the SD, as the mean time was easily estimated from the intervals) and it made good use of the data.
2. the variability of mail delivery times is high enough that the prediction intervals are inherently wide; after relatively few datapoints, whether interval or exact, diminishing returns has set in.
3. ABC really is simple to implement and getting computational tractability is the problem (besides making it tedious to use, long runtimes also interfere with writing a correct implementation in the first place)
4. while powerful, JAGS models can be confusing to write; it's easy to lose track of what the structure of your model is and write the wrong thing, and the use of precision rather than standard-deviation adds boilerplate and makes it even easier to get lost in a welter of variables & distributions, in addition to a fair amount of boilerplate in running the JAGS code at all - leading to errors where you are not certain whether you have a conceptual problem or your boilerplate is out of date
5. the combination of `ggplot2` and [`animation`](http://cran.r-project.org/web/packages/animation/index.html) bade fair to make animations as easy as devising a ggplot2 image, but due to some ugly interactions between them (`ggplot2` interacts with the R toplevel scope/environment in a way which breaks when it's called inside a function, and `animation` has some subtle bug relating to deciding how long to delay frames in an animation which I couldn't figure out), I lost hours to getting it to work at all.

# Appendix
## Thompson sampling

Given a Bayesian model and a reinforcement-learning setting like checking the mailbox, it seems natural to use Thompson sampling to guide checks and do online learning.
I didn't want to reorganize my mornings around such an algorithm, but in reading up on Thompson sampling, I found no source code for any uses outside the stereotypical 3-4 arm binomial bandit, and certainly nothing I could use.
So while I'm not certain it's correct, I've attempted to construct a Thompson sampler in this interval-censored mail-checking problem by treating it as a 200-arm bandit where the probability of being the optimal arm is computed as the number of posterior loss samples which are equal to the minimum possible loss (each sample being a loss/action pair), and then one samples exactly one loss from that filtered distribution, and executes the paired action.

That is:

1. Compute the posterior distribution of observations (`y.new`)
2. take the loss function _l_, and for each action _a_ in 1:1440 (or for efficiency here, just 600:800), for each observation _o_ in `y.new`, compute _l(a,o)_, yielding a posterior loss distribution for each action
3. take the mean of each posterior loss distribution, and then find the minimum of expected losses
4. for each action, filter the posterior losses and drop anything bigger than the minimum expected loss; if after filtering, action _a_ has 10 samples/observations/simulates left while action _a+1_ has 15, then _a+1_ is that much more likely to minimize loss
5. sample uniformly from the filtered posterior losses, and take the action which created that particular sample

An implementation of Thompson sampling for this problem, and a test harness to run it on a simulated version of mail-checking to see how it performs in terms of actions, losses, and regret:

~~~{.R}
posterior_losses <- function(times, actions) {
   lossPosterior <- function(t, walk_cost, predictions, lastResortT) {
       sapply(predictions,
                   function(delivered) { if (delivered<t) { return(walk_cost   + (t           - delivered)); } else
                                                          { return(2*walk_cost + (lastResortT - t) ); }})
       }
   pl <- sapply(actions, function (tm) { list(losses = lossPosterior(tm, 60, times, 780));})
   names(pl) <- actions
   return(pl)
}
optimize_posterior <- function(losses)  {
   expectedLosses <- unlist(lapply(losses, mean))
   optimumLoss <- min(expectedLosses)
   optimumAction <- which.min(expectedLosses)
   posteriorOfOptimal <- lapply(losses, function(a) { Filter(function (l) { l<=optimumLoss; }, a); })
   return(list(expectedLosses, optimumLoss, as.integer(names(optimumAction)), posteriorOfOptimal))
}

thompsonSample <- function(dt, model) {
   library(R2jags)
   data <- list("dt"=dt, "n"=nrow(dt), "y"=rep(1, nrow(dt)))
   inits <- function() { list(mu=rnorm(1),sd=30,t=as.vector(apply(dt,1,mean))) }
   params <- c("y.new")
   j1 <- jags(data, inits, params, model, n.iter=1000, progress.bar="none")

   # 0:1440 is overkill, so let's only look at the plausible region of times to actively check:
   actions <- c(600:800)
   posteriorTimes <- j1$BUGSoutput$sims.list[["y.new"]]
   posteriorLosses <- posterior_losses(posteriorTimes, actions)

   optimums <- optimize_posterior(posteriorLosses)
   posteriorOfOptimal <- optimums[[4]]

   # now to rewrite and then sample one weighted action:
   library(utils)
   df <- stack(posteriorOfOptimal)
   df$ind <- as.integer(as.character(df$ind))
   # what's the probability the current optimum is optimal? might be nice to know, to get an idea
   # of how much has been learned and how fast it's converging:
   print(paste0("Probability of current estimated-optimal action being optimal: ", nrow(df[df$ind==optimums[[3]],]) / nrow(df)))
   # the Thompson sample itself!
   actionToTake <- sample(df$ind, size=1)
   return(actionToTake) }

simulate_mab <- function(n) {
  walk_cost = 60; lastResortT = 780

  # pick a concrete mean/SD for this hypothetical run, based on the full MCMC estimates as of 2015-09-20 for verisimilitude:
  mail_mean <- rnorm(1, mean=658.9, sd=4.69)
  mail_sd <- rnorm(1, mean=27.3, sd=8.33)
  # to calculate regret, we need an optimal loss, which we can actual losses to:
  posteriorTimes <- rnorm(5000, mean=mail_mean, sd=mail_sd)
  actions <- c(600:800)
  posteriorLosses <- posterior_losses(posteriorTimes, actions)
  optimums <- optimize_posterior(posteriorLosses)
  optimalLoss <- optimums[[2]]
  optimalAction <- optimums [[3]]
  print(paste0("Parameters for this simulation; Mean: ", mail_mean, "; SD: ", mail_sd, "; minimum average loss: ",
               optimalLoss, "; optimal action: ", optimalAction))

  # seed data:
  mail3 <- data.frame(Time1=660, Time2=1440, Delivered=NA, Action=660, Loss=NA,  Regret=NA, Total.regret=NA)

  for (i in 1:n) {
    # today's delivery:
    delivered <- rnorm(1, mean=mail_mean, sd=mail_sd)
    # when should we check?
    t <- thompsonSample(subset(mail3,select=c(Time1,Time2)), model1)
    # discover what our loss is, update the database:

    loss <- if (delivered<t) { walk_cost  + (t - delivered); } else
               { 2*walk_cost + (lastResortT - t); }
    mail3 <- if(delivered<t) { rbind(mail3, data.frame(Time1=0, Time2=t,    Delivered=delivered, Action=t, Loss=loss, Regret=loss-optimalLoss, Total.regret=sum(c(loss,mail3$Regret),na.rm=TRUE))) } else
              { rbind(mail3,                data.frame(Time1=t, Time2=1440, Delivered=delivered, Action=t, Loss=loss, Regret=loss-optimalLoss, Total.regret=sum(c(loss,mail3$Regret),na.rm=TRUE))) }
   }
   return(mail3)
}
run <- simulate_mab(100); run
# [1] "Parameters for this simulation; Mean: 652.284509072865; SD: 33.1807599290166; minimum average loss: 118.560610994536; optimal action: 692"
# last estimate:
# [1] "Probability of current estimated-optimal action being optimal: 0.01055932721"
# ...
#     Time1 Time2   Delivered Action         Loss          Regret Total.regret
# 1     660  1440          NA    660           NA              NA           NA
# 2       0   715 627.8084020    715 147.19159803  28.63098703195  147.1915980
# 3       0   732 620.5552582    732 171.44474183  52.88413083952  200.0757289
# 4       0   677 675.9171439    677  61.08285612 -57.47775486957  142.5979740
# 5       0   725 633.4140947    725 151.58590531  33.02529431491  175.6232683
# 6       0   760 648.3155173    760 171.68448269  53.12387170030  228.7471400
# 7       0   685 611.2824123    685 133.71758772  15.15697672581  243.9041167
# 8     667  1440 716.5567134    667 233.00000000 114.43938900546  358.3435057
# 9       0   718 589.8287347    718 188.17126527  69.61065427500  427.9541600
# 10    670  1440 697.3583662    670 230.00000000 111.43938900546  539.3935490
# 11      0   677 674.6866959    677  62.31330405 -56.24730694442  483.1462421
# 12      0   691 637.7795855    691 113.22041449  -5.34019650785  477.8060456
# 13      0   746 646.8473411    746 159.15265891  40.59204791098  518.3980935
# 14      0   772 631.6795209    772 200.32047908  81.75986808891  600.1579616
# 15    620  1440 667.7679051    620 280.00000000 161.43938900546  761.5973506
# 16      0   765 661.7253761    765 163.27462390  44.71401290154  806.3113635
# 17      0   659 609.5240464    659 109.47595358  -9.08465741264  797.2267061
# 18    606  1440 685.7777542    606 294.00000000 175.43938900546  972.6660951
# 19    611  1440 635.2589317    611 289.00000000 170.43938900546 1143.1054841
# 20      0   698 636.7853367    698 121.21466329   2.65405229485 1145.7595364
# 21      0   647 603.9625318    647 103.03746822 -15.52314277290 1130.2363936
# 22      0   750 606.7172689    750 203.28273106  84.72212006155 1214.9585137
# 23      0   702 664.6092881    702  97.39071189 -21.16989910379 1193.7886146
# 24    645  1440 677.5866617    645 255.00000000 136.43938900546 1330.2280036
# 25      0   675 622.7556231    675 112.24437692  -6.31623407236 1323.9117695
# 26    630  1440 640.8950003    630 270.00000000 151.43938900546 1475.3511585
# 27      0   759 668.5994752    759 150.40052478  31.83991378552 1507.1910723
# 28      0   711 644.1780825    711 126.82191754   8.26130654703 1515.4523788
# 29    674  1440 699.6155426    674 226.00000000 107.43938900546 1622.8917678
# 30      0   706 608.0919274    706 157.90807260  39.34746160105 1662.2392294
# 31      0   703 658.8938782    703 104.10612185 -14.45448914733 1647.7847403
# 32    665  1440 705.7443431    665 235.00000000 116.43938900546 1764.2241293
# 33      0   674 642.3336905    674  91.66630949 -26.89430150333 1737.3298278
# 34      0   710 633.5048344    710 136.49516563  17.93455463380 1755.2643824
# 35      0   658 648.8459318    658  69.15406824 -49.40654275525 1705.8578397
# 36      0   721 655.1939143    721 125.80608568   7.24547468563 1713.1033144
# 37      0   694 675.5563436    694  78.44365637 -40.11695462652 1672.9863597
# 38      0   732 639.9091890    732 152.09081102  33.53020002879 1706.5165598
# 39      0   715 688.1279312    715  86.87206882 -31.68854217612 1674.8280176
# 40      0   646 585.8585108    646 120.14148923   1.58087823876 1676.4088958
# 41    642  1440 695.3785223    642 258.00000000 139.43938900546 1815.8482848
# 42      0   709 634.1009321    709 134.89906785  16.33845685992 1832.1867417
# 43      0   713 577.8404132    713 195.15958677  76.59897577341 1908.7857175
# 44      0   696 616.2782608    696 139.72173919  21.16112819377 1929.9468457
# 45      0   690 639.5414911    690 110.45850890  -8.10210209318 1921.8447436
# 46      0   642 625.3276557    642  76.67234428 -41.88826671073 1879.9564768
# 47      0   688 602.9815250    688 145.01847500  26.45786400318 1906.4143408
# 48      0   677 636.4162781    677 100.58372189 -17.97688909999 1888.4374517
# 49      0   680 585.8080006    680 154.19199938  35.63138839021 1924.0688401
# 50    621  1440 656.0041495    621 279.00000000 160.43938900546 2084.5082291
# 51      0   646 641.8895007    646  64.11049927 -54.45011172800 2030.0581174
# 52      0   651 640.1109106    651  70.88908941 -47.67152158648 1982.3865958
# 53    654  1440 693.3740789    654 246.00000000 127.43938900546 2109.8259848
# 54      0   702 595.2336565    702 166.76634352  48.20573252195 2158.0317174
# 55      0   713 670.4853416    713 102.51465840 -16.04595259532 2141.9857648
# 56      0   713 663.1696646    713 109.83033537  -8.73027562201 2133.2554891
# 57      0   742 663.0346158    742 138.96538418  20.40477318844 2153.6602623
# 58      0   682 607.7165689    682 134.28343107  15.72282007608 2169.3830824
# 59    673  1440 677.2851570    673 227.00000000 108.43938900546 2277.8224714
# 60    659  1440 684.3798249    659 241.00000000 122.43938900546 2400.2618604
# 61      0   685 633.1187680    685 111.88123195  -6.67937904380 2393.5824814
# 62      0   701 680.1054935    701  80.89450648 -37.66610451415 2355.9163769
# 63      0   696 621.6042366    696 134.39576343  15.83515243649 2371.7515293
# 64    662  1440 693.8072818    662 238.00000000 119.43938900546 2491.1909183
# 65      0   673 655.3728580    673  77.62714198 -40.93346901803 2450.2574493
# 66      0   690 658.0426097    690  91.95739029 -26.60322070655 2423.6542286
# 67      0   667 613.3341881    667 113.66581190  -4.89479909305 2418.7594295
# 68    656  1440 700.4197853    656 244.00000000 125.43938900546 2544.1988185
# 69      0   684 601.0853037    684 142.91469634  24.35408534530 2568.5529038
# 70      0   664 611.9715482    664 112.02845184  -6.53215915861 2562.0207447
# 71    668  1440 680.3354581    668 232.00000000 113.43938900546 2675.4601337
# 72      0   693 610.2313284    693 142.76867161  24.20806061466 2699.6681943
# 73      0   708 630.3118016    708 137.68819842  19.12758742097 2718.7957817
# 74      0   719 660.4021943    719 118.59780574   0.03719474235 2718.8329765
# 75      0   676 636.5799015    676  99.42009852 -19.14051247024 2699.6924640
# 76      0   698 625.2036774    698 132.79632258  14.23571158160 2713.9281756
# 77      0   668 606.4258465    668 121.57415354   3.01354254290 2716.9417181
# 78    623  1440 656.3955047    623 277.00000000 158.43938900546 2875.3811071
# 79    646  1440 672.2594580    646 254.00000000 135.43938900546 3010.8204961
# 80    692  1440 696.5494961    692 208.00000000  89.43938900546 3100.2598851
# 81    692  1440 693.0719431    692 208.00000000  89.43938900546 3189.6992741
# 82      0   649 613.0656011    649  95.93439895 -22.62621204696 3167.0730621
# 83      0   746 650.2982686    746 155.70173136  37.14112036061 3204.2141824
# 84      0   635 630.3302528    635  64.66974716 -53.89086383500 3150.3233186
# 85      0   691 636.3183255    691 114.68167449  -3.87893650150 3146.4443821
# 86      0   685 633.6342636    685 111.36573637  -7.19487462734 3139.2495075
# 87    661  1440 690.9044588    661 239.00000000 120.43938900546 3259.6888965
# 88      0   712 655.4131008    712 116.58689919  -1.97371180597 3257.7151847
# 89      0   641 621.4642923    641  79.53570766 -39.02490333784 3218.6902813
# 90    672  1440 680.1790357    672 228.00000000 109.43938900546 3328.1296704
# 91    680  1440 697.6656642    680 220.00000000 101.43938900546 3429.5690594
# 92      0   720 654.4254360    720 125.57456400   7.01395300054 3436.5830124
# 93      0   656 634.5331573    656  81.46684273 -37.09376826699 3399.4892441
# 94    617  1440 630.1733576    617 283.00000000 164.43938900546 3563.9286331
# 95    615  1440 666.6395197    615 285.00000000 166.43938900546 3730.3680221
# 96      0   689 640.0697013    689 108.93029870  -9.63031229923 3720.7377098
# 97      0   714 696.3436876    714  77.65631238 -40.90429861141 3679.8334112
# 98    622  1440 658.8444344    622 278.00000000 159.43938900546 3839.2728002
# 99      0   679 667.4792287    679  71.52077135 -47.03983964934 3792.2329605
# 100   645  1440 684.8291151    645 255.00000000 136.43938900546 3928.6723496
# 101   651  1440 690.3757918    651 249.00000000 130.43938900546 4059.1117386
~~~

In this example, TS does not seem to perform too spectacularly, wandering sometimes quite far from the optimal action 692 despite having 100 data samples to work with.
But the probability of the current optimal choice being optimal is still a remarkably low 1%, indicating that there hasn't been nearly as much learning as one might hope.

I speculate that interval-censoring, or the asymmetry of the loss function, make this a hard problem to reach good performance on compared to the usual scenarios one sees Thompson sampling employed on: the interval-censoring drains data of most of its information, while the asymmetry encourages conservativeness and trying to avoid checking mail early; in contrast, my main analysis exploits exact mail delivery times to get more accuracy, and the loss from each mailbox check is ignored, so to speak, since nothing is being delivered most days I gathered data, while this TS model is implicitly in a package-checking situation.
It may be that no reinforcement-learning agent can perform well in this setting with only _n_=100 of weak data, and the somewhat informative priors specified.
