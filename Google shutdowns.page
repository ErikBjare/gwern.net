---
title: Predicting Google closures
description: Analyzing predictors of Google abandoning products; predicting future shutdowns
created: 28 Mar 2013
tags: statistics, archiving, predictions
status: in progress
belief: likely
...

> Google has occasionally shut down services which I use, costing me time as I adopt alternatives. I collect and classify >300 Google products, and look for predictive variables. I find some and make some predictions about future shutdowns.

The shut down of the popular service [Google Reader](!Wikipedia), announced on [13 March 2013](http://googleblog.blogspot.com/2013/03/a-second-spring-of-cleaning.html), has brought home to many people that some products they rely on exist only at Google's sufferance: it provides the products for reasons that are difficult for outsiders to divine, may have little commitment to a product[^Reader-threats], may not include their users' best interests, may choose to withdraw the product at any time for any reason[^Reader-popularity] (especially since most of the products are services[^cloud-suckers] & not FLOSS, and may be too tightly coupled with the Google infrastructure[^unportable]), and users have no voice[^voice-utility] - [only exit](!Wikipedia "Exit, Voice, and Loyalty") as an option.

[^unportable]: From Gannes's ["Another Reason Google Reader Died: Increased Concern About Privacy and Compliance"](http://allthingsd.com/20130324/another-reason-google-reader-died-increased-concern-about-privacy-and-compliance/)

    > But at the same time, Google Reader was too deeply integrated into Google Apps to spin it off and sell it, like the company did last year with its SketchUp 3-D modeling software.

    [mattbarrie](https://news.ycombinator.com/item?id=5373584) on Hacker News:

    > I'm here with Alan Noble who runs engineering at Google Australia and ran the Google Reader project until 18 months ago. They looked at open sourcing it but it was too much effort to do so because it's tied to closely to Google infrastructure. Basically it's been culled due to long term declining use.
[^cloud-suckers]: This would not come as news to [Jason](http://ascii.textfiles.com/archives/1717 "FUCK THE CLOUD - January 16, 2009") [Scott](http://ascii.textfiles.com/archives/2229 "Oh Boy, The Cloud - October 5, 2009"), of course, but nevertheless [James Fallows points out](http://www.theatlantic.com/technology/archive/2013/03/finale-for-now-on-googles-self-inflicted-trust-problem/274286/ "Finale for Now on Google's Self-Inflicted Trust Problem") that when a cloud service evaporates, it's simply gone and gives an interesting comparison:

    > [Kevin Drum](!Wikipedia), [in _Mother Jones_](http://www.motherjones.com/kevin-drum/2013/03/problem-google-and-cloud "The Problem With Google -- and The Cloud"), on why the inability to rely on Google services is more disruptive than the familiar pre-cloud experience of having favorite programs get orphaned. My example is [Lotus Agenda](!Wikipedia): it has officially been dead for nearly 20 years, but *I can still use it* (if I want, in a DOS session under the VMware Fusion Windows emulator on my Macs. Talk about layered legacy systems!). When a cloud program goes away, as Google Reader has done, it's gone. There is no way you can keep using your own "legacy" copy, as you could with previous orphaned software.
[^voice-utility]: The sheer size & dominance of some Google services have lead to comparisons to natural monopolies, such as the _Economist_ column ["Google's Google problem"](http://www.economist.com/blogs/freeexchange/2013/03/utilities). I saw this comparison mocked, but it's worth noting that at least one Googler made the same comparison years before. From [Levy](!Wikipedia "Steven Levy")'s _[In the Plex](!Wikipedia)_ 2011, part 7, section 2:

    > While some Googlers felt singled out unfairly for the attention, the more measured among them understood it as a natural consequence of Google's increasing power, especially in regard to distributing and storing massive amounts of information. "It's as if Google took over the water supply for the entire United States", says Mike Jones, who handled some of Google's policy issues. "It's only fair that society slaps us around a little bit to make sure we're doing the right thing."
[^Reader-threats]: Google Reader affords two examples of this lack of transparency on a key issue - Google's willingness to support Reader (extremely relevant to users, and even more so to the third-party web services and applications which relied on Reader to function); from BuzzFeed's ["Google's Lost Social Network: How Google accidentally built a truly beloved social network, only to steamroll it with Google+. The sad, surprising story of Google Reader"](http://www.buzzfeed.com/robf4/googles-lost-social-network):

    > The difficulty was that Reader users, while hyper-engaged with the product, never snowballed into the tens or hundreds of millions. Brian Shih became the product manager for Reader in the fall of 2008. "If Reader were its own startup, it's the kind of company that Google would have bought. Because we were at Google, when you stack it up against some of these products, it's tiny and isn't worth the investment", he said. At one point, Shih remembers, engineers were pulled off Reader to work on OpenSocial, a "half-baked" development platform that never amounted to much. "There was always a political fight internally on keeping people staffed on this little project", he recalled. Someone hung a sign in the Reader offices that said "DAYS SINCE LAST THREAT OF CANCELLATION." The number was almost always zero. At the same time, user growth - while small next to Gmail's hundreds of millions - more than doubled under Shih's tenure. But the "senior types", as Bilotta remembers, "would look at absolute user numbers. They wouldn't look at market saturation. So Reader was constantly on the chopping block."
    >
    > So when news spread internally of Reader's gelding, it was like Hemingway's line about going broke: "Two ways. Gradually, then suddenly." Shih found out in the spring that Reader's internal sharing functions - the asymmetrical following model, endemic commenting and liking, and its advanced privacy settings - would be superseded by the forthcoming Google+ model. Of course, he was forbidden from breathing a word to users.

    The sign story is confirmed by another Googler; ["Google Reader lived on borrowed time: creator Chris Wetherell reflects"](http://gigaom.com/2013/03/13/chris-wetherll-google-reader/):

    > "When they replaced sharing with +1 on Google Reader, it was clear that this day was going to come," he said. Wetherell, 43, is amazed that Reader has lasted this long. Even before the project saw the light of the day, Google executives were unsure about the service and it was through sheer perseverance that it squeaked out into the market. At one point, the management team threatened to cancel the project even before it saw the light of the day, if there was a delay. "We had a sign that said, '*days since cancellation*' and it was there from the very beginning," added a very sanguine Wetherell. My translation: Google never really believed in the project. Google Reader started in 2005 at what was really the golden age of RSS, blogging systems and a new content ecosystem. The big kahuna at that time was Bloglines (acquired by Ask.com) and Google Reader was an upstart.

    Jason Scott in 2009 reminded us that this lack of transparency is completely predictable: "Since the dawn of time, companies have hired people whose entire job is to tell you everything is all right and you can completely trust them and the company is as stable as a rock, and to do so until they, themselves are fired because the company is out of business."
[^Reader-popularity]: The official PR release stated that too little usage was the reason Reader was being abandoned. Whether this is the genuine reason has been questioned by third parties, who observe that Reader seems to [drive far more traffic](http://www.buzzfeed.com/jwherrman/google-reader-still-sends-far-more-traffic-than-google) than another service which Google has yet to ax, Google+; that one app had [>2m users who also had Reader accounts](http://allthingsd.com/20130324/another-reason-google-reader-died-increased-concern-about-privacy-and-compliance/); that just one alternative to Reader (Feedly) had in excess of [3 million signups post-announcement](http://blog.feedly.com/2013/04/02/announcing-the-new-feedly-mobile-and-welcoming-3-million-reader-refugees/); and the largest of several petitions to Google reached [148k](https://www.change.org/petitions/google-keep-google-reader-running) signatures. Given that few users will sign up at Feedly specifically, sign a petition, visit the BuzzFeed network, or use the apps in question, it seems likely that Reader had closer to 20m users than 2m users when its closure was announced.

In the case of Reader, while Reader destroyed the original RSS reader market, there still exist some usable alternatives; the main damage is a shrinkage in the RSS audience as inevitably many users choose not to invest in a new reader or give up, and an irreversible loss of Reader's uniquely comprehensive RSS archives covering back to 2005.

But every shut down hurts its users to some degree, even if we - currently[^survival] - can rule out the most devastating shut downs, like a shut down of Gmail. It would be interesting to see if shut downs are to some degree predictable, whether there are any obvious patterns, whether common claims about relevant factors can be confirmed, and what the results might suggest for the future.

[^survival]: If there is one truth of the tech industry, it's that no giant (except IBM) survives forever. Death rates for all corporations and nonprofits are [very high](Girl Scouts and good governance#fn1), but particularly so for tech. [One blogger](http://shkspr.mobi/blog/2013/03/preparing-for-the-collapse-of-digital-civilization/ "Preparing for the Collapse of Digital Civilization") asks a good question:

    > As we come to rely more and more on the Internet, it's becoming clear that there is a real threat posed by tying oneself to a 3rd party service. The Internet is famously designed to route around failures caused by a nuclear strike - but it cannot defend against a service being withdrawn or a company going bankrupt. It's tempting to say that multi-billion dollar companies like Apple and Google will never disappear - but a quick look at history shows Nokia, Enron, Amstrad, Sega, and many more which have fallen from great heights until they are mere shells and no longer offer the services which many people once relied on...I like to pose this question to my photography friends - "What would you do if Yahoo! suddenly decided to delete all your Flickr photos?" Some of them have backups - most faint at the thought of all their work vanishing.

# Data
## Sources
### Dead products

> The summer grasses -- \
> the sole remnants of many \
> brave warriors' dreams.

I begin with a list of services/APIs/programs that Google has shut down or abandoned taken from the _Guardian_ article ["Google Keep? It'll probably be with us until March 2017 - on average: The closure of Google Reader has got early adopters and developers worried that Google services or APIs they adopt will just get shut off. An analysis of 39 shuttered offerings says how long they get"](http://www.guardian.co.uk/technology/2013/mar/22/google-keep-services-closed) by Charles Arthur. Arthur's list seemed relatively complete, but I've added in >83 items he missed based on the [Slate graveyard](http://www.slate.com/articles/technology/map_of_the_week/2013/03/google_reader_joins_graveyard_of_dead_google_products.html), Weber's ["Google Fails 36% Of The Time"](http://thenextweb.com/google/2011/10/17/google-fails/), the [Wikipedia category](!Wikipedia "Category:Google acquisitions")/[list](!Wikipedia "List of mergers and acquisitions by Google") for Google acquisitions, the [Wikipedia category](!Wikipedia "Category:Discontinued Google services")/[list](!Wikipedia "List of Google products#Discontinued products and services"), and finally the official [Google History](https://www.google.com/about/company/history/). (The additional shut downs include many shutdowns predating 2010, suggesting that Arthur's list was biased towards recent shutdowns.)

In a few cases, the start dates are well-informed guesses (eg. [Google Translate](https://plus.google.com/u/0/103530621949492999968/posts/fqxuM2SBRQ5)) and dates of abandonment/shut-down are even harder to get due to the lack of attention paid to most (Joga Bonito) and so I infer the date from archived pages on the Internet Archive, news reports, blogs, the dates of press releases, the shutdown of closely related services (eReader Play based on Reader), source code repositories (AngularJS) etc; some are listed as discontinued (Google Catalogs) but are still supported or were merged into other software (Spreadsheets, Docs, Writely, News Archive) or sold/given to third parties (Flu Shot Finder, App Inventor, Body) or active effort has ceased but the content remains and I do not list those as dead; for cases of acquired software/services that were shut down, I date the start from Google's purchase.

### Live products

A major criticism of Arthur's post was that it was fundamentally using the wrong data: if you have a dataset of all Google products which have been shut down, you can make statements like "the average dead Google product lived 1459 days", but you can't infer very much about a live product's life expectancy - because you don't know if it will join the dead products. If, for example, only 1% of products ever died, then 1459 days would lead to a massive underestimates of the average lifespan of all currently living products. With his data, you can only make inferences conditional on a product eventually dying, you cannot make an unconditional inference. Unfortunately, the unconditional question "will it die?" is the real question any Google user wants answered!

So drawing on the same sources, I have compiled a second list of *living* products; the ratio of living to dead gives a base rate for how likely a randomly selected Google product is to be canceled within the 1997-2013 window, and with the date of the founding of each living product, we can also do a simple right-censored [survival analysis](!Wikipedia) which will let us make better still predictions by extracting results like mean time to failure. Some items are obviously dead in a meaningful sense since they have been closed to new users (Sync), lost major functionality (FeedBurner, Meebo), degraded severely due to neglect (eg. Google Alerts), or just been completely neglected for a decade or more (Google Group's Usenet archive) - but haven't actually died or closed yet, so I list them as alive.

## Variables

> To my good friend \
> Would I show, I thought, \
> The plum blossoms, \
> Now lost to sight \
> Amid the falling snow.^[[Yamabe no Akahito](!Wikipedia), _Man'yōshū_ [VIII: 1426](http://www.temcauley.staff.shef.ac.uk/waka0088.shtml)]

Finally, for all products, I have collected several covariates which I thought might help predict longevity:

- `Hits`: the number of Google hits for a service

    While number of Google hits is a very crude measure, at best, for underlying variables like "popularity" or "number of users" or "profitability", and clearly biased towards recently released products (there aren't going to be as many hits for, say, "Google Answers" as there would have been if we had searched for it in 2002), it may add some insight.

    There do not seem to be any other free quality sources indicating either historical or contemporary traffic to a product URL/homepage which could be used in the analysis - services like Alexa or Google Ad Planner either are commercial, for domains only, or simply do not cover many of the URLs.
- `Type`: a categorization into "service"/"program"/"thing"/"other"

    1. A *service* is anything primarily accessed through a web browser or API or the Internet; so Gmail or a browser loading fonts from a Google server, but not a Gmail notification program one runs on one's computer or a FLOSS font available for download & distribution.
    2. A *program* is anything which is an application, plugin, library, framework, or all of these combined; some are very small (Authenticator) and some are very large (Android). This does include programs which require Internet connections or Google APIs as well as programs for which the source code has not been released, so things in the program category are not immune to shut down and may be useful only as long as Google supports them.
    3. A *thing* is anything which is primarily a physical object. A cellphone or laptop running Android would be an example.
    4. *Other* is the catch-all category for things which don't quite seem to fit. Where does a Google thinktank, charity, conference, or venture capital fund fit in? They certainly aren't software, but they don't seem to be quite services either.
- `Profit`: whether Google *directly* makes money off a product

    This is a tricky one. Google excuses many of its products by saying that anything which increases Internet usage benefits Google and so by this logic, every single one of its services could potentially increase profit; but this is a little stretched, the truth very hard to judge by an outsider, and one would expect that products without direct monetization are more likely to be killed.

    Generally, I classify as for profit any Google product directly relating to producing/displaying advertising, paid subscriptions, fees, or purchases (Adwords, Gmail, Blogger, Search, shopping engines, surveys); but many do not seem to have any form of monetization related to them (Alerts, Office, Drive, Gears, Reader[^Reader-monetization]). Some services like Voice charge (for international calls) but the amounts are minor enough that one might wonder if classifying them as for profit is really right. While it might make sense to define every feature added to, say, Google Search (eg. Personalized Search, or Search History) as being 'for profit' since Search lucratively displays ads, I have chosen to classify these secondary features as being not for profit.
- `FLOSS`: whether the source code was released or Google otherwise made it possible for third parties to continue the service or maintain the application.

    Android, AngularJS, and Chrome are all examples of software products where Google losing interest would not be fatal; services spun off to third parties would also count. Many of the codebases rely on a proprietary Google API or service (especially the mobile applications), which means that this variable is not as meaningful and laudable as one might expect, so in the minority of cases where this variable is relevant, I code `Dead` & `Ended` as related to whether & when Google abandoned it, regardless of whether it was then picked up by third parties or not. (Example: App Inventor for Android is listed as dying in December 2011, though it was then half a year later handed over to MIT, who has supported it since.) It's important to not naively believe that simply because source code is available, Google support doesn't matter.
- `Acquisition`: whether it was related to a purchase of a company or licensing, or internally developed.

    Many startups have been bought (DoubleClick, Dodgeball, Android, Picasa), or technologies/data licensed (SYSTRAN for Translate, Twitter data for Real-Time Search). If a closely related product is developed and released after purchase, like a mobile application, I do not class it as an acquisition; just products that were in existence when the company was purchased. I do not include products that Google dropped immediately on purchase (Apture, fflick, Sparrow, Reqwireless, PeakStream) or where products based on them have not been released (BumpTop).

[^Reader-monetization]: Some have justified Reader's shut down as simply a rational act, since Reader was not bringing in any money and Google is not a charity. The truth seems to be related more to Google's lack of interest since the start - it's hard to see how Google could possibly be able to monetize Gmail and not also monetize Reader, which is confirmed by two involved Googlers (from "Google Reader lived on borrowed time: creator Chris Wetherell reflects"):

    > I wonder, did the company (Google) and the ecosystem at large misread the tea leaves? Did the world at large see an RSS/reader market when in reality the actual market opportunity was in data and sentiment analysis? [Chris] Wetherell agreed. "The reader market never went past the experimental phase and none was iterating on the business model," he said. "Monetization abilities were never tried."
    >
    > "There was so much data we had and so much information about the affinity readers had with certain content that we always felt there was monetization opportunity," he said. Dick Costolo (currently CEO of Twitter), who worked for Google at the time (having sold Google his company, Feedburner), came up with many monetization ideas but they fell on deaf ears. Costolo, of course is working hard to mine those affinity-and-context connections for Twitter, and is succeeding. What Costolo understood, Google and its mandarins totally missed, as noted in this [November 2011 blog post](http://massless.org/?p=174 "Dreams, discernment, and Google Reader") by Chris who wrote:
    >
    >> ***Reader exhibits the best unpaid representation I've yet seen of a consumer's relationship to a content producer***. You pay for HBO? That's a strong signal. Consuming free stuff? Reader's model was a dream. Even better than Netflix. You get affinity (which has clear monetary value) for free, and a tracked pattern of behavior for the act of iterating over differently sourced items - and a mechanism for distributing that quickly to an ostensible audience which didn't include social guilt or gameification - along with an extensible, scalable platform available via commonly used web technologies - all of which would be an amazing opportunity for the right product visionary. ***Reader is (was?) for information junkies; not just tech nerds***. This market totally exists and is weirdly under-served (and is possibly affluent).





~~~{.R}
google <- read.csv("wiki/docs/2013-google.csv", colClasses=c("character","logical","Date","Date",
                                                             "double","factor","logical","logical",
                                                             "logical","logical"))
# if no end-date is listed, use today, since we know not what the morrow brings; and
# sufficient unto the user is the evil of the day
google$Ended[is.na(google$Ended)] <- as.Date(Sys.Date())
# infer lifetimes
google$Days <- as.integer(google$Ended - google$Started)
# data looks clean, although Hits spans such a huge magnitude we'll log-transform it when we use it
summary(google[-1])


    Dead            Started               Ended                 Hits               Type
 Mode :logical   Min.   :1997-09-15   Min.   :2005-10-22   Min.   :8.00e+00   other  :  5
 FALSE:193       1st Qu.:2006-05-10   1st Qu.:2009-12-11   1st Qu.:1.72e+05   program: 75
 TRUE :106       Median :2008-07-23   Median :2011-09-30   Median :9.16e+05   service:211
 NA's :0         Mean   :2008-03-20   Mean   :2011-03-14   Mean   :8.02e+07   thing  :  8
                 3rd Qu.:2010-03-16   3rd Qu.:2012-06-20   3rd Qu.:6.51e+06
                 Max.   :2013-03-20   Max.   :2013-11-01   Max.   :3.86e+09
   Profit          FLOSS
 Mode :logical   Mode :logical
 FALSE:17        FALSE:21
 TRUE :10        TRUE :6
 NA's :272       NA's :272
plot(dead$Ended ~ dead$Started, xlab="Opened", ylab="Shut down")
~~~

![A scatterplot of openings vs closings](http://i.imgur.com/Zt5vAw3.png)

Better yet, a stacked line plot of start/end intervals?
something like http://stackoverflow.com/questions/9871043/increasing-the-performance-of-visualising-overlapping-segments or

~~~{.R}
viewReads <- function(reads){
    # sort by start
    sorted <- reads[order(reads$Started),];

    #---
    # In the first iteration we work out the y-axis
    # positions that segments should be plotted on
    # segments should be plotted on the next availible
    # y position without merging with another segment
    #---
    yread <- c(); #keeps track of the x space that is used up by segments

    # get x axis limits
    minstart <- min(sorted$Started);
    maxend <- max(sorted$Ended);

    # initialise yread
    yread[1] <- minstart - 1;
    ypos <- c(); #holds the y pos of the ith segment

    # for each read
    for (r in 1:nrow(sorted)){
        read <- sorted[r,];
        start <- read$Started;
        placed <- FALSE;

        # iterate through yread to find the next availible
        # y pos at this x pos (start)
        y <- 1;
        while(!placed){
            print(start)
            if(yread[y] < start){
                ypos[r] <- y;
                yread[y] <- read$Ended;
                placed <- TRUE;
            }

            # current y pos is used by another segment, increment
            y <- y + 1;
            # initialize another y pos if we're at the end of the list
            if(y > length(yread)){
                yread[y] <- minstart-1;
            }
        }
    }

    # find the maximum y pos that is used to size up the plot
    maxy <- length(yread);
    sorted$ypos <- ypos;

    # Now we have all the information, start the plot
    plot.new();
    plot.window(xlim=c(minstart, maxend+((maxend-minstart)/10)), ylim=c(1,maxy));
    axis(3);

    #---
    # This second iteration plots the segments using the found y pos and
    # the start and end values
    #---
    for (r in 1:nrow(sorted)){
        read <- sorted[r,];
        #plot this segment!
        segments(read$Started, maxy-read$ypos, read$Ended, maxy-read$ypos);
    }
}

# Predicting
## Logistic regression

Since we're predicting a binary outcome (a product living or dying), we need a [logistic regression](!Wikipedia).


summary(glm(Dead ~ Type * Days * Profit * FLOSS * Acquisition * Social, data=google[-1],family="binomial"))
step(glm(Dead ~ Type * Days * Profit * FLOSS * Acquisition * Social, data=google[-1],family="binomial"))

summary(glm(Dead ~ Type + Profit + FLOSS + Acquisition + Social + log(Hits), data=google,family="binomial"))
...
Deviance Residuals:
   Min      1Q  Median      3Q     Max
-1.772  -0.914  -0.616   1.109   2.674

Coefficients:
                Estimate Std. Error z value Pr(>|z|)
(Intercept)       2.3953     1.0676    2.24    0.025
Typeprogram       0.9353     0.8180    1.14    0.253
Typeservice       1.2271     0.7893    1.55    0.120
Typething         0.8737     1.1613    0.75    0.452
ProfitTRUE       -0.3833     0.2951   -1.30    0.194
FLOSSTRUE        -0.1481     0.3803   -0.39    0.697
AcquisitionTRUE   0.4893     0.3433    1.43    0.154
SocialTRUE        0.7832     0.3887    2.02    0.044
log(Hits)        -0.3090     0.0567   -5.45    5e-08

(Dispersion parameter for binomial family taken to be 1)

    Null deviance: 452.96  on 348  degrees of freedom
Residual deviance: 402.14  on 340  degrees of freedom
AIC: 420.1
~~~

So, we can say that:

- Google has a past history of screwing up social and then killing them

    This seems useless for predicting since Page got obsessed with social in 2009, and we might expect anything to do with "social" will now either be merged into Google+ or otherwise be kept on life support far longer than before
- Google is deprecating software products in favor of web services

    This should have been obvious to anyone watching - a lot of Google's efforts with Firefox and then Chromium was for improving web browsers as a platform for delivering applications.
- things which charge or show advertising are more likely to survive

    Also obvious, but it's good to have confirmation (if nothing else, it partially validates the data).
- Popularity as measured by Google hits seems to matter

    Likewise obvious... or is it? All this data has been collected after the fact, sometimes many years; what if the data have been contaminated by the fact that something shut down? For example, by a burst of publicity about an obscure service shutting down? (Ironically, this page is contributing to the inflation of hits for any dead service mentioned.) Are we just seeing [leakage](http://www.cs.umb.edu/~ding/history/470_670_fall_2011/papers/cs670_Tran_PreferredPaper_LeakingInDataMining.pdf "'Leakage in Data Mining: Formulation, Detection, and Avoidance', Kaufman et al 2011")?


Investigating further, hits by themselves do matter:

~~~{.R}

summary(glm(Dead ~ log(Hits), data=google,family="binomial"))
...
Deviance Residuals:
   Min      1Q  Median      3Q     Max
-1.491  -0.934  -0.653   1.160   2.540

Coefficients:
            Estimate Std. Error z value Pr(>|z|)
(Intercept)   3.4221     0.7305    4.68  2.8e-06
log(Hits)    -0.3008     0.0549   -5.48  4.3e-08

(Dispersion parameter for binomial family taken to be 1)

    Null deviance: 452.96  on 348  degrees of freedom
Residual deviance: 416.61  on 347  degrees of freedom
AIC: 420.6
~~~

Ideally we would have Google hits from the day before a product was officially killed, but the past is, alas, no longer accessible to us, and we only have hits from searches I conducted 1-5 April 2013.

There are three main problems with the Google hits metric: the Web keeps growing, so 1 million hits in 2000 are not equivalent to 1 million hits in 2013; services which are not killed live longer and can rack up more hits; and the longer ago a product's hits came into existence, the more likely the relevant hits may be to have disappeared themselves.

We can partially compensate by looking at hits averaged by lifespan; 100k hits means much less for something that lived for a decade than 100k hits means for something that lived just 6 months. Does this add anything?

<!--
WARNING: leakage
Yes, but curiously, the higher the average hits is, the *more* likely a product is to have died:

~~~{.R}
google$AvgHits <- google$Hits / google$Started
summary(glm(Dead ~ log(Hits) + log(AvgHits), data=google,family="binomial"))
...
Deviance Residuals:
   Min      1Q  Median      3Q     Max
-1.558  -0.917  -0.613   1.129   2.645

Coefficients:
             Estimate Std. Error z value Pr(>|z|)
(Intercept)     6.231      1.204    5.18  2.3e-07
log(Hits)      -0.718      0.149   -4.81  1.5e-06
log(AvgHits)    0.432      0.141    3.07   0.0021

(Dispersion parameter for binomial family taken to be 1)

    Null deviance: 452.96  on 348  degrees of freedom
Residual deviance: 406.29  on 346  degrees of freedom
AIC: 412.3
~~~

Another form of data leakage comes if we include the end-date; then a random forest, for example, will predict correctly every single shut down except for 4. Why were those 4 wrong? Because they are correctly marked as "dead", but scheduled to die *after* the present day. So the random forests were just emitting 'dead' for everything with an end date before 2013-04-04, and alive for everything thereafter.

~~~{.R}
library(randomForest)
rf <- randomForest(as.factor(Dead) ~ ., data=google[-1])
google[rf$predicted != google$Dead,]
                     Product Dead    Started      Ended     Hits    Type Profit FLOSS Acquisition
24 Gmail Exchange ActiveSync TRUE 2009-02-09 2013-07-01   637000 service  FALSE FALSE       FALSE
30  CalDAV support for Gmail TRUE 2008-07-28 2013-09-16   245000 service  FALSE FALSE       FALSE
37                    Reader TRUE 2005-10-07 2013-07-01 79100000 service  FALSE FALSE       FALSE
38               Reader Play TRUE 2010-03-10 2013-07-01    43500 service  FALSE FALSE       FALSE
39                   iGoogle TRUE 2005-05-01 2013-11-01 33600000 service   TRUE FALSE       FALSE
74            Building Maker TRUE 2009-10-13 2013-06-01  1730000 service  FALSE FALSE       FALSE
75             Cloud Connect TRUE 2011-02-24 2013-04-30   530000 program  FALSE FALSE       FALSE
77   Search API for Shopping TRUE 2011-02-11 2013-09-16   217000 service   TRUE FALSE       FALSE
   Social Days  AvgHits DeflatedHits AvgDeflatedHits
24  FALSE 1603   419.35    9.308e-06         -0.5213
30  FALSE 1876   142.86    4.823e-06         -0.4053
37   TRUE 2824 28868.61    7.396e-03         -2.0931
38  FALSE 1209    38.67    3.492e-07         -0.2458
39   TRUE 3106 11590.20    4.001e-03         -1.6949
74  FALSE 1327  1358.99    1.739e-05         -0.6583
75  FALSE  796   684.75    2.496e-06         -0.5061
77  FALSE  948   275.73    1.042e-06         -0.4080
rf
...
    Type of random forest: classification
                     Number of trees: 500
No. of variables tried at each split: 3

        OOB estimate of  error rate: 2.29%
Confusion matrix:
      FALSE TRUE class.error
FALSE   226    0     0.00000
TRUE      8  115     0.06504



~~~
-->

~~~{.R}
google$AvgHits <- google$Hits / as.integer(Sys.Date() - google$Started)
summary(glm(Dead ~ log(Hits) + log(AvgHits), data=google,family="binomial"))
...
Coefficients:
             Estimate Std. Error z value Pr(>|z|)
(Intercept)    -2.157      1.559   -1.38    0.166
log(Hits)       0.495      0.206    2.41    0.016
log(AvgHits)   -0.837      0.213   -3.93  8.4e-05

(Dispersion parameter for binomial family taken to be 1)

    Null deviance: 452.96  on 348  degrees of freedom
Residual deviance: 398.16  on 346  degrees of freedom
AIC: 404.2
~~~

This is more than a little strange; the higher the average hits, the less likely to be killed makes perfect sense but then, surely the higher the hits, the less likely as well? But no.

What about the growth objection? We can estimate the size of Google's index at any period and interpret the current hits as a fraction of the index when the service died (example: suppose Answers has 1 million hits, died in 2006, and in 2006 the index held 1 billion URLs, then we'd turn our 1m hit figure into 1/1000 or 0.001); then

We'll deflate the hits by first estimating the size of the index by fitting an exponential to the rare public reports and third-party estimates of the size of the Google index:

~~~{.R}
index <- read.csv("wiki/docs/2013-google-index.csv", colClasses=c("Date","double","character"))
# an exponential doesn't fit too badly:
model1 <- lm(log(Size) ~ Date, data=index); summary(model1)

# plot logged size data and the fit:
plot(log(index$Size) ~ index$Date, ylab="WWW index size", xlab="Date")
abline(model1)

~~~
http://i.imgur.com/tv1LyaA.png
It fits reasonably well. A sigmoid might work a bit better, but maybe not, given the large disagreements towards the end.

Then we can divide total hits for a property by the total estimated size of the Google index when that property started

~~~{.R}
google$DeflatedHits <- google$Hits / exp(predict(model1, newdata=data.frame(Date = google$Started)))
summary(glm(Dead ~ log(Hits) + log(AvgHits) + log(DeflatedHits), data=google,family="binomial"))
...
Coefficients:
                  Estimate Std. Error z value Pr(>|z|)
(Intercept)        -20.707     11.667   -1.77   0.0759
log(Hits)            1.972      0.953    2.07   0.0385
log(AvgHits)        -1.853      0.686   -2.70   0.0069
log(DeflatedHits)   -0.442      0.273   -1.62   0.1050

(Dispersion parameter for binomial family taken to be 1)

    Null deviance: 452.96  on 348  degrees of freedom
Residual deviance: 395.23  on 345  degrees of freedom
AIC: 403.2
~~~

Finally, let's combine the two strategies: deflate and then average.

~~~{.R}
google$AvgHits <- google$Hits / as.integer(Sys.Date() - google$Started)
google$DeflatedHits <- google$Hits / exp(predict(model1, newdata=data.frame(Date = google$Started)))
google$AvgDeflatedHits <- log(google$AvgHits) / log(google$DeflatedHits)
summary(glm(Dead ~ log(Hits) + log(AvgHits) + log(DeflatedHits) + AvgDeflatedHits, data=google, family="binomial"))
...
Coefficients:
                  Estimate Std. Error z value Pr(>|z|)
(Intercept)       -23.7689    12.1867   -1.95   0.0511
log(Hits)           2.2056     0.9936    2.22   0.0264
log(AvgHits)       -2.0244     0.7182   -2.82   0.0048
log(DeflatedHits)  -0.5238     0.2870   -1.82   0.0680
AvgDeflatedHits    -0.0644     0.0604   -1.07   0.2865

(Dispersion parameter for binomial family taken to be 1)

    Null deviance: 452.96  on 348  degrees of freedom
Residual deviance: 394.29  on 344  degrees of freedom
AIC: 404.3
~~~

If we toss in all the non-hits predictors and look for a reduced simpler model with reasonable fit, we get:

~~~{.R}
summary(step(glm(Dead ~ Type + Profit + FLOSS + Acquisition + Social + log(Hits) + log(AvgHits) + log(DeflatedHits) + AvgDeflatedHits, data=google, family="binomial")))
...
Coefficients:
                  Estimate Std. Error z value Pr(>|z|)
(Intercept)        -22.396     11.737   -1.91   0.0564
AcquisitionTRUE      0.627      0.350    1.79   0.0734
SocialTRUE           0.902      0.394    2.29   0.0220
log(Hits)            2.117      0.957    2.21   0.0270
log(AvgHits)        -1.996      0.690   -2.89   0.0038
log(DeflatedHits)   -0.477      0.276   -1.73   0.0835
~~~

Most of the predictors were removed as not helping a lot, 3 of the 4 hit variables survived (but not the both averaged & deflated hits, suggesting it wasn't adding much in combination), and we see two of the better predictors from earlier survived: whether something was an acquisition and whether it was social.

The original hits variable has the wrong sign, as expected of data leakage; now the average and deflated hits have the predicted sign (the higher the hit count, the lower the risk of death), but this doesn't put to rest my concerns: the average hits has the right sign, yes, but now the effect size seems way too high - we reject the hits with a log-odds of +2.1 as obviously contaminated and a correlation almost 4 times larger than one of the known-good correlations (being an acquisition), but the average hits is -2 & almost as big a log odds! The only variable which seems trustworthy is the deflated hits: it has the right sign and is a more plausible 5x smaller. I'll use just the deflated hits variable (although I will keep in mind that I'm still not sure it is free from data leakage).

# Survival curve

Normally, we would expect a sort of diagonal: services opened early on would be shut down early on, services opened later would be shut down later, etc. Instead, we get a horizontal line with no diagonal trend. What's going on? A survival curve might help:

~~~{.R}
library(survival)
dead.surv <- survfit(Surv(google$Days) ~ google$Dead, conf.type="none")

Surv(google$Started, google$Days, google$Dead, type="right")
surv <- survfit(Surv(as.integer(google$Started), google$Dead, type="right") ~ 1)
plot(surv, xlab="Days", ylab="Survival Probability")
~~~
http://i.imgur.com/GvYTH8x.png

~~~{.R}
library(randomForest)
rf <- randomForest(as.factor(Dead) ~ ., data=google[-1])
google[rf$predicted != google$Dead,]
rf
# data leakage on the end-date! those 4 were wrong because they are dead, but scheduled to due after today. the random forests were just emitting 'dead' for everything with an end date before 2013-04-04 and alive for then or after


rf <- randomForest(as.factor(Dead) ~ google$Started + Hits + Type + Profit + FLOSS + Acquisition + Social + Days, data=google[-1])
google[rf$predicted != google$Dead,]$Product
rf

# 17% error rate. not much of an improvement over a constant guess of 'alive', though, which has a 35% error rate:

sum(google$Dead) / nrow(google)
[1] 0.3524

rf <- randomForest(as.factor(Dead) ~ Hits + Type + Profit + FLOSS + Acquisition + Social, data=google)
google[rf$predicted != google$Dead,]$Product
rf
~~~

;_; if I take out the starting date the random forests error rate shoots up to 30%. the stupidest possible predictor scoring 35%, remember. on a dataset of 321 google properties. implying that all my painstakingly gathered data basically doesn't amount to a hill of beans


The abrupt increase in mortality in the middle is suspicious. We can plot lifetime against shut-down to get a better picture:

plot(google$Days ~ google$Ended, xlab="Opened", ylab="Total lifetime")
http://i.imgur.com/P3l4jxr.png

Here something jumps out of the plot: almost every shut down is in 2012 or 2013. As late as 2011, there are a grand total of perhaps 3 shut-downs (out of 39), and then in late 2011 there is the start of a veritable wall of shut-downs of services, from services as young as 365 days (Google Aardvark) to as old as 8.5 years (iGoogle).

~~~{.R}
head(dead[dead$Ended > "2012-01-01",], 1); tail(dead[dead$Ended > "2012-01-01",], 1)
   Product    Started      Ended Days
2 Aardvark 2011-02-01 2012-02-01  365
                   Product    Started      Ended Days
78 Search API for Shopping 2011-02-11 2013-09-16  939
~~~

To emphasize this bulge in late 2011-2012, we can plot the histogram by year and then the density:

~~~{.R}
hist(google$Ended, breaks=seq.Date(as.Date("2005-01-01"), as.Date("2014-01-01"), "years"),
                   main="Shut downs per year", xlab="Year")
~~~
http://i.imgur.com/RxBXN0j.png


~~~{.R}
plot(table(months(dead$Ended)))
# beware the ides of September?
chisq.test(table(months(dead$Ended)))

    Chi-squared test for given probabilities

data:  table(months(dead$Ended))
X-squared = 38.52, df = 11, p-value = 6.4e-05

Popularity help? doesn't look like it:

plot(dead$Days, log(dead$Hits))

cor.test(dead$Days, log(dead$Hits))

    Pearson's product-moment correlation

data:  dead$Days and log(dead$Hits)
t = 1.037, df = 93, p-value = 0.3025
alternative hypothesis: true correlation is not equal to 0
95 percent confidence interval:
 -0.09673  0.30194
sample estimates:
   cor
0.1069

plot(density(as.numeric(dead$Ended)), main="Shut down density over time")
~~~
http://i.imgur.com/0XpKAoi.png

The kernel density brings out an aspect of shut downs we might have missed before: there seems to be an absence of recent shut downs. There are 4 shut downs scheduled for 2013 but the last one is scheduled for November, suggesting that we have seen the last of the 2013 casualties and that any future shut downs may be for 2014.

What explains such a graph over time? The obvious candidate is the 4 April 2011 accession of Larry Page to CEO, replacing Eric Schmidt who had been hired to provide "adult supervision" for pre-IPO Google. He respected Steve Jobs greatly (he and Brin suggested, before meeting Schmidt, that their CEO be Jobs). Isaacon's _Steve Jobs_ records that before his death, Jobs had strongly advised Page to "focus", and asked "What are the five products you want to focus on?", saying "Get rid of the rest, because they're dragging you down." And indeed, on [14 July 2011](https://plus.google.com/+LarryPage/posts/dRtqKJCbpZ7) Page posted:

> ...Greater focus has also been another big feature for me this quarter -- more wood behind fewer arrows. Last month, for example, we announced that we will be closing Google Health and Google PowerMeter. We've also done substantial internal work simplifying and streamlining our product lines. While much of that work has not yet become visible externally, I am very happy with our progress here. Focus and prioritization are crucial given our amazing opportunities.

While some have [tried to disagree](http://thenextweb.com/google/2013/01/12/larry-page-did-well-to-ignore-steve-jobs/ "Larry Page ignored Steve Jobs's deathbed advice, and Google is doing great"), it's hard not to conclude that indeed, a wall of shut downs followed in late 2011 and 2012. But this sound very much like a one-time purge: if one has a new focus on focus (if you'll pardon the expression), then you may not be starting up as many services as before.

Deadweight loss: in some of the more successful acquisitions, google's modus operandi was to take a paid or premium service and make it free. Analytics, Maps, Earth, Feedburner all come to mind as services whose predecessors (multiple, in the cases of Maps and Eart) charged money for their services (sometimes a great deal). this leads to deadweight loss as people do not use them, who would benefit to some degree but not to the full amount of the price (plus other factors like riskiness of investing time and money into trying it out). Google cites figures like billions of users over the years for several of these formerly-premium services, suggesting the gains from reduced deadweight loss are large.

maybe blogger is not as doomed as I thought. it seems to e favored with g4etting exclusive access to quite a few otherwise shut down things, for example, Scribe, Friend Connect, it was the ground zero for google's Dynamic Views skin redesign, and google is still heavily using blogger for all the official announcements even into the google+ era


> We discovered there's been a total of about 251 independent Google products since 1998 (avoiding add-on features and experiments that merged into other projects), and found that 90, or approximately 36% of them have been cancelled. Awesomely, we also collected 8 major flops and 14 major successes, which means that 36% of its high-profile products are failures. That's quite the coincidence! NOTE: We did not manipulate data to come to this conclusion. It was a happy accident.
http://www.alltechienews.com/posts/google-fails-36-of-the-time

interesting the patterns showing up in these shut downs.
you can see the wall of shut downs when Page became CEO; you can see a clump of shut downs when Chromium was launched and Google ceased caring about firefox support and extensions; you can see its pre-YouTube video efforts; and you can see a whole rash of failed social media stuff
plus there are like 4 or 5 things which all got rolled into Docs+Drive

18:51:02 < Kiba> focus, focus, focus
18:52:12 <@gwern> Kiba: not really. the docs+drive thing is just logical; the
                  Firefox stuff is interesting and sorta ruthless competition;
                  the YouTube-competitor and social media stuff are just failures

# Predictions


TODO: lifespan predictions for: http://www.ats.ucla.edu/stat/r/examples/asa/asa_ch2_r.htm
Google Alerts
Blogger
FeedBurner
Scholar
Book Search

Google products I regularly use; what are the predictions and how well can I cope with any shut downs?

1. Search
2. Gmail
3. Analytics
4. AdSense
5. Calendar
6. Alerts
7. JavaScript CDN
8. Google+
9. Docs (Spreadsheet)

Alerts search for updates: `google ("shut down" OR "shutting" OR "closing" OR "killing" OR "abandoning" OR "leaving")`

# See also

> In the long run, the utility of all non-Free software approaches zero. All non-Free software is a dead end.^[[Mark Pilgrim](!Wikipedia), ["Freedom 0"](http://web.archive.org/web/20110726001925/http://diveintomark.org/archives/2004/05/14/freedom-0); ironically, Pilgrim (hired by Google in 2007) seems to be responsible for at least one of the entries being marked dead, Google's Doctype tech encyclopedia, since it disappeared around the time of his "infosuicide" and has not been resurrected - it was only *partially* FLOSS.]

- [Archive Team](!Wikipedia)
- [Archiving URLs]()
- [Wikipedia and Knol]()

> I spur my horse past ruins \
> Ruins move a traveler's heart \
> the old parapets high and low \
> the ancient graves great and small \
> the shuddering shadow of a tumbleweed \
> the steady sound of giant trees. \
> But what I lament are the common bones \
> unnamed in the records of Immortals.^[[Han-Shan](!Wikipedia "Hanshan (poet)"); #18 in _The Collected Songs of Cold Mountain_, Red Pine 2000, ISBN 1-55659-140-3]


TODO: finished?
- run `markdown-lint`
- Ping the dead service sources like http://www.guardian.co.uk/technology/2013/mar/22/google-keep-services-closed http://www.theatlantic.com/technology/archive/2013/03/finale-for-now-on-googles-self-inflicted-trust-problem/274286/ http://www.slate.com/articles/technology/technology/2013/03/google_reader_why_did_everyone_s_favorite_rss_program_die_what_free_web.html
- https://news.ycombinator.com/item?id=5495523
- thank gostips@gmail.com/http://googlesystem.blogspot.com/
- ask for help http://www.reddit.com/r/MachineLearning/ / Coursera 'Data Analysis' forum
