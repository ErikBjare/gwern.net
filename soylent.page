---
title: Diet Variance: Soylent study
description: Proposal to use meal-replacements to partition daily variance between diet and other factors
tags: psychology, experiments, statistics
created: 22 May 2013
status: notes
belief: possible
...

<!--
my criticism of a regular diet: http://lesswrong.com/lw/hht/link_soylent_crowdfunding/90y7
add a link in http://lesswrong.com/lw/hht/link_soylent_crowdfunding/914n

https://soylent.reddit.com/

Soylent Orange?

http://lesswrong.com/lw/h2h/soylent_orange_whole_food_open_source_soylent/

Hacker School soylent?

http://diy.soylent.me/recipes/hacker-school-soylent
http://diy.soylent.me/recipes/2potatoes-hacker-school-not-modified
http://www.cookingfor20.com/2013/06/18/hacker-school-soylent-recipe/
http://diy.soylent.me/recipes/my-take-on-hacker-school-soylent-recipe
-->

<!-- MealSquares; Vaniver gave me 5 on 29 December 2014; ate last on 6 January 2015, so 30/01/02/05/06? -->
<!-- first Soylent 1.5 order: 28 meals / 7 bags, $54, 17 Sep 2015 and arrived on 28 September 2015; only discount is for subscription, so didn't bother with a big order - I can try out the first order to see what it's like, then stockpile subsequent orders if need be. -->

Time comparison of my regular food & Soylent, inclusive of total shopping, meal prep, consumption, & cleanup time:

- 16 Sep 2015: standard egg/rice/spinach/kimchi breakfast: 18m33s; bread lunch: 3m; lentil soup dinner: 17.3m; second dinner of sausage: 22m
- 17 Sep 2015: breakfast, 26m30s; grocery planning/shopping/putting-away/post-shopping-meal, 3h17m
- 18 Sep 2015: breakfast, 19m22s; dinner, 26m33s; dessert, 15m
- 19 Sep 2015: breakfast, 21m31s; cooking chili & pear picking, 1h58m; oatmeal dinner, 5m
- 20 Sep 2015: breakfast, 25m; bread, 8m30s; lunch, 19m
- 21 Sep 2015: breakfast, 44m; dinner, 27m
- 22 Sep 2015: breakfast, 38m; pear sauce, 1h19m
- 23 Sep 2015: breakfast, 30m; lunch, 20m; dinner, 25m
- 24 Sep 2015: breakfast, 24m; lunch, 30m
- 25 Sep 2015: breakfast, 18m
- 26 Sep 2015: breakfast, 21m
- 27 Sep 2015: breakfast, 28m; lunch, 10m
- 28 Sep 2015: breakfast, 18m

Soylent usage:

- 28 Sep 2015: 2 meals
- 29 Sep 2015: 1 meal

    initial impressions: much better cold. Minor grittiness goes away after sitting overnight. Soylent Inc conveniently provides a big pitcher to make your Soylent in, narrow and tall but *just* short enough to fit in my refrigerator's top shelf. Well suited to shaking and pouring. Flavor is unoffensive and has a sort of nutty/oatmeal overtone. Some stomach rumblings the first night. This stomach rumbling and flatulence is a well-known effect of Soylent and I'm afraid it hasn't gone away for me even by 2016...
- 30 Sep 2015: 3 meals
- 2 Oct: 1 meal
- 3 Oct: 2 meals
- 18 Oct: 1 meal
- 19 Oct: 2 meals
- 21 Oct: 1 meal
- 22 Oct: 1 meal
- 27 Oct: 1 meal
- 28 Oct: 1 meal
- 2 Nov: 1 meal
- 12 Nov: 1 meal
- 14 Nov: 1 meal
- 21 Nov: 1 meal
- 25 Nov: 1 meal
- 28 Nov: 1 meal
- 2016
- 19 May: 3
- 22 May: 3
- 2 June: 3
- 5 June: 1
- 6 June: 1
- 7 June: 1
- 8 July: 3

<!-- variance component estimation for gut bacteria? https://www.reddit.com/r/Microbiome/comments/44j6zg/do_we_know_how_important_gut_microbiome/ using American Gut Project data http://nbviewer.jupyter.org/github/biocore/American-Gut/blob/master/ipynb/index.ipynb ; maybe also http://www.ncbi.nlm.nih.gov/pmc/articles/PMC4255478/ or http://www.ncbi.nlm.nih.gov/pmc/articles/PMC3787266/ ? -->

# Variance experiment
## Background

One of my hobbies is gathering data & [running experiments](/tags/experiments) on myself.
For the most part, I can take the usual experimental approaches of measure data on relevant variables such as my usual mood/productivity MP self-rating, randomize an intervention, and regress to get the mean treatment effect.

One set of data I *don't* collect is data about my diet and the food I eat: this is tedious, difficult to do right (weighing portions and calculating calories or nutritional value), cannot be automated, and I don't want to do it unless I absolutely have to - say, because diet turns out to be a large influence on my daily energy levels and explain a lot of currently unexplained variation.
I could try to estimate how big the influence is by simply collecting the data and regressing on it but that would incur a lot of work for possibly little benefit.

What if I could do something easier, to just estimate the total [variance](!Wikipedia) attributable to differences in food rather than nailing down the exact effect of each food?
It occurred to me that this was a similar setup as with the famous twin studies of the genetic heritability of traits: since one knows how related on average identical twins, fraternal twins, siblings etc are, one can examine the variation of sets and estimate how much all genes influence traits, without ever having to sequence a genome or identify the exact effect of a particular gene.
But instead of heritability with genes, it's 'heritability' for food.

In this scenario, the role of identical genomes would be played by identical meals (prepared meals like those sold to dieters, or liquid meals like the notorious Soylent & its knockoffs & Mealsquares).
Days on which all 3 meals were one of the identical meals would be similar to identical twins (a genetic relatedness of 1), days with 2 of 3 meals (0.66) or 1 of 3 meals identical (0.33) would be like siblings, and days of normal food would be like unrelated (0) people from the general population.
Do this for a month or two, then plug daily data on mood/energy/work into one's favorite R library along with the meal similarity for each day (which would be possible to record as compared to my exact diet), and see if the reduction in variance is something meaningful like >10%.
If it's large, then I've learned I may not want to neglect recording data on my diet, since apparently day-to-day differences
(I suggested this [some time ago](https://www.reddit.com/r/HPMOR/comments/2i1mb6/hpmor_progress_report_oct_2014/ckzlqk3) and have been looking into it in more detail since.)

## Design


The outcome variable is my MP self-rating; it's not a good outcome variable, but I am still working on creating a better productivity variable using factor analysis, so it will have to do.
I have MPs going back several years, so the variance of normal eating can be estimated from that and reducing the sample size requirements.

The full blown SEM apparatus of twin study designs turn out to not be necessary here as all days are expected to have the same mean (hence, 'pairs' of days will have zero correlation) and I can frame the problem as a test of different variances in two 'populations': days with 0 Soylent meals and days with 3 Soylent meals (to maximize the contrast).
This can be done as an [_F_-test](!Wikipedia "F-test").

Randomization can be done as pairs of days to avoid temporal trends.
I began 7 July 2016 with 35 Soylent bags available.


### Power

Twin and family designs are powerful and can estimate low levels of heritability with surprisingly little data - for an additive food factor explaining 10% of variance on a continuous variable using 100% vs 50% identity (MZ vs DZ), [a twin power calculator](https://genepi.qimr.edu.au/general/TwinPowerCalculator/) indicates that >28 pairs are necessary, so >14 pairs each, or >28 days each and >56 total, which is not too bad.
Unfortunately, as I know from discussions of sex-linked variance in psychology, tests of difference in variance in general populations (as opposed to differences in means) tend to be data-hungry, presumably because twin studies benefit from pairs of twins having widely distributed means.

The power of a variance test can probably be estimated with standard ANOVA-linked power routines.
However, my mood/productivity self-rating is not a continuous normal variable as such routines would assume, but a discrete variable with 3 levels, which will be less informative (so any standard power routine will underestimate the power, possibly by a lot).
So a power simulation is necessary.

We can simulate realistic MP data by generating normal deviates with the same population mean/SD as my current MP dataset, rounding, and then ceiling/flooring:

~~~{.R}
mp <- read.csv("~/selfexperiment/mp.csv", colClasses=c("Date", "integer"))
sd(mp$MP, na.rm=T)
# [1] 0.6947745861
mean(mp$MP, na.rm=T)
# [1] 3.127727856
M <- round(rnorm(100, mean=3.127727856, sd=0.6947745861)); M2 <- ifelse(M>4, 4, M); M3 <- ifelse(M2<2, 2, M2)
M3
#  [1] 2 3 2 3 3 2 2 3 3 2 3 3 3 2 4 4 2 3 4 3 3 2 4 3 4 3 3 4 4 2 4 3 4 3 3 4 3 3 3 2 3 3 4 3 3 3 2 3 3 3 3 4 4 3 2 4 3 3 2 2 4 3 4 3 4 2 3 2 4 3 3 3 4 4 2 4 3 3 2 4 3 2 3 4 4 3 4 3 3 3 3 3
# [93] 4 3 2 3 3 4 2 3
sd(M3)
# [1] 0.6936945446
mean(M3)
# [1] 3.06
~~~

How to generate such deviates with particular diet-related reduction in variance, though? Not too hard:

~~~{.R}
mpGenerate <- function(n, m, s, variance) {
    M <- round(rnorm(n, mean=m, sd=sqrt((1-variance))*s))
    M2 <- ifelse(M>4, 4, M)
    M3 <- ifelse(M2<2, 2, M2)
    return(M3) }
~~~

Once that is set up, it is straightforward to pass in a set of possible reductions in variance, loop over sample sizes, simulate out a few hundred 3-meal Soylent datasets, and record the result of an _F_-test comparing the Soylent variance with the past non-Soylent MP variance:

~~~{.R}
mpVariancePowerAnalysis <- function(heritabilities, n_additional) {

 mp <- read.csv("~/selfexperiment/mp.csv", colClasses=c("Date", "integer"))
 population_m <- mean(mp$MP, na.rm=TRUE)
 population_sd <- sd(mp$MP, na.rm=TRUE)

 iters <- 300

 ## preallocate space for all the possible combinations, so we don't spend all our time
 ## merging data frames
 results <- data.frame(N=rep(NA, n_additional*iters*length(heritabilities)),
    Heritability=rep(NA, n_additional*iters*length(heritabilities)),
    Pvalue=rep(NA, n_additional*iters*length(heritabilities)),
    Ratio=rep(NA, n_additional*iters*length(heritabilities)))

 for (heritability in heritabilities) {
   for (n in 10:n_additional) {
    for (i in 1:iters) {
      allSoylent <- mpGenerate(n, population_m, population_sd, heritability*(3/3))

      test <- var.test(mp$MP, allSoylent)

      results$N[n*i*which(heritabilities==heritability)]            <- n
      results$Heritability[n*i*which(heritabilities==heritability)] <- heritability
      results$Pvalue[n*i*which(heritabilities==heritability)]       <- test$p.value
      results$Ratio[n*i*which(heritabilities==heritability)]        <- test$estimate
      }
   }
 }
 return(results)
}
power <- rbind(power, mpVariancePowerAnalysis(seq(0.1, 0.5, by=0.05), 1000))
powerSummary <- aggregate(Pvalue ~ N + Heritability, function(ps) { sum(ps<=0.05)/length(ps)})
plot(powerSummary[power$Heritability==0.5,])
library(ggplot2)
qplot(powerSummary$Pvalue, powerSummary$N, facets=as.ordered(powerSummary$Heritability), data=powerSummary)
~~~

Power depends tremendously on how much variance is reduced. At an implausible 50%, a variance test is easy. At a more reasonable 10%, it requires hundreds of days.
