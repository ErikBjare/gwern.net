---
title: Resorting Media Ratings
description: Commandline tool providing interactive Bayesian pairwise ranking and sorting of items
created: 7 Sep 2015
tags: computer science, statistics
status: in progress
belief: log
...


In rating hundreds of media on a review site like GoodReads, Amazon, MyAnimeList etc, the distributions tend to become 'lumpy' and concentrate on the top few possible ratings: if it's a 10-point scale, you won't see many below 7, usually, or if it's 5-stars [then anything](https://xkcd.com/325/) [below 4-stars indicates profound hatred](https://xkcd.com/1098/), leading to a J-shaped distribution and the Internet's version of [grade inflation](!Wikipedia).
After enough time and inflation, the ratings have degenerated into a noninformative binary rating scale, and some sites, recognizing this, abandon the pretense, like [YouTube switching from 5-stars to like/dislike](http://youtube-global.blogspot.com/2009/09/five-stars-dominate-ratings.html "Five Stars Dominate Ratings").

This is unfortunate if you want to provide ratings & reviews to other people and indicate your true preferences; when I like on [MALgraph](http://graph.anime.plus/gwern/ratings,anime) and see that over half my anime ratings are in the range 8-10, then, my ratings having degenerated into a roughly 1-3 scale (crap-good-great) makes it harder to see which ones are truly worth watching & also which ones I might want to go back and rewatch.
So the ratings carry much less information than one might have guessed from the scale (a 10-point scale has 3.32 bits of information in each rating, but if it's de facto degenerated into a 3-point scale, then the information has halved to 1.58 bits).
If instead, my ratings were uniformly distributed over the 1-10 scale, such that 10% were rated 1, 10% were rated 2, etc, then my ratings would be much more informative, and my opinion clearer.
(First-world problems, perhaps, but still annoying.)

For only a few ratings like 10 or 20, it's easy to manually review and rescale ratings and resolve the ambiguities (which '8' is worse than the other '8's and should plummet down to 7?), but past that, it starts to become tedious and because judgment is so fragile and subjective and ['choice fatigue'](!Wikipedia "Decision fatigue") begins to set in, I find myself having to repeatedly scan the list and ask myself "is X *really* better than Y...? hm...".
So unsurprisingly, for a large corpus like my 408 anime or 2059 books, I've never bothered to try to do this - much less do it occasionally as the ratings drift.

If I had some sort of program which could query me repeatedly for new ratings, store the results, and then spit out a consolidated list of exactly how to change ratings, then I might be able to, once in a great while, correct my ratings.
This would help us re-rate our existing media corpuses, but it could also help us sort other things: for example, we could try to prioritize future books or movies by taking all the ones we marked 'to read' and then doing comparisons to rank each item by how excited we are about it or how important it is or how much we want to read it soon.
(If we have way too many things to do, we could also sort our overall To Do lists this way, but probably it's very easy to sort them by hand.)

But it can't ask me for absolute ratings, because if I was able to easily give uniformly-distributed ratings, I wouldn't have this problem in the first place!
So instead it should calculate rankings, and then take the final ranking and distribute them across however many buckets there are in the scale: if I rate 100 anime for MAL's 1-10 ranking, then it will put the bottom 10 in the '1' bucket, the 10-20th into the '2' bucket, etc
This cannot be done automatically.

How to get rankings: if there are 1000 media, it's impossible for me to explicitly rank a book '#952', or '#501'. Nobody has that firm a grip.
Perhaps it would be better for it to ask me to compare pairs of media?
Comparison is much more natural, less fatiguing, and helps my judgment by reminding me of what other media there are and how I think of them - when a terrible movie gets mentioned in the same breath as a great movie, it reminds you why one was terrible and the other great.
Comparison also immediately suggests an implementation as the classic comparison sort algorithms like Quicksort or Mergesort, where the comparison operation simply calls out to the user; yielding a reasonable O(n * log(n)) number of queries (and possibly much less, O(n), if we have the pre-existing ratings and can treat it as an [adaptive sort](!Wikipedia)).

The comparison-sort algorithms make an assumption that is outrageously unrealistic in this context: they assume that the comparison is 100% accurate.
That is, say, you have two sorted lists of 1000 elements each and you compare the lowest element of one with the highest element of the other and the first is higher, then all 1000 items in the first are higher than all 1000 items in the second, that of the  1000^2=1000000 possible pairwise comparisons, the sorting algorithms assume that not a single item is out of place, not a single pairwise comparison would be incorrect.
This assumption is fine in programming, since the CPU is good at comparing bits for equality & may go trillions of operations without making a single mistake; but it's absurd to expect this of a human for any task, much less one in which we are clumsily feeling for the invisible movements of our souls in response to great artists.

Our comparisons of movies, or books, or music, *are* error-prone and so we need some sort of statistical sorting algorithm.

So we'd like a commandline tool which consumes a list of pairs of media & ratings, then queries the user repeatedly with pairs of media to get the user's rating of which one is better, somehow modeling underlying scores while allowing for the user to be wrong in multiple comparisons and ideally picking whatever is the 'most informative' next pair to ask about, and after enough questions, inferring the full ranking of media and mapping it onto a uniform distribution for a particular scale.

The natural way to see the problem is to treat every competitor as having an unobserved latent variable 'quality' or 'ability' or 'skill' on an ordinal scale, and then weaken transitivity to chain our comparisons: if A beats B and B beats C, then *probably* A beats C, weighted by how many times we've observed beating and have much precise our estimate of A-C's latent variables are.
Paired or comparison-based data comes up a lot in competitive contexts, such as the famous [Elo rating system](!Wikipedia) or the Bayesian [TrueSkill](!Wikipedia).
A general model for dealing with it is the [Bradleyâ€“Terry model](!Wikipedia).

There are at least two packages in R for dealing with Bradley-Terry models, [`BradleyTerry2`](http://www.jstatsoft.org/v48/i09/paper"Bradley-Terry Models in R: The BradleyTerry2 Package") and [`prefmod`](http://www.jstatsoft.org/v48/i10/paper "prefmod: An R Package for Modeling Preferences Based on Paired Comparisons, Rankings, or Ratings").
The latter supports more sophisticated analyses than the former, but it wants its data in an inconvenient format, while `BradleyTerry2` is more easily incrementally updated.
(I want to supply my data to the library as a long list of triplets of media/media/rating, which is then easily updated with user-input by simply adding another triplet to the bottom; but `prefmod` wants a column for each possible media, which is awkward to start with and gets worse if there are 1000+ media to compare.)

Here is a worked-out example using `BradleyTerry2`.
I start with a few MAL anime ratings, which exhibit considerable lumpiness with almost half of them clustered onto a single rating & obscuring the real differences between them.
The original ratings are useful information which should not be ignored; if BT-2 were a Bayesian library, we could use them as very informative priors, but it's not, so instead I adopt a hack in which the program runs through the list, comparing each anime with the next anime, and if equal, it's treated as a tie, and otherwise they are recorded as a win/loss - so even from the start we've made a lot of progress towards inferring their latent scores.
Then a loop asks the user _n_ comparisons; we want to ask about the media whose estimates are most uncertain, and a way of estimating that is looking at which media have the biggest [standard error](!Wikipedia).
You might think that asking about only the two media with the current biggest standard error would be a great strategy, but surprisingly, because of the original ratings' inclusion, this leads to asking about the same media again and again; so instead, it picks randomly with weights for each media of its standard error, so it usually focuses on the most uncertain ones but will occasionally ask about other media as well.
After all the questions are asked, a final estimation is done, the media are sorted by rank, and mapped back onto the 1-10 MAL rating scale.
For this example run, answering all the questions is intuitive and easy, typically requiring no thought, and the final ranking is plausible: I would say specifically that _El Hazard: The Wanderers_ &  _Chobits_ are ranked 1 too high, and _.hack//Sign_ & _Mai-HiME_ should move upwards to replace them, but those might have been fixed with some more questions and otherwise it looks good.

~~~{.R}
library(BradleyTerry2)
## http://myanimelist.net/animelist/gwern&show=0&order=4
ranking <- read.csv(stdin(),header=TRUE)
Media,Rating
"Cowboy Bebop", 10
"Monster", 10
"Neon Genesis Evangelion: The End of Evangelion", 10
"Gankutsuou", 10
"Serial Experiments Lain", 10
"Perfect Blue", 10
"Jin-Rou", 10
"Death Note", 10
"Last Exile", 9
"Fullmetal Alchemist", 9
"Gunslinger Girl", 9
"RahXephon", 9
"Trigun", 9
"Fruits Basket", 9
"FLCL", 9
"Witch Hunter Robin", 7
".hack//Sign", 7
"Chobits", 7
"Full Metal Panic!", 7
"Mobile Suit Gundam Wing", 7
"El Hazard: The Wanderers", 7
"Mai-HiME", 6
"Kimi ga Nozomu Eien", 6


comparisons <- NULL
## seed comparison dataset based on input data; same rating == tie
for (i in 1:(nrow(ranking)-1)) {
 rating1 <- ranking[i,]$Rating
 media1 <- ranking[i,]$Media
 rating2 <- ranking[i+1,]$Rating
 media2 <- ranking[i+1,]$Media
 if (rating1 == rating2)
  {
     comparisons <- rbind(comparisons, data.frame("Media.1"=media1, "Media.2"=media2, "win1"=0.5, "win2"=0.5))
  }
  else{ if (rating1 > rating2)
           {
            comparisons <- rbind(comparisons, data.frame("Media.1"=media1, "Media.2"=media2, "win1"=1, "win2"=0))
            } else {
              comparisons <- rbind(comparisons, data.frame("Media.1"=media1, "Media.2"=media2, "win1"=0, "win2"=1))
                   } } }
### print(comparisons)

priorRankings <- BTm(cbind(win1, win2), Media.1, Media.2, ~ Media, id = "Media", data = comparisons)
## higher=better:
### print(summary(priorRankings))
### print(sort(BTabilities(priorRankings)[,1]))

set.seed(2015-09-07)
nQuestions <- 20
for (i in 1:nQuestions) {

 ## with the current data, calculate and extract the new estimates:
 updatedRankings <- BTm(cbind(win1, win2), Media.1, Media.2, ~ Media, id = "Media", data = comparisons)
 coefficients <- BTabilities(updatedRankings)
 ### print(coefficients)

 ## pick two media to compare at random but weighted by relative size of standard-error,
 ## which is a crude indicator of its uncertainty and the ones where value of information is high:
 targets <- sample(row.names(coefficients), 2, prob=coefficients[,2])
 media1 <- targets[1]
 media2 <- targets[2]

 cat(paste("Do you like '", as.character(media1), "' better than '", as.character(media2), "'? (1=yes, 0=tied/unsure, -1=second is better)"))

 rating <- as.integer(readline("1/0: "))

 if (rating==1) {
   comparisons <- rbind(comparisons, data.frame("Media.1"=media1, "Media.2"=media2, "win1"=1, "win2"=0)) }
  else { if (rating==-1) { comparisons <- rbind(comparisons, data.frame("Media.1"=media1, "Media.2"=media2, "win1"=0, "win2"=1)) }
   else { comparisons <- rbind(comparisons, data.frame("Media.1"=media1, "Media.2"=media2, "win1"=0.5, "win2"=0.5)) } }
}
# Do you prefer ' Monster ' to ' Kimi ga Nozomu Eien '? (-1=second is better, 0=tied, 1=first is better)1/0: 1
# Do you prefer ' Perfect Blue ' to ' Death Note '? (-1=second is better, 0=tied, 1=first is better)1/0: -1
# Do you prefer ' Fruits Basket ' to ' Kimi ga Nozomu Eien '? (-1=second is better, 0=tied, 1=first is better)1/0: 1
# Do you prefer ' Gankutsuou ' to ' Neon Genesis Evangelion: The End of Evangelion '? (-1=second is better, 0=tied, 1=first is better)1/0: -1
# Do you prefer ' Gankutsuou ' to ' Trigun '? (-1=second is better, 0=tied, 1=first is better)1/0: 1
# Do you prefer ' Neon Genesis Evangelion: The End of Evangelion ' to ' Mai-HiME '? (-1=second is better, 0=tied, 1=first is better)1/0: 1
# Do you prefer ' Gankutsuou ' to ' Mai-HiME '? (-1=second is better, 0=tied, 1=first is better)1/0: 1
# Do you prefer ' Serial Experiments Lain ' to ' Fruits Basket '? (-1=second is better, 0=tied, 1=first is better)1/0: 1
# Do you prefer ' Neon Genesis Evangelion: The End of Evangelion ' to ' Trigun '? (-1=second is better, 0=tied, 1=first is better)1/0: 1
# Do you prefer ' Neon Genesis Evangelion: The End of Evangelion ' to ' Serial Experiments Lain '? (-1=second is better, 0=tied, 1=first is better)1/0: 1
# Do you prefer ' Kimi ga Nozomu Eien ' to ' Cowboy Bebop '? (-1=second is better, 0=tied, 1=first is better)1/0: -1
# Do you prefer ' Cowboy Bebop ' to ' Fullmetal Alchemist '? (-1=second is better, 0=tied, 1=first is better)1/0: 1
# Do you prefer ' Serial Experiments Lain ' to ' RahXephon '? (-1=second is better, 0=tied, 1=first is better)1/0: 1
# Do you prefer ' Fruits Basket ' to ' Neon Genesis Evangelion: The End of Evangelion '? (-1=second is better, 0=tied, 1=first is better)1/0: -1
# Do you prefer ' Perfect Blue ' to ' Serial Experiments Lain '? (-1=second is better, 0=tied, 1=first is better)1/0: -1
# Do you prefer ' Mai-HiME ' to ' Monster '? (-1=second is better, 0=tied, 1=first is better)1/0: -1
# Do you prefer ' Neon Genesis Evangelion: The End of Evangelion ' to ' Fruits Basket '? (-1=second is better, 0=tied, 1=first is better)1/0: 1
# Do you prefer ' Gankutsuou ' to ' Perfect Blue '? (-1=second is better, 0=tied, 1=first is better)1/0: 1
# Do you prefer ' Jin-Rou ' to ' Gankutsuou '? (-1=second is better, 0=tied, 1=first is better)1/0: -1
# Do you prefer ' Monster ' to ' Trigun '? (-1=second is better, 0=tied, 1=first is better)1/0: 1

## results of all the questioning:
comparisons
updatedRankings <- BTm(cbind(win1, win2), Media.1, Media.2, ~ Media, id = "Media", data = comparisons)
coefficients <- BTabilities(updatedRankings)
rownames(coefficients)[which.max(coefficients[2,])]

### print(summary(updatedRankings))
### print(sort(coefficients[,1]))

foo <- as.data.frame(BTabilities(updatedRankings))
foo$Media <- rownames(foo)
rownames(foo) <- NULL

quantiles <- seq(0,1, by=0.1)
foo$Quantile <- with(foo, cut(ability,
                                breaks=quantile(ability, probs=quantiles),
                                labels=(quantiles*(length(quantiles)-1))[-1],
                                include.lowest=TRUE))
print(subset(foo[order(foo$Quantile, decreasing=TRUE),], select=c("Media", "Quantile")))
#                                             Media  Quantile
# 2                                    Cowboy Bebop     10
# 17                                        Monster     10
# 18 Neon Genesis Evangelion: The End of Evangelion     10
# 9                                      Gankutsuou      9
# 21                        Serial Experiments Lain      9
# 3                                      Death Note      8
# 12                                        Jin-Rou      8
# 5                                            FLCL      7
# 19                                   Perfect Blue      7
# 6                                   Fruits Basket      6
# 20                                      RahXephon      6
# 7                             Fullmetal Alchemist      5
# 10                                Gunslinger Girl      5
# 22                                         Trigun      5
# 4                        El Hazard: The Wanderers      4
# 14                                     Last Exile      4
# 8                               Full Metal Panic!      3
# 16                        Mobile Suit Gundam Wing      3
# 1                                         Chobits      2
# 11                                    .hack//Sign      2
# 13                            Kimi ga Nozomu Eien      1
# 15                                       Mai-HiME      1
# 23                             Witch Hunter Robin      1
~~~

This R demonstration code has many flaws:

- it's terminal-only & cannot be run as a commandline tool
- the data and scale are both hardwired
- the UX leaves something to be desired ('-1' is surprisingly annoying to type repeatedly, and it'd be easier to use '1'/'2'/'3' or perhaps the arrow keys)
- it may not be asking particularly information-optimal questions, and, worst of all,
- it does not come with any useful indication of uncertainty

The last is the biggest problem because we have no way of knowing when to stop.
Perhaps after 20 questions, the uncertainty was still high and this is why some of them are missorted; or perhaps diminishing returns had set in and it would have taken so many more questions to stamp out the error that we would prefer just to correct the few erroneous ratings manually in a once-over.
And we don't want to crank up the question-count to something burdensome like 200 just on the off-chance that the sorting is insufficient.
Instead, we'd like some comprehensible probability that each media is assigned to its correct bucket, and perhaps a bound on overall error: for example, I would be satisfied if I could specify something like '90% probability that all media are at least in their correct bucket'.
Since BF-2 is fundamentally a frequentist library, it will never give us this kind of answer; all it has to offer in this vein are _p_-values, which are answers to questions I've never asked.
Between our interest in a sequential trial approach and our interest in producing meaningful probabilities of errors, this motivates looking at Bayesian implementations.

"Paired Comparison Models for Ranking National Soccer Teams", Shawn E. Hallinan https://www.wpi.edu/Pubs/ETD/Available/etd-050505-154305/unrestricted/shawnhal.pdf
Part 4: INTRODUCTION TO JAGS, Jim Albert http://bayes.bgsu.edu/webinar.11.2012/R%20output/Rcode.part4.html
library(BradleyTerry2)
data(baseball)
head(baseball)
teamH = as.numeric(baseball2$home.team)
teamA = as.numeric(baseball2$away.team)
y = baseball2$home.wins
n = baseball2$home.wins + baseball2$away.wins
N = length(y)
J = max(teamH)
model1 <- "model {
for (i in 1:N){
    y[i] ~ dbin (p[i], n[i])
    logit(p[i]) <- a[teamH[i]] - a[teamA[i]]
}
for (j in 1:J){
a[j] ~ dnorm(0, tau)
}
tau <- pow(sigma, -2)
sigma ~ dunif (0, 100)
}"
data = list(y = y, n = n, N = N, J = J, teamA = teamA, teamH = teamH)
library(rjags)
jfit = jags.model(textConnection(model1), data = data, n.chains = 1, n.adapt = 5000)
update(jfit, 5000)
bfit <- coda.samples(jfit, c("a", "sigma"), n.iter = 10000, progress.bar = "gui")
S = summary(bfit)
S

comparisons2 <- comparisons[!(comparisons$win1==0.5),]
teamH = as.numeric(comparisons2$Media.1)
teamA = as.numeric(comparisons2$Media.2)
y = comparisons2$win1
n = comparisons2$win1 + comparisons2$win2
N = length(y)
J = length(levels(comparisons$Media.1))
data = list(y = y, n = n, N = N, J = J, teamA = teamA, teamH = teamH)
library(rjags)
### testjags()
model1 <- "model {
for (i in 1:N){
    y[i] ~ dbin (p[i], n[i])
    logit(p[i]) <- a[teamH[i]] - a[teamA[i]]
}
for (j in 1:J){
    a[j] ~ dnorm(0, tau)
}

for (m in 1:J){
 for(o in 1:J){
  Y[m,o] ~ dbin(logit(y[m] - y[o]), 10)
  }
}

tau <- pow(sigma, -2)
sigma ~ dunif (0, 100)
}"


j1 <- autorun.jags(model=model1, monitor=c("a", "Y"), data=data); j1

j1$mcmc[[2]]

samples <- as.mcmc.list(j1)

lapply
     lapply(X, FUN, ...)



bfit <- coda.samples(j1, c("a", "sigma"), n.iter = 10000, progress.bar = "gui")

jfit <- jags(textConnection(model1), data = data, n.chains = 1, n.adapt = 5000)
update(jfit, 5000)
bfit <- coda.samples(jfit, c("a", "sigma"), n.iter = 10000, progress.bar = "gui")
S = summary(bfit)
S


quantiles <- seq(0,1, by=0.1)
foo$Quantile <- with(foo, cut(ability,
                                breaks=quantile(ability, probs=quantiles),
                                labels=(quantiles*(length(quantiles)-1))[-1],
                                include.lowest=TRUE))
foo <- foo[order(foo$Quantile),]
controlling total error: take the posterior distribution of
                  all the anime; draw, say, 100 samples and create a new discretization into (sorted) deciles; compare all 100 with the discretization you get from the point-estimates and
                  see if any anime shifted deciles; if <5 of the samples...
21:39:15 <@gwern> ...have shifts & are not equal, then you have 95% certainty


R> autoextend.jags(j1, data=data)


We use nested Monte Carlo methods to calculate EI(t) as a function of
time. At each time, we sample a datum from the predictive distribution by
first drawing a set of parameter values from the posterior, and then draw-
ing a data value from the sampling distribution with those parameters. We
then estimate p(d|t, D, I) for that datum using equation (1.5). Repeating
this process and averaging the logarithm of the estimates provides a Monte
Carlo estimate of equation (1.8). The thick solid curve in Figure 2d shows
this estimate of EI(t), using base-2 logarithms so that the relative infor-
mation gain is measured in bits (with an offset so the smallest EI(t) is at
0 bits; the raggedness in the curve reflects the Monte Carlo uncertainties).
EI(t) quantifies the uncertainty that is apparent in the set of thin sampled
v(t) curves. It is maximized near the periastron crossing subsequent to the
available data, at t = 1925 d.

t=time,
d=the value, d, of a future datum at time t; For given values of (Ï„, e, K), the predictive probability density for d is just the likelihood for d (a Gaus- sian centered at v(t; Ï„, e, K)).
D= all the data???
I=denotes the modeling assumptions (Keplerian orbit, noise properties, etc.).

  -l FILE        --load-file=FILE         A local file for Mueval to load, providing definitions. Contents are trusted! Do not put anything dubious in it!

~~~{.Bash}
$ cat anime.txt
"Cowboy Bebop", 10
"Monster", 10
"Neon Genesis Evangelion: The End of Evangelion", 10
"Gankutsuou", 10
"Serial Experiments Lain", 10
"Perfect Blue", 10
"Jin-Rou", 10
"Death Note", 10
"Last Exile", 9
"Fullmetal Alchemist", 9
"Gunslinger Girl", 9
"RahXephon", 9
"Trigun", 9
"Fruits Basket", 9
"FLCL", 9
"Witch Hunter Robin", 7
".hack//Sign", 7
"Chobits", 7
"Full Metal Panic!", 7
"Mobile Suit Gundam Wing", 7
"El Hazard: The Wanderers", 7
"Mai-HiME", 6
"Kimi ga Nozomu Eien", 6
$ cat resorter.R
#!/usr/bin/Rscript

# attempt to load a library implementing the Bradley-Terry model for inferring rankings based on
# comparisons; if it doesn't load, try to install it through R's in-language package management;
# otherwise, abort and warn the user
# https://cran.r-project.org/web/packages/BradleyTerry2/index.html
# http://www.jstatsoft.org/v48/i09/paper
loaded <- library(BradleyTerry2, quietly=TRUE, logical.return=TRUE)
if (!loaded) { write("warning: R library 'BradleyTerry2' unavailable; attempting to install locally...", stderr())
              install.packages("BradleyTerry2")
              loadedAfterInstall <- library(BradleyTerry2, quietly=TRUE, logical.return=TRUE)
              if(!loadedAfterInstall) { write("error: 'BradleyTerry2' unavailable and cannot be installed. Aborting.", stderr()); quit() }
}
# similarly, but for the library to parse commandline arguments:
loaded <- library(argparser, quietly=TRUE, logical.return=TRUE)
if (!loaded) { write("warning: R library 'argparser' unavailable; attempting to install locally...", stderr())
              install.packages("argparser")
              loadedAfterInstall <- library(argparser, quietly=TRUE, logical.return=TRUE)
              if(!loadedAfterInstall) { write("error: 'argparser' unavailable and cannot be installed. Aborting.", stderr()); quit() }
}
p <- arg_parser("sort a list using comparative rankings under the Bradley-Terry statistical model; see http://www.gwern.net/Resorter", name="resorter")
p <- add_argument(p, "--input", short="-i",
                  "input file - a CSV file of items to sort: one per line, with up to two columns. (eg both 'Akira\\n' and 'Akira, 10\\n' are valid). If not set, `resorter` falls back to stdin.", type="character", )
p <- add_argument(p, "--verbose", "whether to print out intermediate statistics", flag=TRUE)
p <- add_argument(p, "--queries", short="-n", default=.Machine$integer.max,
                  "Maximum number of questions to ask the user; if already rated, O(items) is a good max, but the more items and more levels in the scale, the more comparisons are needed.")
p <- add_argument(p, "--levels", short="-l", default=5, "The highest level; rated items will be discretized into 1-l levels, so l=5 means items are bucketed into 5 levels: [1,2,3,4,5], etc")
p <- add_argument(p, "--no-scale", short="-ns", flag=TRUE, "Do not discretize/bucket the final estimated latent ratings into 1-l ratings; print out inferred latent scores.")
p <- add_argument(p, "--no-progress", flag=TRUE, "Do not print out mean standard error of items")
argv <- parse_args(p)

# read in the data from either the specified file or stdin:
if(!is.na(argv$input)) { ranking <- read.csv(file=argv$input, header=FALSE); } else { ranking <- read.csv(file=file('stdin'), header=FALSE); }

# if user did not specify a second column of initial ratings, then put in a default of '1':
if(ncol(ranking)==1) { ranking$Rating <- 1; }
colnames(ranking) <- c("Media", "Rating")
# A set of ratings like 'foo,1\nbar,2' is not comparisons, though. We *could* throw out everything except the 'Media' column
# but we would like to accelerate the interactive querying process by exploiting the valuable data the user has given us.
# So we 'seed' the comparison dataset based on input data: higher rating means +1, lower means -1, same rating == tie (0.5 to both)
comparisons <- NULL
for (i in 1:(nrow(ranking)-1)) {
 rating1 <- ranking[i,]$Rating
 media1 <- ranking[i,]$Media
 rating2 <- ranking[i+1,]$Rating
 media2 <- ranking[i+1,]$Media
 if (rating1 == rating2)
  {
     comparisons <- rbind(comparisons, data.frame("Media.1"=media1, "Media.2"=media2, "win1"=0.5, "win2"=0.5))
  } else { if (rating1 > rating2)
           {
            comparisons <- rbind(comparisons, data.frame("Media.1"=media1, "Media.2"=media2, "win1"=1, "win2"=0))
            } else {
              comparisons <- rbind(comparisons, data.frame("Media.1"=media1, "Media.2"=media2, "win1"=0, "win2"=1))
                   } } }
# the use of '0.5' is recommended by the BT2 paper, despite causing quasi-spurious warnings:
# > In several of the data examples (e.g., `?CEMS`, `?springall`, `?sound.fields`), ties are handled by the crude but
# > simple device of adding half of a 'win' to the tally for each player involved; in each of the examples where this
# > has been done it is found that the result is very similar, after a simple re-scaling, to the more sophisticated
# > analyses that have appeared in the literature. Note that this device when used with `BTm` typically gives rise to
# > warnings produced by the back-end glm function, about non-integer 'binomial' counts; such warnings are of no
# > consequence and can be safely ignored. It is likely that a future version of `BradleyTerry2` will have a more
# > general method for handling ties.
suppressWarnings(priorRankings <- BTm(cbind(win1, win2), Media.1, Media.2, ~ Media, id = "Media", data = comparisons))

if(argv$verbose) {
  print("higher=better:")
  print(summary(priorRankings))
  print(sort(BTabilities(priorRankings)[,1]))
}

set.seed(2015-09-10)
cat("Comparison commands: 1=yes, 2=tied, 3=second is better, p=print estimates, s=skip, q=quit\n")
for (i in 1:argv$queries) {

 # with the current data, calculate and extract the new estimates:
 suppressWarnings(updatedRankings <- BTm(cbind(win1, win2), Media.1, Media.2, ~ Media, id = "Media", data = comparisons))
 coefficients <- BTabilities(updatedRankings)
 if(argv$verbose) { print(i); print(coefficients); }

 # pick two media to compare at random but weighted by relative size of standard-error,
 # which is a crude indicator of its uncertainty and the ones where value of information is high:
 targets <- sample(row.names(coefficients), 2, prob=coefficients[,2])
 media1 <- targets[1]
 media2 <- targets[2]

 if (!(argv$`no-progress`)) { cat(paste0("Mean stderr: ", round(mean(coefficients[,2]))), " | "); }
 cat(paste0("Do you like '", as.character(media1), "' better than '", as.character(media2), "'? "))
 rating <- scan("stdin", character(), n=1, quiet=TRUE)

switch(rating,
        "1" = { comparisons <- rbind(comparisons, data.frame("Media.1"=media1, "Media.2"=media2, "win1"=1, "win2"=0)) },
        "3" = { comparisons <- rbind(comparisons, data.frame("Media.1"=media1, "Media.2"=media2, "win1"=0, "win2"=1)) },
        "2" = { comparisons <- rbind(comparisons, data.frame("Media.1"=media1, "Media.2"=media2, "win1"=0.5, "win2"=0.5))},
        "p" = { estimates <- data.frame(Media=row.names(coefficients), Estimate=coefficients[,1], SE=coefficients[,2]);
                print(estimates[order(estimates$Estimate),], row.names=FALSE) },
        "s" = {},
        "q" = { break; }
        )
}

# results of all the questioning:
if(argv$verbose) { print(comparisons); }

suppressWarnings(updatedRankings <- BTm(cbind(win1, win2), Media.1, Media.2, ~ Media, id = "Media", data = comparisons))
coefficients <- BTabilities(updatedRankings)
if(argv$verbose) { print(rownames(coefficients)[which.max(coefficients[2,])]);
                 print(summary(updatedRankings))
                 print(sort(coefficients[,1])) }

ranking2 <- as.data.frame(BTabilities(updatedRankings))
ranking2$Media <- rownames(ranking2)
rownames(ranking2) <- NULL

if(!(argv$`no-scale`)) {
    quantiles <- seq(0, 1, length.out=(argv$levels+1))
    ranking2$Quantile <- with(ranking2, cut(ability,
                                    breaks=quantile(ability, probs=quantiles),
                                    labels=(quantiles*(length(quantiles)-1))[-1],
                                    include.lowest=TRUE))
    print(subset(ranking2[order(ranking2$Quantile, decreasing=TRUE),], select=c("Media", "Quantile")))
} else { # print out just the latent continuous scores:
         print(sort(coefficients[,1])) }


$ ./resorter.R  --input anime.txt
Warning message:
In eval(expr, envir, enclos) : non-integer counts in a binomial glm!
Calls: BTm ... <Anonymous> -> eval -> eval -> glm.fit -> eval -> eval
Do you like 'RahXephon' better than 'Perfect Blue'? (1=yes, 2=tied, 3=second is better, q=quit): 3
Do you like 'Monster' better than 'Serial Experiments Lain'? (1=yes, 2=tied, 3=second is better, q=quit): 3
Do you like 'Last Exile' better than 'Gunslinger Girl'? (1=yes, 2=tied, 3=second is better, q=quit): 3
Do you like 'Kimi ga Nozomu Eien' better than 'Monster'? (1=yes, 2=tied, 3=second is better, q=quit): 3
Do you like 'Cowboy Bebop' better than 'Perfect Blue'? (1=yes, 2=tied, 3=second is better, q=quit): 1
Do you like 'Monster' better than 'Serial Experiments Lain'? (1=yes, 2=tied, 3=second is better, q=quit): 2
Do you like 'FLCL' better than 'Cowboy Bebop'? (1=yes, 2=tied, 3=second is better, q=quit): 3
Do you like 'RahXephon' better than 'Kimi ga Nozomu Eien'? (1=yes, 2=tied, 3=second is better, q=quit): 1
Do you like 'Jin-Rou' better than 'Gunslinger Girl'? (1=yes, 2=tied, 3=second is better, q=quit): 1
Do you like 'Mai-HiME' better than 'Gankutsuou'? (1=yes, 2=tied, 3=second is better, q=quit): 3
Do you like 'Death Note' better than 'Jin-Rou'? (1=yes, 2=tied, 3=second is better, q=quit): 1
Do you like 'Gunslinger Girl' better than 'FLCL'? (1=yes, 2=tied, 3=second is better, q=quit): 2
Do you like 'Jin-Rou' better than 'Neon Genesis Evangelion: The End of Evangelion'? (1=yes, 2=tied, 3=second is better, q=quit): 3
Do you like 'Cowboy Bebop' better than 'Gunslinger Girl'? (1=yes, 2=tied, 3=second is better, q=quit): 1
Do you like 'Fullmetal Alchemist' better than 'Gunslinger Girl'? (1=yes, 2=tied, 3=second is better, q=quit): 3
Do you like 'Monster' better than 'Jin-Rou'? (1=yes, 2=tied, 3=second is better, q=quit): 1
Do you like 'Monster' better than 'Perfect Blue'? (1=yes, 2=tied, 3=second is better, q=quit): 2
Do you like 'Monster' better than 'Jin-Rou'? (1=yes, 2=tied, 3=second is better, q=quit): 1
Do you like 'Jin-Rou' better than 'Perfect Blue'? (1=yes, 2=tied, 3=second is better, q=quit): 2
Do you like 'Fruits Basket' better than 'Death Note'? (1=yes, 2=tied, 3=second is better, q=quit): 3
Do you like 'Neon Genesis Evangelion: The End of Evangelion' better than 'Serial Experiments Lain'? (1=yes, 2=tied, 3=second is better, q=quit): 1
Do you like 'Monster' better than 'Fruits Basket'? (1=yes, 2=tied, 3=second is better, q=quit): 1
Do you like 'Last Exile' better than 'Serial Experiments Lain'? (1=yes, 2=tied, 3=second is better, q=quit): q
There were 44 warnings (use warnings() to see them)
Warning messages:
1: In eval(expr, envir, enclos) : non-integer counts in a binomial glm!
Calls: BTm ... <Anonymous> -> eval -> eval -> glm.fit -> eval -> eval
2: glm.fit: fitted probabilities numerically 0 or 1 occurred
                                            Media Quantile
2                                    Cowboy Bebop       10
9                                      Gankutsuou       10
18 Neon Genesis Evangelion: The End of Evangelion       10
17                                        Monster        9
21                        Serial Experiments Lain        9
3                                      Death Note        8
19                                   Perfect Blue        8
12                                        Jin-Rou        7
5                                            FLCL        6
6                                   Fruits Basket        6
10                                Gunslinger Girl        6
7                             Fullmetal Alchemist        5
20                                      RahXephon        5
22                                         Trigun        5
4                        El Hazard: The Wanderers        4
14                                     Last Exile        4
8                               Full Metal Panic!        3
16                        Mobile Suit Gundam Wing        3
1                                         Chobits        2
11                                    .hack//Sign        2
13                            Kimi ga Nozomu Eien        1
15                                       Mai-HiME        1
23                             Witch Hunter Robin        1
~~~
